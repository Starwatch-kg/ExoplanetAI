# üß™ ExoplanetAI - –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥—É

## üìä –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ò–ò

### 1. Unit —Ç–µ—Å—Ç—ã –¥–ª—è ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

```python
# backend/tests/test_enhanced_classifier.py
import pytest
import numpy as np
from ml.enhanced_classifier import OptimizedEnsemble, EnhancedFeatureExtractor

class TestEnhancedClassifier:
    
    @pytest.fixture
    def sample_data(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        np.random.seed(42)
        time = np.linspace(0, 27.4, 1000)  # TESS sector
        flux = np.ones_like(time) + np.random.normal(0, 0.001, len(time))
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç
        transit_mask = (time % 2.5 < 0.1)  # –ü–µ—Ä–∏–æ–¥ 2.5 –¥–Ω—è
        flux[transit_mask] -= 0.01  # –ì–ª—É–±–∏–Ω–∞ 1%
        
        flux_err = np.full_like(flux, 0.001)
        return time, flux, flux_err
    
    def test_feature_extraction(self, sample_data):
        """–¢–µ—Å—Ç –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        time, flux, flux_err = sample_data
        extractor = EnhancedFeatureExtractor()
        
        features = extractor.extract_features(time, flux, flux_err)
        
        assert len(features) > 0
        assert not np.any(np.isnan(features))
        assert not np.any(np.isinf(features))
    
    def test_astrophysical_features(self, sample_data):
        """–¢–µ—Å—Ç –∞—Å—Ç—Ä–æ—Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        time, flux, flux_err = sample_data
        extractor = EnhancedFeatureExtractor()
        
        transit_params = {
            'primary_depth': 0.01,
            'secondary_depth': 0.0001,
            'ingress_duration': 0.1,
            'egress_duration': 0.1,
            'transit_duration': 1.0
        }
        
        features = extractor.extract_astrophysical_features(time, flux, transit_params)
        
        assert 'stellar_contamination_ratio' in features
        assert 'limb_darkening_coefficient' in features
        assert features['stellar_contamination_ratio'] >= 0
    
    def test_model_prediction_consistency(self, sample_data):
        """–¢–µ—Å—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
        time, flux, flux_err = sample_data
        classifier = OptimizedEnsemble()
        
        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        features = np.random.random(50)
        
        # –î–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        predictions = []
        for _ in range(5):
            pred = classifier.predict_with_uncertainty(features)
            predictions.append(pred['confidence'])
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å (—Ä–∞–∑–±—Ä–æ—Å < 5%)
        confidence_std = np.std(predictions)
        assert confidence_std < 0.05, "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã"

    def test_uncertainty_calibration(self, sample_data):
        """–¢–µ—Å—Ç –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏"""
        time, flux, flux_err = sample_data
        classifier = OptimizedEnsemble()
        
        features = np.random.random(50)
        prediction = classifier.predict_with_uncertainty(features)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö
        assert 0 <= prediction['uncertainty'] <= 1
        assert len(prediction['uncertainty_bounds']) == 2
        assert prediction['uncertainty_bounds'][0] <= prediction['uncertainty_bounds'][1]
```

### 2. Integration —Ç–µ—Å—Ç—ã –¥–ª—è API

```python
# backend/tests/test_smart_search_api.py
import pytest
from fastapi.testclient import TestClient
from main import app

client = TestClient(app)

class TestSmartSearchAPI:
    
    def test_smart_analysis_endpoint(self):
        """–¢–µ—Å—Ç —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ —É–º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        request_data = {
            "target_name": "TOI-715",
            "mission": "TESS",
            "include_uncertainty": True,
            "explain_prediction": True
        }
        
        response = client.post("/api/v1/ai/analyze_lightcurve", json=request_data)
        
        assert response.status_code == 200
        data = response.json()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
        assert "predicted_class" in data
        assert "confidence_score" in data
        assert "uncertainty_bounds" in data
        assert "feature_importance" in data
        assert "decision_reasoning" in data
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        assert 0 <= data["confidence_score"] <= 1
        assert len(data["uncertainty_bounds"]) == 2
        assert isinstance(data["decision_reasoning"], list)
    
    def test_smart_search_endpoint(self):
        """–¢–µ—Å—Ç —ç–Ω–¥–ø–æ–∏–Ω—Ç–∞ —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        request_data = {
            "query": "TOI-715",
            "filters": {
                "confidence_min": 0.7,
                "snr_min": 5.0,
                "missions": ["TESS"]
            },
            "use_ai_ranking": True,
            "max_results": 10
        }
        
        response = client.post("/api/v1/ai/smart_search", json=request_data)
        
        assert response.status_code == 200
        data = response.json()
        
        assert "results" in data
        assert "recommendations" in data
        assert "search_time_ms" in data
        assert data["search_time_ms"] > 0
    
    def test_batch_analysis_endpoint(self):
        """–¢–µ—Å—Ç –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        request_data = {
            "targets": ["TOI-715", "Kepler-452b"],
            "analysis_params": {
                "mission": "TESS",
                "include_uncertainty": True
            },
            "parallel_limit": 2
        }
        
        response = client.post("/api/v1/ai/batch_analyze", json=request_data)
        
        assert response.status_code == 200
        data = response.json()
        
        assert "successful_analyses" in data
        assert "failed_analyses" in data
        assert "success_rate" in data
        assert 0 <= data["success_rate"] <= 1
```

### 3. Performance —Ç–µ—Å—Ç—ã

```python
# backend/tests/test_performance.py
import time
import pytest
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from ml.enhanced_classifier import OptimizedEnsemble

class TestPerformance:
    
    def test_analysis_speed(self):
        """–¢–µ—Å—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        classifier = OptimizedEnsemble()
        features = np.random.random(50)
        
        start_time = time.time()
        prediction = classifier.predict_with_uncertainty(features)
        end_time = time.time()
        
        analysis_time = (end_time - start_time) * 1000  # –º—Å
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–ª–∂–µ–Ω –∑–∞–Ω–∏–º–∞—Ç—å –º–µ–Ω–µ–µ 500–º—Å
        assert analysis_time < 500, f"–ê–Ω–∞–ª–∏–∑ —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π: {analysis_time:.1f}–º—Å"
    
    def test_concurrent_analysis(self):
        """–¢–µ—Å—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
        classifier = OptimizedEnsemble()
        
        def analyze_sample():
            features = np.random.random(50)
            return classifier.predict_with_uncertainty(features)
        
        start_time = time.time()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º 10 –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(analyze_sample) for _ in range(10)]
            results = [future.result() for future in futures]
        
        end_time = time.time()
        total_time = (end_time - start_time) * 1000
        
        # –í—Å–µ –∞–Ω–∞–ª–∏–∑—ã –¥–æ–ª–∂–Ω—ã –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è –º–µ–Ω–µ–µ —á–µ–º –∑–∞ 2 —Å–µ–∫—É–Ω–¥—ã
        assert total_time < 2000, f"–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π: {total_time:.1f}–º—Å"
        assert len(results) == 10
        
    def test_memory_usage(self):
        """–¢–µ—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        classifier = OptimizedEnsemble()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –º–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–æ–≤
        for _ in range(100):
            features = np.random.random(50)
            classifier.predict_with_uncertainty(features)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–µ–≤—ã—à–∞—Ç—å 100MB
        assert memory_increase < 100, f"–£—Ç–µ—á–∫–∞ –ø–∞–º—è—Ç–∏: +{memory_increase:.1f}MB"
```

## üìà –°–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

### 1. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏

```python
# backend/monitoring/model_monitor.py
import numpy as np
import logging
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import redis
import json

logger = logging.getLogger(__name__)

class ModelPerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ML –º–æ–¥–µ–ª–∏"""
    
    def __init__(self, redis_client=None):
        self.redis_client = redis_client
        self.metrics_history = []
        
    async def track_prediction(self, 
                             prediction: Dict,
                             ground_truth: Optional[str] = None,
                             user_feedback: Optional[Dict] = None):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        
        timestamp = datetime.now()
        
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics = {
            'timestamp': timestamp.isoformat(),
            'predicted_class': prediction['class_name'],
            'confidence': prediction['confidence'],
            'uncertainty': prediction.get('uncertainty', 0),
            'processing_time_ms': prediction.get('processing_time_ms', 0)
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º ground truth –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if ground_truth:
            metrics['ground_truth'] = ground_truth
            metrics['correct_prediction'] = (prediction['class_name'] == ground_truth)
            
        # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if user_feedback:
            metrics['user_rating'] = user_feedback.get('rating')
            metrics['user_comment'] = user_feedback.get('comment')
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Redis
        if self.redis_client:
            key = f"model_metrics:{timestamp.strftime('%Y%m%d')}"
            self.redis_client.lpush(key, json.dumps(metrics))
            self.redis_client.expire(key, 86400 * 30)  # 30 –¥–Ω–µ–π
            
        self.metrics_history.append(metrics)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
        await self._check_anomalies(metrics)
        
    async def _check_anomalies(self, metrics: Dict):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if metrics['processing_time_ms'] > 5000:  # 5 —Å–µ–∫—É–Ω–¥
            await self._send_alert(
                "Slow prediction detected",
                f"Processing time: {metrics['processing_time_ms']}ms"
            )
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        if metrics['confidence'] < 0.3:
            await self._send_alert(
                "Low confidence prediction",
                f"Confidence: {metrics['confidence']:.2f}"
            )
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—ã—Å–æ–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        if metrics['uncertainty'] > 0.8:
            await self._send_alert(
                "High uncertainty detected",
                f"Uncertainty: {metrics['uncertainty']:.2f}"
            )
    
    async def calculate_drift_metrics(self, window_days: int = 7) -> Dict:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥—Ä–µ–π—Ñ–∞ –º–æ–¥–µ–ª–∏"""
        
        cutoff_date = datetime.now() - timedelta(days=window_days)
        recent_metrics = [
            m for m in self.metrics_history 
            if datetime.fromisoformat(m['timestamp']) > cutoff_date
        ]
        
        if len(recent_metrics) < 10:
            return {"error": "Insufficient data for drift calculation"}
            
        # –î—Ä–µ–π—Ñ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        confidences = [m['confidence'] for m in recent_metrics]
        confidence_drift = np.std(confidences)
        
        # –î—Ä–µ–π—Ñ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        processing_times = [m['processing_time_ms'] for m in recent_metrics]
        time_drift = np.std(processing_times)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        class_distribution = {}
        for m in recent_metrics:
            cls = m['predicted_class']
            class_distribution[cls] = class_distribution.get(cls, 0) + 1
            
        return {
            'confidence_drift': confidence_drift,
            'processing_time_drift': time_drift,
            'class_distribution': class_distribution,
            'total_predictions': len(recent_metrics),
            'window_days': window_days
        }
    
    async def _send_alert(self, title: str, message: str):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–∞"""
        alert = {
            'timestamp': datetime.now().isoformat(),
            'title': title,
            'message': message,
            'severity': 'warning'
        }
        
        logger.warning(f"Model Alert: {title} - {message}")
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –æ—Ç–ø—Ä–∞–≤–∫—É –≤ Slack, email –∏ —Ç.–¥.
        if self.redis_client:
            self.redis_client.lpush("model_alerts", json.dumps(alert))
```

### 2. –°–∏—Å—Ç–µ–º–∞ –º–µ—Ç—Ä–∏–∫ –∏ –¥–∞—à–±–æ—Ä–¥

```python
# backend/monitoring/metrics_collector.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
from functools import wraps

# Prometheus –º–µ—Ç—Ä–∏–∫–∏
PREDICTION_COUNTER = Counter('ml_predictions_total', 'Total ML predictions', ['model', 'class'])
PREDICTION_LATENCY = Histogram('ml_prediction_duration_seconds', 'ML prediction latency')
CONFIDENCE_GAUGE = Gauge('ml_confidence_score', 'Current confidence score')
ACCURACY_GAUGE = Gauge('ml_accuracy_score', 'Current accuracy score')
ACTIVE_USERS = Gauge('active_users_total', 'Number of active users')

def track_prediction_metrics(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            result = await func(*args, **kwargs)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            PREDICTION_COUNTER.labels(
                model='enhanced_ensemble',
                class=result.get('predicted_class', 'unknown')
            ).inc()
            
            CONFIDENCE_GAUGE.set(result.get('confidence_score', 0))
            
            return result
            
        finally:
            # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            PREDICTION_LATENCY.observe(time.time() - start_time)
            
    return wrapper

class MetricsCollector:
    """–°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    
    def __init__(self):
        self.start_prometheus_server()
        
    def start_prometheus_server(self, port: int = 8000):
        """–ó–∞–ø—É—Å–∫ Prometheus —Å–µ—Ä–≤–µ—Ä–∞ –º–µ—Ç—Ä–∏–∫"""
        start_http_server(port)
        
    def update_accuracy_metrics(self, accuracy: float):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏"""
        ACCURACY_GAUGE.set(accuracy)
        
    def track_user_activity(self, user_count: int):
        """–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        ACTIVE_USERS.set(user_count)
```

### 3. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```python
# backend/monitoring/ab_testing.py
import random
import hashlib
from typing import Dict, Any
from enum import Enum

class ModelVersion(Enum):
    CURRENT = "current"
    ENHANCED = "enhanced"
    EXPERIMENTAL = "experimental"

class ABTestManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self):
        self.model_weights = {
            ModelVersion.CURRENT: 0.7,      # 70% —Ç—Ä–∞—Ñ–∏–∫–∞
            ModelVersion.ENHANCED: 0.25,     # 25% —Ç—Ä–∞—Ñ–∏–∫–∞  
            ModelVersion.EXPERIMENTAL: 0.05  # 5% —Ç—Ä–∞—Ñ–∏–∫–∞
        }
        
        self.results = {version: [] for version in ModelVersion}
        
    def get_model_version(self, user_id: str) -> ModelVersion:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º hash –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        hash_value = int(hashlib.md5(user_id.encode()).hexdigest(), 16)
        normalized_hash = (hash_value % 1000) / 1000.0
        
        cumulative_weight = 0
        for version, weight in self.model_weights.items():
            cumulative_weight += weight
            if normalized_hash <= cumulative_weight:
                return version
                
        return ModelVersion.CURRENT
    
    def record_result(self, 
                     version: ModelVersion,
                     prediction: Dict[str, Any],
                     user_feedback: Dict[str, Any] = None):
        """–ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è A/B —Ç–µ—Å—Ç–∞"""
        
        result = {
            'timestamp': time.time(),
            'confidence': prediction.get('confidence', 0),
            'processing_time': prediction.get('processing_time_ms', 0),
            'user_satisfaction': user_feedback.get('rating', 0) if user_feedback else None
        }
        
        self.results[version].append(result)
        
    def calculate_test_results(self) -> Dict[str, Dict]:
        """–†–∞—Å—á–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ A/B —Ç–µ—Å—Ç–∞"""
        
        results = {}
        
        for version, data in self.results.items():
            if not data:
                continue
                
            confidences = [r['confidence'] for r in data]
            processing_times = [r['processing_time'] for r in data]
            satisfactions = [r['user_satisfaction'] for r in data if r['user_satisfaction'] is not None]
            
            results[version.value] = {
                'sample_size': len(data),
                'avg_confidence': np.mean(confidences),
                'avg_processing_time': np.mean(processing_times),
                'avg_user_satisfaction': np.mean(satisfactions) if satisfactions else None,
                'confidence_std': np.std(confidences),
                'processing_time_std': np.std(processing_times)
            }
            
        return results
    
    def should_promote_model(self, 
                           test_version: ModelVersion,
                           control_version: ModelVersion = ModelVersion.CURRENT,
                           confidence_threshold: float = 0.95) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏—è –º–æ–¥–µ–ª–∏"""
        
        results = self.calculate_test_results()
        
        if test_version.value not in results or control_version.value not in results:
            return False
            
        test_data = results[test_version.value]
        control_data = results[control_version.value]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏–π
        confidence_improvement = test_data['avg_confidence'] - control_data['avg_confidence']
        speed_improvement = control_data['avg_processing_time'] - test_data['avg_processing_time']
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
        if (confidence_improvement > 0.05 and  # +5% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            speed_improvement > 0 and          # –ù–µ –º–µ–¥–ª–µ–Ω–Ω–µ–µ
            test_data['sample_size'] > 100):   # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
            return True
            
        return False
```

## üö® –ê–ª–µ—Ä—Ç—ã –∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è

### 1. –°–∏—Å—Ç–µ–º–∞ –∞–ª–µ—Ä—Ç–æ–≤

```python
# backend/monitoring/alerting.py
import asyncio
import aiohttp
from typing import List, Dict
from enum import Enum

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"

class AlertManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —Å–∏—Å—Ç–µ–º—ã –∞–ª–µ—Ä—Ç–æ–≤"""
    
    def __init__(self, slack_webhook: str = None, email_config: Dict = None):
        self.slack_webhook = slack_webhook
        self.email_config = email_config
        self.alert_rules = self._setup_alert_rules()
        
    def _setup_alert_rules(self) -> List[Dict]:
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–∞–≤–∏–ª –∞–ª–µ—Ä—Ç–æ–≤"""
        return [
            {
                'name': 'High Error Rate',
                'condition': lambda metrics: metrics.get('error_rate', 0) > 0.05,
                'severity': AlertSeverity.ERROR,
                'message': 'Error rate exceeded 5%'
            },
            {
                'name': 'Low Model Confidence',
                'condition': lambda metrics: metrics.get('avg_confidence', 1) < 0.5,
                'severity': AlertSeverity.WARNING,
                'message': 'Average model confidence below 50%'
            },
            {
                'name': 'Slow Response Time',
                'condition': lambda metrics: metrics.get('avg_response_time', 0) > 2000,
                'severity': AlertSeverity.WARNING,
                'message': 'Average response time above 2 seconds'
            },
            {
                'name': 'Model Drift Detected',
                'condition': lambda metrics: metrics.get('confidence_drift', 0) > 0.3,
                'severity': AlertSeverity.ERROR,
                'message': 'Significant model drift detected'
            }
        ]
    
    async def check_alerts(self, metrics: Dict):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –∞–ª–µ—Ä—Ç–æ–≤"""
        
        for rule in self.alert_rules:
            if rule['condition'](metrics):
                await self.send_alert(
                    title=rule['name'],
                    message=rule['message'],
                    severity=rule['severity'],
                    metrics=metrics
                )
    
    async def send_alert(self, 
                        title: str,
                        message: str,
                        severity: AlertSeverity,
                        metrics: Dict = None):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–∞"""
        
        alert_data = {
            'title': title,
            'message': message,
            'severity': severity.value,
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics or {}
        }
        
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Slack
        if self.slack_webhook:
            await self._send_slack_alert(alert_data)
            
        # –û—Ç–ø—Ä–∞–≤–∫–∞ –ø–æ email
        if self.email_config:
            await self._send_email_alert(alert_data)
            
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        logger.error(f"ALERT: {title} - {message}")
    
    async def _send_slack_alert(self, alert_data: Dict):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–∞ –≤ Slack"""
        
        color_map = {
            'info': '#36a64f',
            'warning': '#ff9500', 
            'error': '#ff0000',
            'critical': '#8b0000'
        }
        
        slack_message = {
            'attachments': [{
                'color': color_map.get(alert_data['severity'], '#ff0000'),
                'title': f"üö® {alert_data['title']}",
                'text': alert_data['message'],
                'fields': [
                    {
                        'title': 'Severity',
                        'value': alert_data['severity'].upper(),
                        'short': True
                    },
                    {
                        'title': 'Timestamp',
                        'value': alert_data['timestamp'],
                        'short': True
                    }
                ]
            }]
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                await session.post(self.slack_webhook, json=slack_message)
        except Exception as e:
            logger.error(f"Failed to send Slack alert: {e}")
```

## üìã –ß–µ–∫-–ª–∏—Å—Ç –¥–ª—è production

### –ü–µ—Ä–µ–¥ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ–º:

- [ ] –í—Å–µ unit —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç (>95% –ø–æ–∫—Ä—ã—Ç–∏–µ)
- [ ] Integration —Ç–µ—Å—Ç—ã API –ø—Ä–æ—Ö–æ–¥—è—Ç
- [ ] Performance —Ç–µ—Å—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç <500ms –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
- [ ] –ù–∞–≥—Ä—É–∑–æ—á–Ω—ã–µ —Ç–µ—Å—Ç—ã —Å 100+ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç
- [ ] –ê–ª–µ—Ä—Ç—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
- [ ] A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–æ –∫ –∑–∞–ø—É—Å–∫—É
- [ ] –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –Ω–∞ production —É—Ä–æ–≤–Ω–µ
- [ ] –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ
- [ ] Rollback –ø–ª–∞–Ω –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω

### –ü–æ—Å–ª–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏—è:

- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–ª–µ—Ä—Ç–æ–≤ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- [ ] –í–∞–ª–∏–¥–∞—Ü–∏—è A/B —Ç–µ—Å—Ç–æ–≤
- [ ] –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥ –Ω–∞–≥—Ä—É–∑–∫–æ–π
- [ ] –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
- [ ] –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥—Ä–µ–π—Ñ–∞ –º–æ–¥–µ–ª–∏
- [ ] –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
