"""
AI Module Usage Examples

–ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è AI –º–æ–¥—É–ª—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–≤—ã—Ö –±–ª–µ—Å–∫–∞ –∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç.
"""

import asyncio
import numpy as np
import torch
from pathlib import Path
import matplotlib.pyplot as plt
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç—ã AI –º–æ–¥—É–ª—è
from backend.ai.models import CNNClassifier, LSTMClassifier, TransformerClassifier
from backend.ai.ensemble import EnsembleClassifier, create_default_ensemble
from backend.ai.trainer import ModelTrainer, TransitDataset
from backend.ai.predictor import TransitPredictor, AIAssistant
from backend.ai.embeddings import EmbeddingManager
from backend.ai.database import DatabaseManager
from backend.ai.config import AIConfig
from backend.ai.utils import (
    normalize_lightcurve, remove_outliers, resample_lightcurve,
    fold_lightcurve, validate_lightcurve_data
)

def example_1_basic_model_usage():
    """
    –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    """
    print("=== –ü—Ä–∏–º–µ—Ä 1: –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
    np.random.seed(42)
    lightcurve_length = 1024
    time = np.linspace(0, 27.4, lightcurve_length)  # 27.4 –¥–Ω—è (—Å–µ–∫—Ç–æ—Ä TESS)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ —Å —Ç—Ä–∞–Ω–∑–∏—Ç–æ–º
    flux = np.ones_like(time) + np.random.normal(0, 0.001, len(time))
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç
    period = 3.5  # –¥–Ω–∏
    transit_depth = 0.01  # 1% –≥–ª—É–±–∏–Ω–∞
    transit_duration = 0.1  # –¥–Ω–∏
    
    for i in range(int(27.4 / period)):
        transit_center = i * period + 1.0
        transit_mask = np.abs(time - transit_center) < transit_duration / 2
        flux[transit_mask] -= transit_depth
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CNN –º–æ–¥–µ–ª–∏
    print("–°–æ–∑–¥–∞–Ω–∏–µ CNN –º–æ–¥–µ–ª–∏...")
    cnn_model = CNNClassifier(
        input_size=lightcurve_length,
        num_classes=2,
        num_filters=(32, 64, 128),
        use_attention=True
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    cnn_model.eval()
    with torch.no_grad():
        input_tensor = torch.FloatTensor(flux).unsqueeze(0)
        logits = cnn_model(input_tensor)
        probabilities = torch.softmax(logits, dim=-1)
        
    print(f"CNN –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {probabilities[0].numpy()}")
    print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∏—Ç–∞: {probabilities[0, 1].item():.3f}")
    
    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features = cnn_model.extract_features(input_tensor)
    print(f"–†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features.shape}")

def example_2_ensemble_usage():
    """
    –ü—Ä–∏–º–µ—Ä 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 2: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è
    ensemble = create_default_ensemble(
        input_size=1024,
        num_classes=2,
        device='cpu'
    )
    
    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    batch_size = 4
    sequence_length = 1024
    test_data = torch.randn(batch_size, sequence_length)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ—Ü–µ–Ω–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
    predictions, uncertainties, individual_preds = ensemble.predict_with_uncertainty(test_data)
    
    print(f"–ê–Ω—Å–∞–º–±–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {predictions}")
    print(f"–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏: {uncertainties}")
    print(f"–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏: {list(individual_preds.keys())}")
    
    # –í–∫–ª–∞–¥ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    contributions = ensemble.get_model_contributions(test_data)
    print(f"–í–∫–ª–∞–¥ –º–æ–¥–µ–ª–µ–π: {contributions}")

def example_3_training_workflow():
    """
    –ü—Ä–∏–º–µ—Ä 3: –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 3: –ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    def create_synthetic_dataset(n_samples=1000, sequence_length=1024):
        lightcurves = []
        labels = []
        
        for i in range(n_samples):
            # –ë–∞–∑–æ–≤–∞—è –∫—Ä–∏–≤–∞—è –±–ª–µ—Å–∫–∞
            time = np.linspace(0, 27.4, sequence_length)
            flux = np.ones_like(time) + np.random.normal(0, 0.001, len(time))
            
            # 50% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∏—Ç–∞
            has_transit = np.random.random() > 0.5
            
            if has_transit:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç
                period = np.random.uniform(1.0, 10.0)
                depth = np.random.uniform(0.001, 0.02)
                duration = np.random.uniform(0.05, 0.3)
                
                n_transits = int(27.4 / period)
                for j in range(n_transits):
                    center = j * period + np.random.uniform(0.5, period - 0.5)
                    if center < 27.4:
                        mask = np.abs(time - center) < duration / 2
                        flux[mask] -= depth
                
                labels.append(1)  # –¢—Ä–∞–Ω–∑–∏—Ç
            else:
                labels.append(0)  # –ù–µ—Ç —Ç—Ä–∞–Ω–∑–∏—Ç–∞
            
            lightcurves.append(flux)
        
        return np.array(lightcurves), np.array(labels)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    print("–°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    train_lightcurves, train_labels = create_synthetic_dataset(800)
    val_lightcurves, val_labels = create_synthetic_dataset(200)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ DataLoader'–æ–≤
    train_dataset = TransitDataset(train_lightcurves, train_labels, augment=True)
    val_dataset = TransitDataset(val_lightcurves, val_labels, augment=False)
    
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç—Ä–µ–Ω–µ—Ä–∞
    model = CNNClassifier(input_size=1024, num_classes=2)
    trainer = ModelTrainer(
        model=model,
        device='cpu',
        experiment_name='synthetic_transit_detection',
        use_wandb=False,
        use_mlflow=False
    )
    
    # –û–±—É—á–µ–Ω–∏–µ (–∫–æ—Ä–æ—Ç–∫–æ–µ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞)
    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    history = trainer.train(
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=5,
        learning_rate=1e-3,
        early_stopping_patience=3
    )
    
    print(f"–§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {history['val_acc'][-1]:.3f}")

def example_4_predictor_and_assistant():
    """
    –ü—Ä–∏–º–µ—Ä 4: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä–∞ –∏ AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 4: –ü—Ä–µ–¥–∏–∫—Ç–æ—Ä –∏ AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    model = CNNClassifier(input_size=1024, num_classes=2)
    embedding_manager = EmbeddingManager(embedding_dim=256)
    predictor = TransitPredictor(model, embedding_manager, device='cpu')
    assistant = AIAssistant()
    
    # –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –∫—Ä–∏–≤–∞—è –±–ª–µ—Å–∫–∞ —Å —Ç—Ä–∞–Ω–∑–∏—Ç–æ–º
    time = np.linspace(0, 27.4, 1024)
    flux = np.ones_like(time) + np.random.normal(0, 0.001, len(time))
    
    # –î–æ–±–∞–≤–ª—è–µ–º —è–≤–Ω—ã–π —Ç—Ä–∞–Ω–∑–∏—Ç
    period = 4.2
    depth = 0.015
    duration = 0.12
    
    for i in range(int(27.4 / period)):
        center = i * period + 2.1
        if center < 27.4:
            mask = np.abs(time - center) < duration / 2
            flux[mask] -= depth
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    prediction = predictor.predict(
        lightcurve=flux,
        target_name="Synthetic Target 1",
        stellar_params={'radius': 1.0, 'temperature': 5778}
    )
    
    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ç—Ä–∞–Ω–∑–∏—Ç: {prediction.is_transit}")
    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction.confidence:.3f}")
    print(f"–£—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏: {prediction.confidence_level.value}")
    print(f"–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {prediction.explanation}")
    
    # AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
    beginner_explanation = assistant.explain_for_beginners(prediction, "Synthetic Target 1")
    print(f"\n–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö:\n{beginner_explanation}")
    
    comparison = assistant.compare_with_known_planets(prediction)
    print(f"\n–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø–ª–∞–Ω–µ—Ç–∞–º–∏:\n{comparison}")
    
    habitability = assistant.explain_habitability(prediction)
    print(f"\n–û—Ü–µ–Ω–∫–∞ –æ–±–∏—Ç–∞–µ–º–æ—Å—Ç–∏:\n{habitability}")

async def example_5_database_integration():
    """
    –ü—Ä–∏–º–µ—Ä 5: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 5: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö ===")
    
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –î–ª—è —Ä–∞–±–æ—Ç—ã —Ç—Ä–µ–±—É–µ—Ç—Å—è PostgreSQL
    try:
        db_manager = DatabaseManager(
            database_url="postgresql://user:password@localhost/exoplanet_ai_test"
        )
        
        await db_manager.initialize()
        print("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞
        from backend.ai.database import AnalysisResult
        
        analysis_result = AnalysisResult(
            target_name="Test Target",
            analysis_timestamp=datetime.now(),
            model_version="1.0.0",
            is_transit=True,
            confidence=0.85,
            transit_probability=0.85,
            physical_parameters={
                'period': 4.2,
                'depth': 1500,  # ppm
                'duration': 2.9,  # hours
                'planet_radius': 1.2  # Earth radii
            },
            bls_parameters={
                'best_period': 4.2,
                'best_power': 0.75,
                'snr': 9.2
            }
        )
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_id = await db_manager.save_analysis_result(analysis_result)
        print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω —Å ID: {result_id}")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        stats = await db_manager.get_statistics()
        print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–î: {stats}")
        
        await db_manager.close()
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {e}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ PostgreSQL –∑–∞–ø—É—â–µ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

def example_6_data_preprocessing():
    """
    –ü—Ä–∏–º–µ—Ä 6: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 6: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞—à—É–º–ª–µ–Ω–Ω–æ–π –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞
    time = np.linspace(0, 27.4, 2000)
    flux = np.ones_like(time) + np.random.normal(0, 0.002, len(time))
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
    outlier_indices = np.random.choice(len(flux), size=20, replace=False)
    flux[outlier_indices] += np.random.uniform(0.01, 0.05, 20)
    
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(time)} —Ç–æ—á–µ–∫")
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    validation = validate_lightcurve_data(time, flux)
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏—è: {validation['is_valid']}")
    if validation['warnings']:
        print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è: {validation['warnings']}")
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {validation['statistics']}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    flux_normalized = normalize_lightcurve(flux, method='median')
    print(f"–ú–µ–¥–∏–∞–Ω–∞ –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏: {np.median(flux_normalized):.6f}")
    
    # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
    flux_clean, outlier_mask = remove_outliers(flux_normalized, threshold=1.5)
    print(f"–£–¥–∞–ª–µ–Ω–æ –≤—ã–±—Ä–æ—Å–æ–≤: {np.sum(~outlier_mask)}")
    
    # –†–µ—Å—ç–º–ø–ª–∏–Ω–≥ –¥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–π –¥–ª–∏–Ω—ã
    time_resampled, flux_resampled = resample_lightcurve(
        time, flux_clean, target_length=1024
    )
    print(f"–î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —Ä–µ—Å—ç–º–ø–ª–∏–Ω–≥–∞: {len(time_resampled)} —Ç–æ—á–µ–∫")
    
    # –§–æ–ª–¥–∏–Ω–≥ –ø–æ –ø–µ—Ä–∏–æ–¥—É
    test_period = 3.5
    phase, flux_folded = fold_lightcurve(time_resampled, flux_resampled, test_period)
    print(f"–§–æ–ª–¥–∏–Ω–≥ –ø–æ –ø–µ—Ä–∏–æ–¥—É {test_period} –¥–Ω–µ–π –≤—ã–ø–æ–ª–Ω–µ–Ω")

def example_7_embedding_management():
    """
    –ü—Ä–∏–º–µ—Ä 7: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ embeddings
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 7: –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ embeddings ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ embeddings
    embedding_manager = EmbeddingManager(
        embedding_dim=256,
        similarity_threshold=0.9,
        max_cache_size=1000,
        use_faiss=False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º sklearn –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
    )
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–æ–≤ embeddings
    np.random.seed(42)
    
    targets = [
        "TIC 441420236", "KIC 8462852", "EPIC 249622103",
        "TIC 307210830", "KIC 12557548"
    ]
    
    # –°–∏–º—É–ª—è—Ü–∏—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    for i, target in enumerate(targets):
        # –°–æ–∑–¥–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–π embedding
        embedding = np.random.randn(256)
        
        # –°–æ–∑–¥–∞–µ–º mock –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        mock_prediction = {
            'is_transit': i % 2 == 0,
            'confidence': np.random.uniform(0.6, 0.95),
            'explanation': f"Mock prediction for {target}"
        }
        
        # –ö—ç—à–∏—Ä—É–µ–º
        embedding_manager.cache_prediction(
            target_name=target,
            embedding=embedding,
            prediction_result=mock_prediction,
            model_version="1.0.0"
        )
    
    print(f"–ó–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–æ {len(targets)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    
    # –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö —Ü–µ–ª–µ–π
    query_embedding = np.random.randn(256)
    similar_targets = embedding_manager.find_similar_targets(
        embedding=query_embedding,
        top_k=3,
        min_similarity=0.1  # –ù–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    )
    
    print("–ü–æ—Ö–æ–∂–∏–µ —Ü–µ–ª–∏:")
    for target_name, similarity, prediction in similar_targets:
        print(f"  {target_name}: —Å—Ö–æ–∂–µ—Å—Ç—å {similarity:.3f}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = embedding_manager.get_embedding_statistics()
    print(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ embeddings: {stats}")

def example_8_configuration():
    """
    –ü—Ä–∏–º–µ—Ä 8: –†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 8: –†–∞–±–æ—Ç–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π ===")
    
    # –ü—Ä–æ—Å–º–æ—Ç—Ä —Ç–µ–∫—É—â–µ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {AIConfig.DEVICE}")
    print(f"CNN –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {AIConfig.CNN_CONFIG}")
    print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è: {AIConfig.TRAINING_CONFIG}")
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    AIConfig.update_config({
        'cnn_config': {
            'num_filters': (64, 128, 256, 512),
            'dropout': 0.2,
            'use_attention': True
        }
    })
    
    print(f"–û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è CNN –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {AIConfig.CNN_CONFIG}")
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
    lstm_config = AIConfig.get_model_config('lstm')
    print(f"LSTM –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {lstm_config}")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
    AIConfig.create_directories()
    print("–ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")

def example_9_model_comparison():
    """
    –ü—Ä–∏–º–µ—Ä 9: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
    """
    print("\n=== –ü—Ä–∏–º–µ—Ä 9: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π ===")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    test_data = torch.randn(10, 1024)
    
    models = {
        'CNN': CNNClassifier(input_size=1024, num_classes=2),
        'LSTM': LSTMClassifier(input_size=1024, num_classes=2),
        'Transformer': TransformerClassifier(input_size=1024, num_classes=2)
    }
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    import time
    
    for model_name, model in models.items():
        model.eval()
        
        # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        start_time = time.time()
        
        with torch.no_grad():
            for _ in range(10):  # 10 –ø—Ä–æ–≥–æ–Ω–æ–≤ –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
                predictions = model(test_data)
        
        avg_time = (time.time() - start_time) / 10
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_count = model.count_parameters()
        
        print(f"{model_name}:")
        print(f"  –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {avg_time*1000:.1f} –º—Å")
        print(f"  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {param_count['total']:,}")
        print(f"  –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: ~{param_count['total']*4/1024/1024:.1f} –ú–ë")

def main():
    """
    –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
    """
    print("üß† AI Module Usage Examples")
    print("=" * 50)
    
    # –ó–∞–ø—É—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤
    example_1_basic_model_usage()
    example_2_ensemble_usage()
    example_3_training_workflow()
    example_4_predictor_and_assistant()
    
    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –ø—Ä–∏–º–µ—Ä
    asyncio.run(example_5_database_integration())
    
    example_6_data_preprocessing()
    example_7_embedding_management()
    example_8_configuration()
    example_9_model_comparison()
    
    print("\n‚úÖ –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã!")
    print("\n–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ:")
    print("1. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ PostgreSQL –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö")
    print("2. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("3. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ")
    print("4. –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π—Ç–µ –¥–ª—è –≤–∞—à–µ–≥–æ hardware")

if __name__ == "__main__":
    main()
