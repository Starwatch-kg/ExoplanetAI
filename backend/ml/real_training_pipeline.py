"""
Real ML Training Pipeline for ExoplanetAI
–†–µ–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—É—á–µ–Ω–∏—è ML –¥–ª—è ExoplanetAI

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ NASA/TESS/Kepler –±–µ–∑ mock –¥–∞–Ω–Ω—ã—Ö
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
import logging
from pathlib import Path
import joblib
from datetime import datetime

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
import xgboost as xgb

# –ê—Å—Ç—Ä–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import lightkurve as lk
from astroquery.mast import Catalogs
from astropy.coordinates import SkyCoord
import astropy.units as u

logger = logging.getLogger(__name__)

class RealExoplanetTrainer:
    """
    –°–∏—Å—Ç–µ–º–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–∞–Ω–Ω—ã—Ö NASA
    """
    
    def __init__(self, data_dir: str = "data/training"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.models = {}
        self.scalers = {}
        self.label_encoder = LabelEncoder()
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        self.config = {
            "min_training_samples": 1000,
            "validation_split": 0.2,
            "min_snr": 5.0,
            "min_transit_depth": 0.001,
            "max_noise_level": 0.01,
            "real_data_only": True
        }

    def fetch_real_training_data(self) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏–∑ NASA –∞—Ä—Ö–∏–≤–æ–≤
        """
        logger.info("üåå –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö NASA –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        training_data = []
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç
        confirmed_planets = self._fetch_confirmed_planets()
        training_data.extend(confirmed_planets)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
        candidates = self._fetch_planet_candidates()
        training_data.extend(candidates)
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ false positives
        false_positives = self._fetch_false_positives()
        training_data.extend(false_positives)
        
        df = pd.DataFrame(training_data)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        df = self._validate_data_quality(df)
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Ä–µ–∞–ª—å–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return df

    def _fetch_confirmed_planets(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç"""
        try:
            # –ü–æ–∏—Å–∫ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –ø–ª–∞–Ω–µ—Ç –≤ TESS
            confirmed = Catalogs.query_criteria(
                catalog="Tic",
                disposition="CONFIRMED",
                limit=500
            )
            
            planets = []
            for planet in confirmed:
                if self._is_valid_planet_data(planet):
                    planet_data = self._extract_planet_features(planet)
                    planet_data['label'] = 'CONFIRMED'
                    planets.append(planet_data)
            
            return planets
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –ø–ª–∞–Ω–µ—Ç: {e}")
            return []

    def _fetch_planet_candidates(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –ø–ª–∞–Ω–µ—Ç—ã"""
        try:
            candidates = Catalogs.query_criteria(
                catalog="Tic", 
                disposition="CANDIDATE",
                limit=300
            )
            
            planets = []
            for candidate in candidates:
                if self._is_valid_planet_data(candidate):
                    planet_data = self._extract_planet_features(candidate)
                    planet_data['label'] = 'CANDIDATE'
                    planets.append(planet_data)
            
            return planets
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {e}")
            return []

    def _fetch_false_positives(self) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ false positive –æ–±—ä–µ–∫—Ç–æ–≤"""
        try:
            false_positives = Catalogs.query_criteria(
                catalog="Tic",
                disposition="FALSE POSITIVE", 
                limit=200
            )
            
            planets = []
            for fp in false_positives:
                if self._is_valid_planet_data(fp):
                    planet_data = self._extract_planet_features(fp)
                    planet_data['label'] = 'FALSE_POSITIVE'
                    planets.append(planet_data)
            
            return planets
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ false positives: {e}")
            return []

    def _is_valid_planet_data(self, planet_data) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –ø–ª–∞–Ω–µ—Ç—ã"""
        required_fields = ['ra', 'dec', 'Tmag']
        
        for field in required_fields:
            if field not in planet_data.colnames or np.isnan(planet_data[field]):
                return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        if planet_data['Tmag'] > 16.0:  # –°–ª–∏—à–∫–æ–º —Ç—É—Å–∫–ª–∞—è –∑–≤–µ–∑–¥–∞
            return False
            
        return True

    def _extract_planet_features(self, planet_data) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–ª–∞–Ω–µ—Ç—ã"""
        features = {
            'ra': float(planet_data['ra']),
            'dec': float(planet_data['dec']),
            'stellar_magnitude': float(planet_data['Tmag']),
            'stellar_radius': float(planet_data.get('rad', 1.0)),
            'stellar_mass': float(planet_data.get('mass', 1.0)),
            'stellar_temperature': float(planet_data.get('Teff', 5778)),
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        try:
            lightcurve = self._fetch_lightcurve(planet_data['ra'], planet_data['dec'])
            if lightcurve is not None:
                lc_features = self._extract_lightcurve_features(lightcurve)
                features.update(lc_features)
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞: {e}")
        
        return features

    def _fetch_lightcurve(self, ra: float, dec: float):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ –¥–ª—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç"""
        try:
            coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
            search_result = lk.search_lightcurve(coord, mission='TESS', radius=20*u.arcsec)
            
            if len(search_result) > 0:
                lc = search_result[0].download()
                return lc.normalize().remove_outliers()
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞: {e}")
        
        return None

    def _extract_lightcurve_features(self, lightcurve) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞"""
        flux = lightcurve.flux.value
        
        features = {
            'flux_mean': np.mean(flux),
            'flux_std': np.std(flux),
            'flux_skewness': self._calculate_skewness(flux),
            'flux_kurtosis': self._calculate_kurtosis(flux),
            'flux_range': np.ptp(flux),
            'flux_median': np.median(flux),
            'flux_mad': np.median(np.abs(flux - np.median(flux))),
        }
        
        # BLS –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤
        try:
            from astropy.timeseries import BoxLeastSquares
            
            bls = BoxLeastSquares(lightcurve.time, lightcurve.flux)
            periodogram = bls.autopower(0.1)
            
            features.update({
                'bls_period': periodogram.period[np.argmax(periodogram.power)],
                'bls_power': np.max(periodogram.power),
                'bls_depth': periodogram.depth[np.argmax(periodogram.power)],
                'bls_duration': periodogram.duration[np.argmax(periodogram.power)]
            })
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ BLS –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        return features

    def _calculate_skewness(self, data: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç –∞—Å–∏–º–º–µ—Ç—Ä–∏–∏"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 3) if std > 0 else 0

    def _calculate_kurtosis(self, data: np.ndarray) -> float:
        """–†–∞—Å—á–µ—Ç —ç–∫—Å—Ü–µ—Å—Å–∞"""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 4) - 3 if std > 0 else 0

    def _validate_data_quality(self, df: pd.DataFrame) -> pd.DataFrame:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        initial_count = len(df)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å NaN
        df = df.dropna()
        
        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        if 'flux_std' in df.columns:
            df = df[df['flux_std'] < self.config['max_noise_level']]
        
        if 'bls_depth' in df.columns:
            df = df[df['bls_depth'] > self.config['min_transit_depth']]
        
        final_count = len(df)
        logger.info(f"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {initial_count - final_count} –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        return df

    def train_models(self, df: pd.DataFrame) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        logger.info("ü§ñ –û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        X, y = self._prepare_training_data(df)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=self.config['validation_split'], 
            stratify=y, random_state=42
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # –û–±—É—á–µ–Ω–∏–µ XGBoost
        xgb_model = self._train_xgboost(X_train_scaled, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        metrics = self._evaluate_model(xgb_model, X_test_scaled, y_test)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self._save_models(xgb_model, scaler)
        
        logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        return metrics

    def _prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_columns = [col for col in df.columns if col != 'label']
        X = df[feature_columns].values
        
        # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
        y = self.label_encoder.fit_transform(df['label'])
        
        return X, y

    def _train_xgboost(self, X_train: np.ndarray, y_train: np.ndarray):
        """–û–±—É—á–µ–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏"""
        model = xgb.XGBClassifier(
            n_estimators=1000,
            max_depth=8,
            learning_rate=0.01,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1,
            early_stopping_rounds=50,
            eval_metric='mlogloss'
        )
        
        model.fit(X_train, y_train)
        return model

    def _evaluate_model(self, model, X_test: np.ndarray, y_test: np.ndarray) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        report = classification_report(y_test, y_pred, output_dict=True)
        
        metrics = {
            'accuracy': report['accuracy'],
            'precision': report['macro avg']['precision'],
            'recall': report['macro avg']['recall'],
            'f1_score': report['macro avg']['f1-score'],
            'training_samples': len(X_test) * 5,  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ
            'timestamp': datetime.now().isoformat()
        }
        
        logger.info(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {metrics['accuracy']:.3f}")
        logger.info(f"üìä F1-score: {metrics['f1_score']:.3f}")
        
        return metrics

    def _save_models(self, model, scaler):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        model_path = self.data_dir / "real_trained_model.joblib"
        scaler_path = self.data_dir / "feature_scaler.joblib"
        encoder_path = self.data_dir / "label_encoder.joblib"
        
        joblib.dump(model, model_path)
        joblib.dump(scaler, scaler_path)
        joblib.dump(self.label_encoder, encoder_path)
        
        logger.info(f"üíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.data_dir}")

    def run_full_training_pipeline(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è...")
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df = self.fetch_real_training_data()
        
        if len(df) < self.config['min_training_samples']:
            logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(df)} < {self.config['min_training_samples']}")
            return None
        
        # 2. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        metrics = self.train_models(df)
        
        # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        report_path = self.data_dir / "training_report.json"
        with open(report_path, 'w') as f:
            import json
            json.dump(metrics, f, indent=2)
        
        logger.info("üéâ –ü–∞–π–ø–ª–∞–π–Ω —Ä–µ–∞–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω!")
        return metrics

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è"""
    trainer = RealExoplanetTrainer()
    trainer.run_full_training_pipeline()

if __name__ == "__main__":
    main()
