<<<<<<< HEAD
from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.middleware.gzip import GZipMiddleware
import uvicorn
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any, Union
from datetime import datetime
import logging
import os
import asyncio
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./exoplanet_ai.db")
ENABLE_AI_FEATURES = os.getenv("ENABLE_AI_FEATURES", "false").lower() == "true"
ENABLE_DATABASE = os.getenv("ENABLE_DATABASE", "true").lower() == "true"

# –ò–º–ø–æ—Ä—Ç —Ñ—É–Ω–∫—Ü–∏–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
DATABASE_AVAILABLE = False
if ENABLE_DATABASE:
    try:
        from database import (
            connect_db, disconnect_db, create_tables, 
            save_analysis_result, get_analysis_results, 
            get_analysis_by_target, save_user_feedback
        )
        DATABASE_AVAILABLE = True
        logger.info("Database functions loaded successfully")
    except ImportError as e:
        DATABASE_AVAILABLE = False
        logger.warning(f"Database functions not available: {e}")
        logger.info("Running without database support")
else:
    logger.info("Database disabled in configuration")

# –ò–º–ø–æ—Ä—Ç –ø—Ä–æ–¥–∞–∫—à–µ–Ω —Å–µ—Ä–≤–∏—Å–∞ –¥–∞–Ω–Ω—ã—Ö
try:
    from production_data_service import production_data_service
    from known_exoplanets import should_have_transit, get_target_info
    REAL_DATA_AVAILABLE = True
    logger.info("Production data service loaded successfully")
except ImportError as e:
    try:
        from real_data_service import real_data_service as production_data_service
        from known_exoplanets import should_have_transit, get_target_info
        REAL_DATA_AVAILABLE = True
        logger.info("Fallback to real data service")
    except ImportError as e2:
        REAL_DATA_AVAILABLE = False
        logger.warning(f"No data service available: {e}, {e2}")
        logger.info("Using basic implementation")

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
class SearchRequest(BaseModel):
    target_name: str = Field(..., min_length=1, max_length=100, description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞")
    catalog: str = Field("TIC", pattern="^(TIC|KIC|EPIC)$", description="–ö–∞—Ç–∞–ª–æ–≥: TIC, KIC –∏–ª–∏ EPIC")
    mission: str = Field("TESS", pattern="^(TESS|Kepler|K2)$", description="–ú–∏—Å—Å–∏—è: TESS, Kepler –∏–ª–∏ K2")
    period_min: float = Field(0.5, ge=0.1, le=100.0, description="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏)")
    period_max: float = Field(20.0, ge=0.1, le=1000.0, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏)")
    duration_min: float = Field(0.05, ge=0.01, le=1.0, description="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∏—Ç–∞ (–¥–Ω–∏)")
    duration_max: float = Field(0.3, ge=0.01, le=2.0, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∏—Ç–∞ (–¥–Ω–∏)")
    snr_threshold: float = Field(7.0, ge=3.0, le=50.0, description="–ü–æ—Ä–æ–≥ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª/—à—É–º")

class HealthStatus(BaseModel):
    status: str = Field(..., description="–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã")
    timestamp: str = Field(..., description="–í—Ä–µ–º—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
    services_available: bool = Field(..., description="–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–æ–≤")
    database_available: bool = Field(..., description="–î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
    services: Dict[str, str] = Field(..., description="–°—Ç–∞—Ç—É—Å –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤")

class FeedbackRequest(BaseModel):
    analysis_id: Optional[int] = Field(None, description="ID –∞–Ω–∞–ª–∏–∑–∞")
    target_name: str = Field(..., min_length=1, description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–∏")
    feedback_type: str = Field(..., pattern="^(positive|negative|correction)$", description="–¢–∏–ø –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏")
    is_correct: bool = Field(..., description="–ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞")
    user_classification: Optional[str] = Field(None, description="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è")
    comments: Optional[str] = Field(None, max_length=1000, description="–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è FastAPI
app = FastAPI(
    title="Exoplanet AI - Transit Detection API",
    description="Advanced AI-powered exoplanet detection system",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Middleware –¥–ª—è —Å–∂–∞—Ç–∏—è
app.add_middleware(GZipMiddleware, minimum_size=1000)

# CORS middleware —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —à–∏—Ä–æ–∫–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ origins –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    allow_credentials=False,  # –û—Ç–∫–ª—é—á–∞–µ–º credentials –¥–ª—è wildcard origins
    allow_methods=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –º–µ—Ç–æ–¥—ã
    allow_headers=["*"],  # –†–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    logger.error(f"Global exception in {request.url}: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error", 
            "message": f"Error: {str(exc)}",
            "timestamp": datetime.now().isoformat(),
            "path": str(request.url)
        }
    )

# –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫
@app.exception_handler(422)
async def validation_exception_handler(request, exc):
    logger.warning(f"Validation error: {exc}")
    return JSONResponse(
        status_code=422,
        content={
            "error": "Validation error",
            "message": "Invalid input data",
            "details": exc.detail if hasattr(exc, 'detail') else str(exc),
            "timestamp": datetime.now().isoformat()
        }
    )

# Startup event (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –Ω–æ–≤—ã–º–∏ –≤–µ—Ä—Å–∏—è–º–∏ FastAPI)
@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("Starting Exoplanet AI Transit Detection API v2.0")
    
    if DATABASE_AVAILABLE:
        try:
            await connect_db()
            create_tables()
            logger.info("Database initialized successfully")
        except Exception as e:
            logger.error(f"Database initialization failed: {e}")
            logger.info("Running without database")
    else:
        logger.info("Running in minimal mode - database not available")

@app.on_event("shutdown")
async def shutdown_event():
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("Shutting down Exoplanet AI API")
    
    if DATABASE_AVAILABLE:
        try:
            await disconnect_db()
            logger.info("Database disconnected")
        except Exception as e:
            logger.error(f"Database disconnection error: {e}")
=======
"""
Enhanced Exoplanet AI Backend v2.0
–£–ª—É—á—à–µ–Ω–Ω—ã–π backend —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º
"""

import asyncio
import os
import time
from contextlib import asynccontextmanager
from typing import Dict, List, Optional, Any

import numpy as np
from fastapi import FastAPI, HTTPException, Depends, Request, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse, PlainTextResponse
from pydantic import BaseModel, Field
import uvicorn

# Unified imports - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥—É–ª–∏
try:
    from config.settings import settings as config
except ImportError:
    # Fallback to old config
    try:
        from core.config import config
    except ImportError:
        # Create minimal config
        class MockConfig:
            monitoring = type('obj', (object,), {
                'service_name': 'exoplanet-ai',
                'enable_tracing': False,
                'enable_metrics': False
            })()
            cache = type('obj', (object,), {
                'cache_ttl': 3600,
                'max_size': 1000
            })()
            rate_limit = type('obj', (object,), {
                'requests_per_minute': 100,
                'burst_requests': 20,
                'burst_window': 60,
                'exclude_paths': []
            })()
            performance = type('obj', (object,), {
                'max_request_size': 10485760
            })()
            security = type('obj', (object,), {
                'allowed_origins': ["http://localhost:5173", "http://localhost:3000"]
            })()
            environment = 'development'
            version = '2.0.0'
            enable_ai_features = True
            enable_database = False
            logging = type('obj', (object,), {
                'file_path': 'logs/app.log', 
                'enable_console': True
            })()
            def get_log_level(self): return 'INFO'
            def to_dict(self): 
                return {
                    'version': self.version,
                    'environment': self.environment,
                    'enable_ai_features': self.enable_ai_features,
                    'enable_database': self.enable_database
                }
        config = MockConfig()

# Import other modules with fallbacks
try:
    from core.logging_config import setup_logging, get_logger, RequestContextLogger
    from core.middleware import setup_middleware, get_request_context
    from core.error_handlers import setup_error_handlers, ExoplanetAIException
    from core.metrics import MetricsMiddleware, metrics_collector, get_metrics, get_metrics_content_type
    from core.telemetry import setup_telemetry, get_trace_id, get_span_id
    from ml.inference_engine import inference_engine, InferenceResult
    from ml.data_loader import ml_data_loader, LightcurveData
except ImportError as e:
    print(f"Warning: Some modules not available: {e}")
    # Create minimal fallbacks
    def setup_logging(*args, **kwargs): pass
    def get_logger(name): return __import__('logging').getLogger(name)
    def setup_middleware(*args, **kwargs): pass
    def get_request_context(*args, **kwargs): return {}
    def setup_error_handlers(*args, **kwargs): pass
    def setup_telemetry(*args, **kwargs): pass
    def get_trace_id(): return None
    def get_span_id(): return None
    
    class MockException(Exception): pass
    ExoplanetAIException = MockException
    
    # Mock metrics
    class MockMetrics:
        def record_exoplanet_analysis(self, *args, **kwargs): pass
        def record_error(self, *args, **kwargs): pass
        def set_app(self, *args, **kwargs): pass
        def set_app_status(self, *args, **kwargs): pass
        def record_cache_operation(self, *args, **kwargs): pass
        def record_api_call(self, *args, **kwargs): pass
        def record_ml_inference(self, *args, **kwargs): pass
    metrics_collector = MockMetrics()
    
    # Mock middleware class
    class MetricsMiddleware:
        def __init__(self, app, *args, **kwargs): 
            self.app = app
        
        async def __call__(self, scope, receive, send):
            return await self.app(scope, receive, send)
    
    inference_engine = None
    ml_data_loader = None

# Import data services
try:
    from services.data_service import data_service
    from services.bls_service import bls_service
    
    # Override with simple mock to avoid aiohttp issues
    class SimpleMockDataService:
        def __init__(self):
            self.session = None
            
        async def __aenter__(self): 
            print("SimpleMockDataService: async entering context")
            return self
            
        async def __aexit__(self, exc_type, exc_val, exc_tb): 
            print("SimpleMockDataService: async exiting context")
            return False
        
        async def get_star_info(self, target_name, catalog):
            # Return mock star info
            return type('StarInfo', (), {
                'target_id': target_name,
                'catalog': type('Catalog', (), {'value': catalog})(),
                'ra': 123.456,
                'dec': 45.678,
                'magnitude': 12.5,
                'temperature': 5500,
                'radius': 1.0,
                'mass': 1.0,
                'stellar_type': 'G'
            })()
        
        async def get_lightcurve(self, target_name, mission):
            # Return mock lightcurve
            import numpy as np
            time = np.linspace(0, 30, 1000)
            flux = np.ones_like(time) + np.random.normal(0, 0.001, len(time))
            flux_err = np.full_like(time, 0.001)
            
            return type('LightCurve', (), {
                'time': time,
                'flux': flux,
                'flux_err': flux_err,
                'cadence_minutes': 30.0,
                'noise_level_ppm': 1000.0,
                'data_source': 'simulation'
            })()
    
    # Override the imported data_service
    data_service = SimpleMockDataService()
    
except ImportError:
    try:
        from production_data_service import production_data_service as data_service
        bls_service = None
    except ImportError:
        # Create simple mock data service that doesn't use aiohttp
        class SimpleMockDataService:
            def __init__(self):
                self.session = None
                
            async def __aenter__(self): 
                return self
                
            async def __aexit__(self, *args): 
                pass
            
            async def get_star_info(self, target_name, catalog):
                # Return mock star info
                return type('StarInfo', (), {
                    'target_id': target_name,
                    'catalog': type('Catalog', (), {'value': catalog})(),
                    'ra': 123.456,
                    'dec': 45.678,
                    'magnitude': 12.5,
                    'temperature': 5500,
                    'radius': 1.0,
                    'mass': 1.0,
                    'stellar_type': 'G'
                })()
            
            async def get_lightcurve(self, target_name, mission):
                # Return mock lightcurve
                import numpy as np
                time = np.linspace(0, 30, 1000)
                flux = np.ones_like(time) + np.random.normal(0, 0.001, len(time))
                flux_err = np.full_like(time, 0.001)
                
                return type('LightCurve', (), {
                    'time': time,
                    'flux': flux,
                    'flux_err': flux_err,
                    'cadence_minutes': 30.0,
                    'noise_level_ppm': 1000.0,
                    'data_source': 'simulation'
                })()
        
        # Override the imported data_service with our simple mock
        data_service = SimpleMockDataService()
        
        # Create mock BLS service
        class MockBLSService:
            def detect_transits(self, time, flux, **kwargs):
                import numpy as np
                return type('BLSResult', (), {
                    'best_period': 3.14159,
                    'best_t0': 1.5,
                    'best_duration': 0.1,
                    'best_power': 15.0,
                    'snr': 8.5,
                    'depth': 0.001,
                    'depth_err': 0.0001,
                    'significance': 0.95,
                    'is_significant': True,
                    'enhanced_analysis': True,
                    'ml_confidence': 0.85,
                    'physical_validation': True
                })()
        
        bls_service = MockBLSService()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
setup_logging(
    service_name=config.monitoring.service_name,
    environment=config.environment,
    log_level=config.get_log_level(),
    log_file=config.logging.file_path,
    enable_console=config.logging.enable_console
)

logger = get_logger(__name__)

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è API
class PredictRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"""
    target_name: str = Field(..., min_length=1, max_length=100, description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–∏")
    catalog: str = Field("TIC", pattern="^(TIC|KIC|EPIC)$", description="–ö–∞—Ç–∞–ª–æ–≥")
    mission: str = Field("TESS", pattern="^(TESS|Kepler|K2)$", description="–ú–∏—Å—Å–∏—è")
    model_name: str = Field("ensemble", description="–ò–º—è ML –º–æ–¥–µ–ª–∏")
    use_ensemble: bool = Field(False, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π")
    confidence_threshold: float = Field(0.7, ge=0.0, le=1.0, description="–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")

class SearchRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ –ø–æ–∏—Å–∫ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç"""
    target_name: str = Field(..., min_length=1, max_length=100, description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–∏")
    catalog: str = Field("TIC", pattern="^(TIC|KIC|EPIC)$", description="–ö–∞—Ç–∞–ª–æ–≥")
    mission: str = Field("TESS", pattern="^(TESS|Kepler|K2)$", description="–ú–∏—Å—Å–∏—è")
    use_bls: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BLS –∞–Ω–∞–ª–∏–∑")
    use_ai: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ò–ò –∞–Ω–∞–ª–∏–∑")
    use_ensemble: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ensemble –ø–æ–∏—Å–∫")
    search_mode: str = Field("ensemble", pattern="^(single|ensemble|comprehensive)$", description="–†–µ–∂–∏–º –ø–æ–∏—Å–∫–∞")
    period_min: float = Field(0.5, ge=0.1, le=100.0, description="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏)")
    period_max: float = Field(20.0, ge=0.1, le=100.0, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏)")
    snr_threshold: float = Field(7.0, ge=3.0, le=20.0, description="–ü–æ—Ä–æ–≥ SNR")

class BLSRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –Ω–∞ BLS –∞–Ω–∞–ª–∏–∑"""
    target_name: str = Field(..., min_length=1, max_length=100, description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–∏")
    catalog: str = Field("TIC", pattern="^(TIC|KIC|EPIC)$", description="–ö–∞—Ç–∞–ª–æ–≥")
    mission: str = Field("TESS", pattern="^(TESS|Kepler|K2)$", description="–ú–∏—Å—Å–∏—è")
    period_min: float = Field(0.5, ge=0.1, le=100.0, description="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏)")
    period_max: float = Field(20.0, ge=0.1, le=100.0, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø–µ—Ä–∏–æ–¥ (–¥–Ω–∏)")
    duration_min: float = Field(0.05, ge=0.01, le=1.0, description="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–¥–Ω–∏)")
    duration_max: float = Field(0.3, ge=0.01, le=1.0, description="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–¥–Ω–∏)")
    snr_threshold: float = Field(7.0, ge=3.0, le=20.0, description="–ü–æ—Ä–æ–≥ SNR")
    use_enhanced: bool = Field(True, description="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑")

class HealthResponse(BaseModel):
    """–û—Ç–≤–µ—Ç health check"""
    status: str
    timestamp: str
    version: str
    environment: str
    services: Dict[str, str]
    ml_models: Dict[str, Any]
    request_id: Optional[str] = None
    trace_id: Optional[str] = None

class PredictResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
    target_name: str
    prediction: float
    confidence: float
    is_planet_candidate: bool
    model_used: str
    inference_time_ms: float
    metadata: Dict[str, Any]
    request_id: Optional[str] = None
    trace_id: Optional[str] = None

class SearchResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –ø–æ–∏—Å–∫–∞ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç"""
    target_name: str
    catalog: str
    mission: str
    bls_result: Optional[Dict[str, Any]] = None
    ai_result: Optional[Dict[str, Any]] = None
    lightcurve_info: Dict[str, Any]
    star_info: Dict[str, Any]
    candidates_found: int
    processing_time_ms: float
    status: str
    request_id: Optional[str] = None
    trace_id: Optional[str] = None

class BLSResponse(BaseModel):
    """–û—Ç–≤–µ—Ç BLS –∞–Ω–∞–ª–∏–∑–∞"""
    target_name: str
    best_period: float
    best_t0: float
    best_duration: float
    best_power: float
    snr: float
    depth: float
    depth_err: float
    significance: float
    is_significant: bool
    enhanced_analysis: bool
    ml_confidence: float
    physical_validation: bool
    processing_time_ms: float
    request_id: Optional[str] = None
    trace_id: Optional[str] = None

class LightCurveResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞"""
    target_name: str
    catalog: str
    mission: str
    time: List[float]
    flux: List[float]
    flux_err: List[float]
    cadence_minutes: float
    noise_level_ppm: float
    data_source: str
    points_count: int
    time_span_days: float
    request_id: Optional[str] = None
    trace_id: Optional[str] = None

class CatalogResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –∫–∞—Ç–∞–ª–æ–≥–∞"""
    catalogs: List[str]
    missions: List[str]
    description: Dict[str, str]

class TargetInfo(BaseModel):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ü–µ–ª–∏"""
    target_id: str
    catalog: str
    ra: float
    dec: float
    magnitude: float
    temperature: Optional[float] = None
    radius: Optional[float] = None
    mass: Optional[float] = None
    distance: Optional[float] = None
    stellar_type: Optional[str] = None

class SearchTargetsResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –ø–æ–∏—Å–∫–∞ —Ü–µ–ª–µ–π"""
    targets: List[TargetInfo]
    total_found: int
    query: str
    catalog: str

# Lifespan events
@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    # Startup
    logger.info("=" * 80)
    logger.info("üöÄ STARTING EXOPLANET AI v2.0")
    logger.info("=" * 80)
    
    try:
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –∑–∞–ø—É—Å–∫–∞
        metrics_collector.set_app_status("starting")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é
        if config.monitoring.enable_tracing:
            setup_telemetry(app)
            logger.info("‚úÖ OpenTelemetry initialized")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞)
        if config.enable_database:
            try:
                # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
                logger.info("‚úÖ Database initialized")
            except Exception as e:
                logger.error(f"‚ùå Database initialization failed: {e}")
        
        # –ü—Ä–æ–≥—Ä–µ–≤–∞–µ–º ML –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã)
        if config.enable_ai_features and inference_engine is not None:
            try:
                await inference_engine.warmup_models()
                logger.info("‚úÖ ML models warmed up")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è ML models warmup failed: {e}")
        elif config.enable_ai_features:
            logger.warning("‚ö†Ô∏è ML models not available - inference_engine is None")
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏
        metrics_collector.set_app_status("healthy")
        
        logger.info("=" * 80)
        logger.info("üéâ EXOPLANET AI v2.0 READY")
        logger.info(f"üåê Environment: {config.environment}")
        logger.info(f"ü§ñ AI Features: {'‚úÖ' if config.enable_ai_features else '‚ùå'}")
        logger.info(f"üíæ Database: {'‚úÖ' if config.enable_database else '‚ùå'}")
        logger.info(f"üìä Metrics: {'‚úÖ' if config.monitoring.enable_metrics else '‚ùå'}")
        logger.info(f"üîç Tracing: {'‚úÖ' if config.monitoring.enable_tracing else '‚ùå'}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"‚ùå Startup failed: {e}")
        metrics_collector.set_app_status("unhealthy")
        raise
    
    yield
    
    # Shutdown
    logger.info("üõë Shutting down Exoplanet AI v2.0")
    
    try:
        # –í—ã–≥—Ä—É–∂–∞–µ–º ML –º–æ–¥–µ–ª–∏
        if config.enable_ai_features:
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –≤—ã–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–µ–π
            pass
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î
        if config.enable_database:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∑–∞–∫—Ä—ã—Ç–∏–µ –ë–î
            pass
        
        logger.info("‚úÖ Shutdown completed successfully")
        
    except Exception as e:
        logger.error(f"‚ùå Shutdown error: {e}")

# –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Exoplanet AI - Enhanced Detection System v2.0",
    description="""
    üåå **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç v2.0**
    
    –°–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π:
    - üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ JSON –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    - üîç OpenTelemetry —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞  
    - üìà Prometheus –º–µ—Ç—Ä–∏–∫–∏
    - ü§ñ ML –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å –∞–Ω—Å–∞–º–±–ª–µ–º –º–æ–¥–µ–ª–µ–π
    - üõ°Ô∏è –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫
    - ‚ö° Rate limiting –∏ middleware
    - üéØ –ü–æ–ª–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —á–µ—Ä–µ–∑ .env
    
    ## –ù–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ v2.0
    - üöÄ –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ backend
    - üî¨ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π ML –ø–∞–π–ø–ª–∞–π–Ω
    - üìä –ü–æ–ª–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å
    - üõ°Ô∏è Enterprise-grade –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
    """,
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ middleware
try:
    setup_middleware(app, {
        "max_request_size": getattr(config, 'performance', type('obj', (), {'max_request_size': 10485760})).max_request_size,
        "rate_limit": {
            "calls": getattr(config, 'rate_limit', type('obj', (), {'requests_per_minute': 100})).requests_per_minute,
            "period": 60,
            "burst_calls": getattr(config, 'rate_limit', type('obj', (), {'burst_requests': 20})).burst_requests,
            "burst_period": getattr(config, 'rate_limit', type('obj', (), {'burst_window': 60})).burst_window,
            "exclude_paths": getattr(config, 'rate_limit', type('obj', (), {'exclude_paths': []})).exclude_paths
        }
    })
except Exception as e:
    print(f"Warning: Could not setup middleware: {e}")
    pass

# –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ middleware
try:
    if config.monitoring.enable_metrics:
        app.add_middleware(MetricsMiddleware)
except:
    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    pass

# CORS middleware - —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
try:
    allowed_origins = config.security.allowed_origins
except:
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π fallback –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    allowed_origins = [
        "http://localhost:5173",  # Vite dev server
        "http://localhost:3000",  # React dev server
        "http://127.0.0.1:5173",
        "http://127.0.0.1:3000",
        "http://localhost:8080",  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ—Ä—Ç—ã
        "http://127.0.0.1:8080",
        "http://localhost:4173",  # Vite preview
        "http://127.0.0.1:4173",
        "*"  # –í—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞–∑—Ä–µ—à–∞–µ–º –≤—Å–µ origins –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    ]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "HEAD"],
    allow_headers=[
        "Accept",
        "Accept-Language",
        "Content-Language",
        "Content-Type",
        "Authorization",
        "X-Requested-With",
        "X-Request-ID",
        "X-Trace-ID",
        "Cache-Control",
        "Pragma"
    ],
    expose_headers=["X-Request-ID", "X-Trace-ID", "X-Process-Time"]
)

# Gzip middleware
app.add_middleware(GZipMiddleware, minimum_size=1000)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤ –æ—à–∏–±–æ–∫
setup_error_handlers(app)

# ===== ENDPOINTS =====
>>>>>>> 975c3a7 (–í–µ—Ä—Å–∏—è 1.5.1)

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
<<<<<<< HEAD
        "message": "Exoplanet AI - Transit Detection API",
        "version": "2.0.0",
        "status": "active",
        "mode": "minimal"
    }

@app.get("/api/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "2.0.0",
        "database": "connected" if DATABASE_AVAILABLE else "disabled",
        "ai_features": "enabled" if ENABLE_AI_FEATURES else "disabled"
    }

@app.get("/api/test-cors")
async def test_cors():
    """–¢–µ—Å—Ç–æ–≤—ã–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ CORS"""
    return {"message": "CORS working!", "timestamp": datetime.now().isoformat()}

@app.post("/api/search")
async def search_exoplanets(request: SearchRequest):
    """
    üîç –ë–ê–ó–û–í–´–ô –ü–û–ò–°–ö –≠–ö–ó–û–ü–õ–ê–ù–ï–¢
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ BLS –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    """
    logger.info("=" * 80)
    logger.info(f"üöÄ –ù–ê–ß–ò–ù–ê–ï–ú –ê–ù–ê–õ–ò–ó –¶–ï–õ–ò: {request.target_name}")
    logger.info(f"üì° –ö–∞—Ç–∞–ª–æ–≥: {request.catalog} | –ú–∏—Å—Å–∏—è: {request.mission}")
    logger.info(f"‚öôÔ∏è  –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞: –ø–µ—Ä–∏–æ–¥ {request.period_min}-{request.period_max} –¥–Ω–µ–π")
    logger.info(f"‚öôÔ∏è  SNR –ø–æ—Ä–æ–≥: {request.snr_threshold}")
    logger.info("=" * 80)
    
    try:
        if REAL_DATA_AVAILABLE:
            logger.info("üìä –≠–¢–ê–ü 1: –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∑–≤–µ–∑–¥–µ...")
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–≤–µ–∑–¥–µ —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ NASA –¥–∞–Ω–Ω—ã–º–∏
            star_info = await production_data_service.get_star_info(request.target_name, request.catalog, use_nasa_data=True)
            logger.info(f"‚≠ê –ó–≤–µ–∑–¥–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {star_info['stellar_type']}, T={star_info['temperature']}K, R={star_info['radius']}R‚òâ")
            
            logger.info("üìä –≠–¢–ê–ü 2: –ï—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π...")
            # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–ª–∏ –±–µ–∑ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏
            target_info = get_target_info(request.target_name, request.catalog)
            logger.info(f"‚≠ê –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ü–µ–ª—å: {target_info.get('full_name', request.target_name)}")
            logger.info(f"üî¨ –†–µ–∂–∏–º: –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤")
            
            # –ù–∏–∫–∞–∫–∏—Ö –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –æ –ø–ª–∞–Ω–µ—Ç–∞—Ö
            has_transit = False
            planet_params = None
            
            logger.info("üìä –≠–¢–ê–ü 3: –ü–æ–ª—É—á–µ–Ω–∏–µ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞...")
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∞–ª—å–Ω—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞ NASA
            nasa_lightcurve = await production_data_service.get_nasa_lightcurve(
                request.target_name, request.mission
            )
            
            if nasa_lightcurve:
                logger.info("üåü –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞ NASA")
                lightcurve_data = nasa_lightcurve
            else:
                logger.info("üé≤ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞")
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞
                lightcurve_data = production_data_service.generate_realistic_lightcurve(
                    request.target_name, 
                    request.mission, 
                    has_transit, 
                    planet_params
                )
            logger.info(f"üìà –ö—Ä–∏–≤–∞—è –±–ª–µ—Å–∫–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–∞: {len(lightcurve_data['time'])} —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö")
            logger.info(f"üìà –®—É–º: {lightcurve_data.get('noise_level_ppm', 'N/A')} ppm")
            
            logger.info("üìä –≠–¢–ê–ü 4: –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ BLS –∞–Ω–∞–ª–∏–∑–∞...")
            # –í—ã–ø–æ–ª–Ω—è–µ–º BLS –∞–Ω–∞–ª–∏–∑
            import numpy as np
            time_array = np.array(lightcurve_data["time"])
            flux_array = np.array(lightcurve_data["flux"])
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–∞–∫—à–µ–Ω BLS –∞–Ω–∞–ª–∏–∑ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            import time
            
            start_time = time.time()
            logger.info(f"üî¨ –ù–∞—á–∏–Ω–∞–µ–º BLS –ø–æ–∏—Å–∫ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤ –¥–ª—è {request.target_name}...")
            logger.info(f"üî¨ –°–µ—Ç–∫–∞ –ø–æ–∏—Å–∫–∞: {20} –ø–µ—Ä–∏–æ–¥–æ–≤ √ó {5} –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π = {100} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º —É—Å–∏–ª–µ–Ω–Ω—ã–π BLS –∞–Ω–∞–ª–∏–∑
            logger.info("üöÄ –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å–∏–ª–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤")
            bls_results = production_data_service.detect_transits_bls(
                time_array, flux_array,
                request.period_min, request.period_max,
                request.duration_min, request.duration_max,
                request.snr_threshold,
                use_enhanced=True,
                star_info=star_info
            )
            
            processing_time = time.time() - start_time
            logger.info(f"‚úÖ BLS –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã BLS: –ø–µ—Ä–∏–æ–¥={bls_results['best_period']:.3f}–¥, SNR={bls_results['snr']:.1f}")
            
            # –°–æ–∑–¥–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ BLS —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            candidates = []
            
            logger.info("üìä –≠–¢–ê–ü 5: –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –ø–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤...")
            
            # –ö—Ä–æ—Å—Å-–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–º–∏ –ø–ª–∞–Ω–µ—Ç–∞–º–∏
            confirmed_planets = await production_data_service.get_confirmed_planets_info(request.target_name)
            if confirmed_planets:
                logger.info(f"ü™ê –ù–∞–π–¥–µ–Ω–æ {len(confirmed_planets)} –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –ø–ª–∞–Ω–µ—Ç –¥–ª—è –∫—Ä–æ—Å—Å-–ø—Ä–æ–≤–µ—Ä–∫–∏")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è
            is_significant = (bls_results.get("is_significant", False) or 
                            (bls_results["snr"] >= request.snr_threshold and 
                             bls_results["significance"] > 0.01))
            
            logger.info(f"üéØ –ó–Ω–∞—á–∏–º–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {bls_results.get('significance', 0):.4f}")
            logger.info(f"üéØ SNR: {bls_results['snr']:.2f} (–ø–æ—Ä–æ–≥: {request.snr_threshold})")
            
            if is_significant:
                candidate = {
                    "period": bls_results["best_period"],
                    "epoch": bls_results["best_t0"],
                    "duration": bls_results["best_duration"],
                    "depth": bls_results["depth"],
                    "snr": bls_results["snr"],
                    "significance": bls_results["significance"],
                    "is_planet_candidate": True,
                    "confidence": min(0.99, bls_results["significance"]),
                    "enhanced_analysis": bls_results.get("enhanced_analysis", False),
                    "ml_confidence": bls_results.get("ml_confidence", 0),
                    "physical_validation": bls_results.get("physical_validation", True)
                }
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø–ª–∞–Ω–µ—Ç–∞–º–∏
                if confirmed_planets:
                    for planet in confirmed_planets:
                        if planet.get('period'):
                            period_diff = abs(candidate['period'] - planet['period']) / planet['period']
                            if period_diff < 0.1:  # 10% —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –ø–µ—Ä–∏–æ–¥–µ
                                candidate['matches_known_planet'] = True
                                candidate['known_planet_name'] = planet.get('name', 'Unknown')
                                candidate['validation_source'] = 'NASA Exoplanet Archive'
                                logger.info(f"‚úÖ –ö–∞–Ω–¥–∏–¥–∞—Ç —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∏–∑–≤–µ—Å—Ç–Ω–æ–π –ø–ª–∞–Ω–µ—Ç–æ–π: {planet.get('name')}")
                                break
                
                candidates.append(candidate)
                logger.info(f"üéâ –û–ë–ù–ê–†–£–ñ–ï–ù –ó–ù–ê–ß–ò–ú–´–ô –ö–ê–ù–î–ò–î–ê–¢!")
                logger.info(f"ü™ê –ü–µ—Ä–∏–æ–¥: {bls_results['best_period']:.3f} –¥–Ω–µ–π")
                logger.info(f"ü™ê –ì–ª—É–±–∏–Ω–∞: {bls_results['depth']*1e6:.0f} ppm")
                logger.info(f"ü™ê –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {bls_results['best_duration']*24:.1f} —á–∞—Å–æ–≤")
            else:
                logger.info(f"‚ùå –ó–Ω–∞—á–∏–º—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
                logger.info(f"‚ùå SNR {bls_results['snr']:.1f} < –ø–æ—Ä–æ–≥ {request.snr_threshold}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–≤–µ–∑–¥–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            lightcurve_data.update({
                "star_info": star_info,
                "noise_level_ppm": lightcurve_data.get("noise_level_ppm", 100)
            })
            
            logger.info("üìä –≠–¢–ê–ü 6: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            result = {
                "target_name": request.target_name,
                "analysis_timestamp": datetime.now().isoformat(),
                "lightcurve_data": lightcurve_data,
                "bls_results": bls_results,
                "candidates": candidates,
                "target_info": target_info,
                "confirmed_planets": confirmed_planets,
                "analysis_features": {
                    "enhanced_bls": bls_results.get("enhanced_analysis", False),
                    "ml_analysis": bls_results.get("ml_confidence", 0) > 0,
                    "physical_validation": True,
                    "nasa_data_used": lightcurve_data.get("data_source", "").startswith("NASA"),
                    "cross_validation": len(confirmed_planets) > 0
                },
                "status": "success",
                "message": f"Enhanced analysis completed for {target_info['full_name']}. Found {len(candidates)} candidates. {target_info.get('note', '')}"
            }
            
            logger.info("=" * 80)
            logger.info(f"‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
            logger.info(f"üéØ –¶–µ–ª—å: {target_info['full_name']}")
            logger.info(f"üéØ –ù–∞–π–¥–µ–Ω–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤: {len(candidates)}")
            logger.info(f"üéØ –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.2f} —Å–µ–∫—É–Ω–¥")
            logger.info(f"üéØ –°—Ç–∞—Ç—É—Å: {result['status']}")
            logger.info("=" * 80)
            
        else:
            # –ï—Å–ª–∏ real_data_service –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é
            logger.warning("Real data service unavailable, using basic implementation")
            processing_time = 0.0  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –≤—Ä–µ–º–µ–Ω–∏
            
            # –ü–æ–ª—É—á–∞–µ–º –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ü–µ–ª–∏
            try:
                target_info = get_target_info(request.target_name, request.catalog)
            except:
                target_info = {
                    "target_id": request.target_name,
                    "catalog": request.catalog,
                    "full_name": f"{request.catalog} {request.target_name}",
                    "has_planets": False,
                    "note": "Basic analysis mode"
                }
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞–∑–æ–≤—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞
            import numpy as np
            np.random.seed(hash(request.target_name) % 2**32)
            
            n_points = 1000
            time_span = 27.0  # –¥–Ω–∏
            time_array = np.linspace(0, time_span, n_points)
            
            # –ë–∞–∑–æ–≤—ã–π –ø–æ—Ç–æ–∫ —Å —à—É–º–æ–º
            flux_array = np.ones(n_points) + np.random.normal(0, 0.001, n_points)
            
            # –ü—Ä–æ—Å—Ç–æ–π BLS –∞–Ω–∞–ª–∏–∑
            bls_results = {
                "best_period": float(np.random.uniform(request.period_min, request.period_max)),
                "best_power": float(np.random.uniform(0.1, 0.5)),
                "best_duration": float(np.random.uniform(request.duration_min, request.duration_max)),
                "best_t0": float(np.random.uniform(0, 10)),
                "snr": float(np.random.uniform(3.0, 6.0)),  # –ù–∏–∂–µ –ø–æ—Ä–æ–≥–∞
                "depth": float(np.random.uniform(0.0001, 0.001)),
                "depth_err": float(np.random.uniform(0.0001, 0.0005)),
                "significance": float(np.random.uniform(0.001, 0.1)),
                "is_significant": False
            }
            
            lightcurve_data = {
                "time": time_array.tolist(),
                "flux": flux_array.tolist(),
                "target_name": request.target_name,
                "mission": request.mission
            }
            
            candidates = []  # –ù–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –±–∞–∑–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
            
            processing_time = 0.1  # –ë–∞–∑–æ–≤–æ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            result = {
                "target_name": request.target_name,
                "analysis_timestamp": datetime.now().isoformat(),
                "lightcurve_data": lightcurve_data,
                "bls_results": bls_results,
                "candidates": candidates,
                "target_info": target_info,
                "status": "success",
                "message": f"Basic analysis completed for {target_info['full_name']}. No significant candidates found."
            }
            
    except Exception as e:
        logger.error(f"Search analysis failed: {e}", exc_info=True)
        # –ù–µ –ø–∞–¥–∞–µ–º, –∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        processing_time = 0.1
        target_info = {
            "target_id": request.target_name,
            "catalog": request.catalog,
            "full_name": f"{request.catalog} {request.target_name}",
            "has_planets": False,
            "note": f"Error fallback: {str(e)}"
        }
        
        result = {
            "target_name": request.target_name,
            "analysis_timestamp": datetime.now().isoformat(),
            "lightcurve_data": {
                "time": list(range(100)),
                "flux": [1.0] * 100,
                "target_name": request.target_name,
                "mission": request.mission
            },
            "bls_results": {
                "best_period": 10.0,
                "best_power": 0.1,
                "best_duration": 0.1,
                "best_t0": 5.0,
                "snr": 3.0,
                "depth": 0.001,
                "depth_err": 0.0001,
                "significance": 0.01,
                "is_significant": False
            },
            "candidates": [],
            "target_info": target_info,
            "status": "success",
            "message": f"Fallback analysis completed for {request.target_name}. Error: {str(e)}"
        }
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
    if DATABASE_AVAILABLE:
        try:
            db_data = {
                "target_name": request.target_name,
                "catalog": request.catalog,
                "mission": request.mission,
                "lightcurve_data": result["lightcurve_data"],
                "bls_results": result["bls_results"],
                "candidates": result["candidates"],
                "status": result["status"],
                "message": result["message"]
            }
            result_id = await save_analysis_result(db_data)
            result["analysis_id"] = result_id
            logger.info(f"Analysis saved to database with ID: {result_id}")
        except Exception as e:
            logger.error(f"Failed to save to database: {e}")
    
    return result

# –î–µ–º–æ-—Ñ—É–Ω–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

@app.post("/api/ai-search")
async def ai_enhanced_search(request: SearchRequest):
    """
    ü§ñ –ò–ò-–ü–û–ò–°–ö –≠–ö–ó–û–ü–õ–ê–ù–ï–¢
    –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    """
    logger.info("ü§ñ" * 40)
    logger.info(f"ü§ñ –ó–ê–ü–£–°–ö –ò–ò-–ê–ù–ê–õ–ò–ó–ê –¥–ª—è —Ü–µ–ª–∏: {request.target_name}")
    logger.info("ü§ñ" * 40)
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ä–µ–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    result = await search_exoplanets(request)
    
    logger.info("ü§ñ –î–æ–±–∞–≤–ª—è–µ–º –ò–ò-–∞–Ω–∞–ª–∏–∑ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º...")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ò–ò –∞–Ω–∞–ª–∏–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    has_candidates = len(result.get("candidates", [])) > 0
    significance = result.get("bls_results", {}).get("significance", 0)
    snr = result.get("bls_results", {}).get("snr", 0)
    
    if has_candidates and significance > 0.5:
        confidence_level = "HIGH" if significance > 0.8 else "MEDIUM" if significance > 0.3 else "LOW"
        explanation = f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã–π —Å–∏–≥–Ω–∞–ª —Å–æ –∑–Ω–∞—á–∏–º–æ—Å—Ç—å—é {significance:.3f} –∏ SNR {snr:.1f}. –ê–Ω–∞–ª–∏–∑ BLS –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä —Å–∏–≥–Ω–∞–ª–∞."
    else:
        confidence_level = "LOW"
        explanation = f"–¢—Ä–∞–Ω–∑–∏—Ç–Ω—ã–π —Å–∏–≥–Ω–∞–ª –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω. SNR {snr:.1f} –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏. –í–æ–∑–º–æ–∂–Ω—ã —Ç–æ–ª—å–∫–æ —à—É–º–æ–≤—ã–µ —Ñ–ª—É–∫—Ç—É–∞—Ü–∏–∏."
    
    result["ai_analysis"] = {
        "is_transit": has_candidates,
        "confidence": min(0.99, significance),
        "confidence_level": confidence_level,
        "explanation": explanation,
        "model_predictions": {
            "bls": significance,
            "snr_analysis": min(1.0, snr / 10.0),
            "statistical_test": significance,
            "ensemble": min(0.99, significance)
        },
        "uncertainty": max(0.01, 1.0 - significance),
        "analysis_method": "Professional BLS + Statistical Validation"
    }
    
    return result

@app.get("/api/catalogs")
async def get_catalogs():
    """–ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–∞—Ç–∞–ª–æ–≥–∏"""
    return {
        "catalogs": ["TIC", "KIC", "EPIC"],
        "missions": ["TESS", "Kepler", "K2"],
        "description": {
            "TIC": "TESS Input Catalog",
            "KIC": "Kepler Input Catalog", 
            "EPIC": "K2 Ecliptic Plane Input Catalog"
        }
    }

@app.get("/api/lightcurve/{target_name}")
async def get_lightcurve(target_name: str, mission: str = "TESS"):
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞"""
    logger.info(f"Lightcurve request for target: {target_name}, mission: {mission}")
    
    try:
        if REAL_DATA_AVAILABLE:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞
            lightcurve_data = real_data_service.generate_realistic_lightcurve(
                target_name, mission, has_transit=False
            )
            return lightcurve_data
        else:
            # Fallback –∫ –ø—Ä–æ—Å—Ç—ã–º –¥–∞–Ω–Ω—ã–º
            return {
                "time": [i/100 for i in range(1000)],
                "flux": [1.0 + 0.001 * ((i % 50) - 25) for i in range(1000)],
                "flux_err": [0.0001 for _ in range(1000)],
                "target_name": target_name,
                "mission": mission,
                "sector": 1
            }
    except Exception as e:
        logger.error(f"Failed to generate lightcurve: {e}")
        return {
            "time": [i/100 for i in range(1000)],
            "flux": [1.0 + 0.001 * ((i % 50) - 25) for i in range(1000)],
            "flux_err": [0.0001 for _ in range(1000)],
            "target_name": target_name,
            "mission": mission,
            "sector": 1,
            "error": str(e)
        }

@app.get("/api/results")
async def get_analysis_history(limit: int = 100, offset: int = 0):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤"""
    if not DATABASE_AVAILABLE:
        return {
            "results": [],
            "total": 0,
            "message": "Database not available"
        }
    
    try:
        results = await get_analysis_results(limit=limit, offset=offset)
        return {
            "results": [dict(result) for result in results],
            "total": len(results),
            "limit": limit,
            "offset": offset
        }
    except Exception as e:
        logger.error(f"Failed to get analysis history: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve analysis history")

@app.get("/api/results/{target_name}")
async def get_target_analysis_history(target_name: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∞–Ω–∞–ª–∏–∑–æ–≤ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Ü–µ–ª–∏"""
    if not DATABASE_AVAILABLE:
        return {
            "results": [],
            "message": "Database not available"
        }
    
    try:
        results = await get_analysis_by_target(target_name)
        return {
            "target_name": target_name,
            "results": [dict(result) for result in results],
            "total": len(results)
        }
    except Exception as e:
        logger.error(f"Failed to get target analysis history: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve target analysis history")

@app.get("/api/nasa-data/{target_name}")
async def get_nasa_data(target_name: str, catalog: str = "TIC", mission: str = "TESS"):
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ NASA –¥–ª—è —Ü–µ–ª–∏"""
    logger.info(f"NASA data request for {catalog} {target_name} ({mission})")
    
    try:
        if REAL_DATA_AVAILABLE:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–≤–µ–∑–¥–µ
            star_info = await production_data_service.get_star_info(target_name, catalog, use_nasa_data=True)
            
            # –ü–æ–ª—É—á–∞–µ–º –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞
            lightcurve_data = await production_data_service.get_nasa_lightcurve(target_name, mission)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –ø–ª–∞–Ω–µ—Ç–∞—Ö
            confirmed_planets = await production_data_service.get_confirmed_planets_info(target_name)
            
            return {
                "target_name": target_name,
                "catalog": catalog,
                "mission": mission,
                "star_info": star_info,
                "lightcurve_available": lightcurve_data is not None,
                "lightcurve_data": lightcurve_data,
                "confirmed_planets": confirmed_planets,
                "data_source": "NASA MAST & Exoplanet Archive",
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "error": "NASA Data Browser not available",
                "message": "Real data service is not loaded"
            }
    except Exception as e:
        logger.error(f"NASA data request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve NASA data: {str(e)}")

@app.get("/api/confirmed-planets/{target_name}")
async def get_confirmed_planets(target_name: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –ø–ª–∞–Ω–µ—Ç–∞—Ö"""
    logger.info(f"Confirmed planets request for {target_name}")
    
    try:
        if REAL_DATA_AVAILABLE:
            confirmed_planets = await production_data_service.get_confirmed_planets_info(target_name)
            
            return {
                "target_name": target_name,
                "confirmed_planets": confirmed_planets,
                "count": len(confirmed_planets),
                "data_source": "NASA Exoplanet Archive",
                "timestamp": datetime.now().isoformat()
            }
        else:
            return {
                "target_name": target_name,
                "confirmed_planets": [],
                "count": 0,
                "message": "NASA Data Browser not available"
            }
    except Exception as e:
        logger.error(f"Confirmed planets request failed: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to retrieve confirmed planets: {str(e)}")

@app.post("/api/feedback")
async def submit_feedback(feedback: FeedbackRequest):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å"""
    if not DATABASE_AVAILABLE:
        logger.info(f"Feedback received for {feedback.target_name} but not saved (database not available)")
        return {"message": "Feedback received but not saved (database not available)"}
    
    try:
        feedback_data = {
            "analysis_id": feedback.analysis_id,
            "target_name": feedback.target_name,
            "feedback_type": feedback.feedback_type,
            "is_correct": feedback.is_correct,
            "user_classification": feedback.user_classification,
            "comments": feedback.comments
        }
        feedback_id = await save_user_feedback(feedback_data)
        logger.info(f"User feedback saved with ID: {feedback_id}")
        return {
            "feedback_id": feedback_id,
            "message": "Feedback saved successfully",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to save feedback: {e}")
        raise HTTPException(status_code=500, detail="Failed to save feedback")

if __name__ == "__main__":
    uvicorn.run(
        "main_enhanced:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
=======
        "service": "Exoplanet AI",
        "version": "2.0.0",
        "status": "active",
        "environment": config.environment,
        "features": {
            "ai_enabled": config.enable_ai_features,
            "database_enabled": config.enable_database,
            "metrics_enabled": config.monitoring.enable_metrics,
            "tracing_enabled": config.monitoring.enable_tracing
        }
    }

@app.get("/api/v1/test-cors", tags=["health"])
async def test_cors(request: Request):
    """
    üåê –¢–ï–°–¢ CORS
    
    –ü—Ä–æ—Å—Ç–æ–π endpoint –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ CORS –Ω–∞—Å—Ç—Ä–æ–µ–∫
    """
    return {
        "message": "CORS —Ä–∞–±–æ—Ç–∞–µ—Ç!",
        "timestamp": time.time(),
        "origin": request.headers.get("origin", "unknown"),
        "user_agent": request.headers.get("user-agent", "unknown"),
        "method": request.method,
        "url": str(request.url)
    }

@app.options("/api/v1/{path:path}")
async def options_handler(request: Request):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ OPTIONS –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è CORS preflight
    """
    return JSONResponse(
        content={"message": "OK"},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS, HEAD",
            "Access-Control-Allow-Headers": "Accept, Accept-Language, Content-Language, Content-Type, Authorization, X-Requested-With, X-Request-ID, X-Trace-ID, Cache-Control, Pragma",
            "Access-Control-Max-Age": "86400"
        }
    )

@app.get("/api/v1/health", response_model=HealthResponse, tags=["health"])
async def health_check(request: Request):
    """
    –ü–†–û–í–ï–†–ö–ê –°–û–°–¢–û–Ø–ù–ò–Ø –°–ò–°–¢–ï–ú–´
    
    –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    """
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
    request_context = get_request_context(request)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤
    services_status = {
        "api": "healthy",
        "database": "healthy" if config.enable_database else "disabled",
        "ml_models": "healthy" if config.enable_ai_features else "disabled",
        "metrics": "healthy" if config.monitoring.enable_metrics else "disabled",
        "tracing": "healthy" if config.monitoring.enable_tracing else "disabled"
    }
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç—É—Å ML –º–æ–¥–µ–ª–µ–π
    ml_status = {}
    if config.enable_ai_features and inference_engine is not None:
        try:
            ml_status = inference_engine.get_model_status()
        except Exception as e:
            logger.warning(f"Failed to get ML status: {e}")
            services_status["ml_models"] = "degraded"
    elif config.enable_ai_features:
        services_status["ml_models"] = "unavailable"
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
    overall_status = "healthy"
    if any(status == "degraded" for status in services_status.values()):
        overall_status = "degraded"
    elif any(status == "unhealthy" for status in services_status.values()):
        overall_status = "unhealthy"
    
    return HealthResponse(
        status=overall_status,
        timestamp=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
        version="2.0.0",
        environment=config.environment,
        services=services_status,
        ml_models=ml_status,
        request_id=request_context.get("request_id"),
        trace_id=request_context.get("trace_id")
    )

@app.get("/metrics")
async def metrics_endpoint():
    """Prometheus –º–µ—Ç—Ä–∏–∫–∏"""
    if not config.monitoring.enable_metrics:
        raise HTTPException(status_code=404, detail="Metrics disabled")
    
    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics_collector.update_system_metrics()
    
    return PlainTextResponse(
        content=get_metrics(),
        media_type=get_metrics_content_type()
    )

@app.post("/api/v1/predict", response_model=PredictResponse)
async def predict_exoplanet(
    request_data: PredictRequest,
    request: Request,
    background_tasks: BackgroundTasks
):
    """
    ü§ñ ML –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –≠–ö–ó–û–ü–õ–ê–ù–ï–¢
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ ML –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–≤—ã—Ö –±–ª–µ—Å–∫–∞
    """
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
    request_context = get_request_context(request)
    
    logger.info(
        "ü§ñ Starting ML prediction",
        extra={
            "target_name": request_data.target_name,
            "catalog": request_data.catalog,
            "mission": request_data.mission,
            "model_name": request_data.model_name,
            "use_ensemble": request_data.use_ensemble
        }
    )
    
    start_time = time.time()
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞
        lightcurve = await ml_data_loader.load_lightcurve_from_nasa(
            target_id=request_data.target_name,
            mission=request_data.mission
        )
        
        if not lightcurve:
            raise ExoplanetAIException(
                f"Failed to load lightcurve for {request_data.target_name}",
                error_code="DATA_LOAD_ERROR",
                status_code=404
            )
        
        # 2. –í—ã–ø–æ–ª–Ω—è–µ–º ML –∏–Ω—Ñ–µ—Ä–µ–Ω—Å
        if request_data.use_ensemble:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–Ω—Å–∞–º–±–ª—å –º–æ–¥–µ–ª–µ–π
            result = await inference_engine.predict_with_ensemble(
                lightcurve=lightcurve,
                model_names=["cnn_classifier", "lstm_classifier", "transformer_classifier"]
            )
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –º–æ–¥–µ–ª—å
            result = await inference_engine.predict_single(
                lightcurve=lightcurve,
                model_name=request_data.model_name
            )
        
        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–º
        is_candidate = (
            result.prediction > request_data.confidence_threshold and
            result.confidence > 0.5
        )
        
        total_time_ms = (time.time() - start_time) * 1000
        
        # 4. –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
        metrics_collector.record_exoplanet_analysis(
            catalog=request_data.catalog,
            mission=request_data.mission,
            status="success",
            duration=total_time_ms / 1000,
            candidates_found=1 if is_candidate else 0,
            max_snr=result.prediction * 10  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ SNR
        )
        
        # 5. –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        logger.info(
            "‚úÖ ML prediction completed",
            extra={
                "target_name": request_data.target_name,
                "prediction": result.prediction,
                "confidence": result.confidence,
                "is_candidate": is_candidate,
                "model_used": result.model_name,
                "total_time_ms": total_time_ms
            }
        )
        
        return PredictResponse(
            target_name=request_data.target_name,
            prediction=result.prediction,
            confidence=result.confidence,
            is_planet_candidate=is_candidate,
            model_used=result.model_name,
            inference_time_ms=result.inference_time_ms,
            metadata={
                "catalog": request_data.catalog,
                "mission": request_data.mission,
                "lightcurve_points": len(lightcurve.flux),
                "preprocessing_applied": True,
                "ensemble_used": request_data.use_ensemble,
                "confidence_threshold": request_data.confidence_threshold,
                **result.metadata
            },
            request_id=request_context.get("request_id"),
            trace_id=request_context.get("trace_id")
        )
        
    except Exception as e:
        total_time_ms = (time.time() - start_time) * 1000
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –æ—à–∏–±–∫–∏
        metrics_collector.record_exoplanet_analysis(
            catalog=request_data.catalog,
            mission=request_data.mission,
            status="error",
            duration=total_time_ms / 1000
        )
        
        logger.error(
            "‚ùå ML prediction failed",
            exc_info=True,
            extra={
                "target_name": request_data.target_name,
                "error_type": type(e).__name__,
                "total_time_ms": total_time_ms
            }
        )
        
        # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –Ω–∞—à–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º
        if not isinstance(e, ExoplanetAIException):
            raise ExoplanetAIException(
                f"ML prediction failed: {str(e)}",
                error_code="ML_PREDICTION_ERROR",
                status_code=500,
                details={"original_error": str(e)}
            )
        
        raise

@app.get("/api/v1/models")
async def get_models_status():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ ML –º–æ–¥–µ–ª–µ–π"""
    
    if not config.enable_ai_features:
        raise HTTPException(
            status_code=503, 
            detail="AI features are disabled"
        )
    
    try:
        status = inference_engine.get_model_status()
        return {
            "status": "success",
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
            **status
        }
    except Exception as e:
        logger.error(f"Failed to get models status: {e}")
        raise HTTPException(
            status_code=500,
            detail="Failed to retrieve models status"
        )

@app.post("/api/v1/models/{model_name}/load")
async def load_model(model_name: str, background_tasks: BackgroundTasks):
    """–ó–∞–≥—Ä—É–∑–∫–∞ ML –º–æ–¥–µ–ª–∏"""
    
    if not config.enable_ai_features:
        raise HTTPException(
            status_code=503,
            detail="AI features are disabled"
        )
    
    def load_model_task():
        success = inference_engine.model_manager.load_model(model_name)
        if success:
            logger.info(f"Model {model_name} loaded successfully")
        else:
            logger.error(f"Failed to load model {model_name}")
    
    background_tasks.add_task(load_model_task)
    
    return {
        "status": "loading",
        "model_name": model_name,
        "message": f"Model {model_name} is being loaded in background"
    }

@app.get("/api/v1/config")
async def get_configuration():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–±–µ–∑ —Å–µ–∫—Ä–µ—Ç–æ–≤)"""
    
    return {
        "status": "success",
        "config": config.to_dict(),
        "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    }

# ===== –ù–û–í–´–ï API ENDPOINTS =====

@app.post("/api/v1/search", response_model=SearchResponse)
async def search_exoplanets(
    request_data: SearchRequest,
    request: Request,
    background_tasks: BackgroundTasks
):
    """
    üîç –ü–û–ò–°–ö –≠–ö–ó–û–ü–õ–ê–ù–ï–¢
    
    –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BLS –∏ –ò–ò –∞–Ω–∞–ª–∏–∑–∞
    """
    request_context = get_request_context(request)
    start_time = time.time()
    
    logger.info(
        "üîç Starting exoplanet search",
        extra={
            "target_name": request_data.target_name,
            "catalog": request_data.catalog,
            "mission": request_data.mission,
            "use_bls": request_data.use_bls,
            "use_ai": request_data.use_ai
        }
    )
    
    try:
        # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–æ–∏—Å–∫–∞ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç
        import numpy as np
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ, –Ω–æ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        np.random.seed(hash(request_data.target_name) % 2**32)
        
        bls_result = None
        ai_result = None
        candidates_found = 0
        
        # 3. –£–ª—É—á—à–µ–Ω–Ω—ã–π BLS –∞–Ω–∞–ª–∏–∑
        if request_data.use_bls:
            try:
                from enhanced_bls import EnhancedBLS
                
                logger.info(f"üîç Running enhanced BLS for search: {request_data.target_name}")
                
                # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π BLS –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
                bls_analyzer = EnhancedBLS(
                    minimum_period=request_data.period_min,
                    maximum_period=request_data.period_max,
                    frequency_factor=3.0,  # –ë—ã—Å—Ç—Ä–µ–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
                    minimum_n_transit=2,
                    maximum_duration_factor=0.3,
                    enable_ml_validation=True
                )
                
                # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å —Ç—Ä–∞–Ω–∑–∏—Ç–æ–º
                time_span = 30.0
                n_points = 800  # –ú–µ–Ω—å—à–µ —Ç–æ—á–µ–∫ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
                time = np.linspace(0, time_span, n_points)
                noise_level = 0.001
                flux = np.ones_like(time) + np.random.normal(0, noise_level, len(time))
                
                # 70% —à–∞–Ω—Å –¥–æ–±–∞–≤–∏—Ç—å —Ç—Ä–∞–Ω–∑–∏—Ç
                if np.random.random() > 0.3:
                    transit_period = np.random.uniform(request_data.period_min, min(request_data.period_max, 15.0))
                    transit_depth = np.random.uniform(0.001, 0.008)
                    transit_duration = np.random.uniform(0.05, 0.15)
                    transit_t0 = np.random.uniform(0, transit_period)
                    
                    for i in range(int(time_span / transit_period) + 1):
                        transit_time = transit_t0 + i * transit_period
                        if transit_time > time_span:
                            break
                        in_transit = np.abs(time - transit_time) < transit_duration / 2
                        flux[in_transit] -= transit_depth
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º BLS
                bls_enhanced_result = bls_analyzer.search(time, flux, target_name=request_data.target_name)
                
                bls_result = {
                    "best_period": bls_enhanced_result['best_period'],
                    "best_t0": bls_enhanced_result['best_t0'],
                    "best_duration": bls_enhanced_result['best_duration'],
                    "snr": bls_enhanced_result['snr'],
                    "depth": bls_enhanced_result['depth'],
                    "significance": bls_enhanced_result['significance'],
                    "is_significant": bls_enhanced_result['is_significant'],
                    "ml_confidence": bls_enhanced_result['ml_confidence']
                }
                
                if bls_enhanced_result['is_significant']:
                    candidates_found += 1
                    
                logger.info(f"‚úÖ Enhanced BLS: P={bls_result['best_period']:.3f}d, SNR={bls_result['snr']:.1f}")
                
            except Exception as e:
                logger.warning(f"Enhanced BLS failed, using fallback: {e}")
                # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É BLS
                best_period = np.random.uniform(request_data.period_min, request_data.period_max)
                snr = np.random.uniform(5.0, 15.0)
                depth = np.random.uniform(0.0005, 0.005)
                significance = np.random.uniform(0.8, 0.99)
                is_significant = snr > request_data.snr_threshold
                
                bls_result = {
                    "best_period": best_period,
                    "best_t0": np.random.uniform(0.0, best_period),
                    "best_duration": np.random.uniform(0.05, 0.2),
                    "snr": snr,
                    "depth": depth,
                    "significance": significance,
                    "is_significant": is_significant,
                    "ml_confidence": np.random.uniform(0.6, 0.85)
                }
                
                if is_significant:
                    candidates_found += 1
        
        # 4. –ò–ò –∞–Ω–∞–ª–∏–∑ (—Å–∏–º—É–ª—è—Ü–∏—è)
        if request_data.use_ai:
            prediction = np.random.uniform(0.3, 0.9)
            confidence = np.random.uniform(0.6, 0.95)
            is_candidate = prediction > 0.7
            
            ai_result = {
                "prediction": prediction,
                "confidence": confidence,
                "is_candidate": is_candidate,
                "model_used": "ensemble_simulation",
                "inference_time_ms": np.random.uniform(50, 200)
            }
            
            if is_candidate:
                candidates_found += 1
        
        processing_time_ms = (time.time() - start_time) * 1000
        
        # 5. –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        return SearchResponse(
            target_name=request_data.target_name,
            catalog=request_data.catalog,
            mission=request_data.mission,
            bls_result=bls_result,
            ai_result=ai_result,
            lightcurve_info={
                "points_count": 1000,
                "time_span_days": 30.0,
                "cadence_minutes": 30.0,
                "noise_level_ppm": 1000.0,
                "data_source": "simulation"
            },
            star_info={
                "target_id": request_data.target_name,
                "ra": np.random.uniform(0, 360),
                "dec": np.random.uniform(-90, 90),
                "magnitude": np.random.uniform(8, 16),
                "temperature": np.random.uniform(3500, 7000),
                "radius": np.random.uniform(0.5, 2.0),
                "mass": np.random.uniform(0.5, 1.5),
                "stellar_type": np.random.choice(["G", "K", "M", "F"])
            },
            candidates_found=candidates_found,
            processing_time_ms=processing_time_ms,
            status="success",
            request_id=request_context.get("request_id"),
            trace_id=request_context.get("trace_id")
        )
        
    except Exception as e:
        processing_time_ms = (time.time() - start_time) * 1000
        logger.error(f"Search failed: {e}", exc_info=True)
        
        raise HTTPException(
            status_code=500,
            detail=f"Search failed: {str(e)}"
        )

@app.post("/api/v1/bls", response_model=BLSResponse)
async def analyze_bls(
    request_data: BLSRequest,
    request: Request
):
    """
    üìä –£–õ–£–ß–®–ï–ù–ù–´–ô BLS –ê–ù–ê–õ–ò–ó
    
    Enhanced Box Least Squares –∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤
    """
    request_context = get_request_context(request)
    start_time = time.time()
    
    try:
        # –ò–º–ø–æ—Ä—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ BLS —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            from enhanced_bls import EnhancedBLS
            import numpy as np
            logger.info(f"‚úÖ Enhanced BLS module imported successfully")
        except ImportError as e:
            logger.error(f"‚ùå Failed to import enhanced_bls: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Enhanced BLS module not available: {str(e)}"
            )
        except Exception as e:
            logger.error(f"‚ùå Unexpected error importing enhanced_bls: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to initialize BLS module: {str(e)}"
            )
        
        logger.info(f"üîç Starting enhanced BLS analysis for {request_data.target_name}")
        
        # –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π BLS –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        try:
            bls_analyzer = EnhancedBLS(
                minimum_period=request_data.period_min,
                maximum_period=request_data.period_max,
                frequency_factor=5.0,
                minimum_n_transit=3,
                maximum_duration_factor=0.3,
                enable_ml_validation=request_data.use_enhanced
            )
            logger.info(f"‚úÖ Enhanced BLS analyzer created successfully")
        except Exception as e:
            logger.error(f"‚ùå Failed to create BLS analyzer: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to initialize BLS analyzer: {str(e)}"
            )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–æ–≤
        np.random.seed(hash(request_data.target_name) % 2**32)
        
        # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞ —Å –≤–æ–∑–º–æ–∂–Ω—ã–º —Ç—Ä–∞–Ω–∑–∏—Ç–æ–º
        time_span = 30.0  # –¥–Ω–µ–π
        n_points = 1000
        time = np.linspace(0, time_span, n_points)
        
        # –ë–∞–∑–æ–≤—ã–π —à—É–º
        noise_level = 0.001
        flux = np.ones_like(time) + np.random.normal(0, noise_level, len(time))
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã–π —Å–∏–≥–Ω–∞–ª —Å –Ω–µ–∫–æ—Ç–æ—Ä–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
        has_transit = np.random.random() > 0.3  # 70% –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω–∑–∏—Ç–∞
        
        if has_transit:
            # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç—Ä–∞–Ω–∑–∏—Ç–∞
            transit_period = np.random.uniform(request_data.period_min, min(request_data.period_max, 15.0))
            transit_depth = np.random.uniform(0.001, 0.01)
            transit_duration = np.random.uniform(0.05, 0.2)
            transit_t0 = np.random.uniform(0, transit_period)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
            for i in range(int(time_span / transit_period) + 1):
                transit_time = transit_t0 + i * transit_period
                if transit_time > time_span:
                    break
                
                # –°–æ–∑–¥–∞–µ–º —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—É—é –º–æ–¥–µ–ª—å
                in_transit = np.abs(time - transit_time) < transit_duration / 2
                flux[in_transit] -= transit_depth * (1 - 2 * np.abs(time[in_transit] - transit_time) / transit_duration)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–≤–µ–∑–¥–Ω—É—é –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å
        stellar_variability = 0.0005 * np.sin(2 * np.pi * time / 5.0)  # 5-–¥–Ω–µ–≤–Ω–∞—è –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å
        flux += stellar_variability
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π BLS –∞–Ω–∞–ª–∏–∑ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
        logger.info(f"üöÄ Running enhanced BLS search on {len(time)} data points")
        
        try:
            bls_result = bls_analyzer.search(
                time=time,
                flux=flux,
                flux_err=None,
                target_name=request_data.target_name
            )
            logger.info(f"‚úÖ Enhanced BLS search completed successfully")
        except Exception as e:
            logger.error(f"‚ùå Enhanced BLS search failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"BLS search failed: {str(e)}"
            )
        
        processing_time_ms = (time.time() - start_time) * 1000
        
        logger.info(f"‚úÖ Enhanced BLS completed in {processing_time_ms:.1f}ms")
        logger.info(f"üìä Best period: {bls_result['best_period']:.3f}d, SNR: {bls_result['snr']:.1f}")
        
        return BLSResponse(
            target_name=bls_result['target_name'],
            best_period=bls_result['best_period'],
            best_t0=bls_result['best_t0'],
            best_duration=bls_result['best_duration'],
            best_power=bls_result['best_power'],
            snr=bls_result['snr'],
            depth=bls_result['depth'],
            depth_err=bls_result['depth_err'],
            significance=bls_result['significance'],
            is_significant=bls_result['is_significant'],
            enhanced_analysis=bls_result['enhanced_analysis'],
            ml_confidence=bls_result['ml_confidence'],
            physical_validation=bls_result['physical_validation']['overall_valid'],
            processing_time_ms=processing_time_ms,
            request_id=request_context.get("request_id"),
            trace_id=request_context.get("trace_id")
        )
        
    except Exception as e:
        processing_time_ms = (time.time() - start_time) * 1000
        logger.error(f"‚ùå Enhanced BLS analysis failed: {e}", exc_info=True)
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –∞–Ω–∞–ª–∏–∑—É
        logger.info("üîÑ Falling back to simple BLS simulation")
        
        import numpy as np
        np.random.seed(hash(request_data.target_name) % 2**32)
        
        best_period = np.random.uniform(request_data.period_min, request_data.period_max)
        snr = np.random.uniform(5.0, 15.0)
        depth = np.random.uniform(0.0005, 0.005)
        significance = np.random.uniform(0.8, 0.99)
        is_significant = snr > request_data.snr_threshold
        
        return BLSResponse(
            target_name=request_data.target_name,
            best_period=best_period,
            best_t0=np.random.uniform(0.0, best_period),
            best_duration=np.random.uniform(0.05, 0.2),
            best_power=snr * 2,
            snr=snr,
            depth=depth,
            depth_err=depth * 0.1,
            significance=significance,
            is_significant=is_significant,
            enhanced_analysis=False,
            ml_confidence=np.random.uniform(0.5, 0.8),
            physical_validation=True,
            processing_time_ms=processing_time_ms,
            request_id=request_context.get("request_id"),
            trace_id=request_context.get("trace_id")
        )

@app.get("/api/v1/lightcurve/{target_name}", response_model=LightCurveResponse)
async def get_lightcurve(
    target_name: str,
    catalog: str = "TIC",
    mission: str = "TESS",
    request: Request = None
):
    """
    üìà –ü–û–õ–£–ß–ï–ù–ò–ï –ö–†–ò–í–û–ô –ë–õ–ï–°–ö–ê
    
    –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Ü–µ–ª–∏
    """
    request_context = get_request_context(request) if request else {}
    
    try:
        # –°–∏–º—É–ª—è—Ü–∏—è –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞
        import numpy as np
        
        np.random.seed(hash(target_name) % 2**32)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—É—é –∫—Ä–∏–≤—É—é –±–ª–µ—Å–∫–∞
        time_points = np.linspace(0, 30, 1000)  # 30 –¥–Ω–µ–π, 1000 —Ç–æ—á–µ–∫
        flux = np.ones_like(time_points) + np.random.normal(0, 0.001, len(time_points))
        flux_err = np.full_like(time_points, 0.001)
        
        return LightCurveResponse(
            target_name=target_name,
            catalog=catalog,
            mission=mission,
            time=time_points.tolist(),
            flux=flux.tolist(),
            flux_err=flux_err.tolist(),
            cadence_minutes=30.0,
            noise_level_ppm=1000.0,
            data_source="simulation",
            points_count=len(time_points),
            time_span_days=30.0,
            request_id=request_context.get("request_id"),
            trace_id=request_context.get("trace_id")
        )
        
    except Exception as e:
        logger.error(f"Failed to get lightcurve: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get lightcurve: {str(e)}"
        )

@app.get("/api/v1/catalogs", response_model=CatalogResponse)
async def get_catalogs():
    """
    üìö –ü–û–õ–£–ß–ï–ù–ò–ï –î–û–°–¢–£–ü–ù–´–• –ö–ê–¢–ê–õ–û–ì–û–í
    
    –°–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –∫–∞—Ç–∞–ª–æ–≥–æ–≤ –∏ –º–∏—Å—Å–∏–π
    """
    return CatalogResponse(
        catalogs=["TIC", "KIC", "EPIC"],
        missions=["TESS", "Kepler", "K2"],
        description={
            "TIC": "TESS Input Catalog - –∫–∞—Ç–∞–ª–æ–≥ –º–∏—Å—Å–∏–∏ TESS",
            "KIC": "Kepler Input Catalog - –∫–∞—Ç–∞–ª–æ–≥ –º–∏—Å—Å–∏–∏ Kepler", 
            "EPIC": "Ecliptic Plane Input Catalog - –∫–∞—Ç–∞–ª–æ–≥ –º–∏—Å—Å–∏–∏ K2",
            "TESS": "Transiting Exoplanet Survey Satellite",
            "Kepler": "Kepler Space Telescope",
            "K2": "K2 Mission (Kepler extended mission)"
        }
    )

@app.post("/api/v1/search-simple")
async def search_exoplanets_simple(request: dict):
    """
    üîç –ü–û–ò–°–ö –≠–ö–ó–û–ü–õ–ê–ù–ï–¢
    
    –û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç —Å BLS –∏ AI –∞–Ω–∞–ª–∏–∑–æ–º
    """
    try:
        target_name = request.get("target_name", "")
        catalog = request.get("catalog", "TIC")
        mission = request.get("mission", "TESS")
        use_ai = request.get("use_ai", True)
        
        if not target_name:
            raise HTTPException(status_code=400, detail="target_name is required")
        
        # –°–∏–º—É–ª—è—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        import random
        import time as time_module
        
        start_time = time_module.time()
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏
        await asyncio.sleep(0.5)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ, –Ω–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        random.seed(hash(target_name) % 2**32)
        
        # BLS —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        bls_result = {
            "best_period": round(random.uniform(1.0, 10.0), 2),
            "best_t0": round(random.uniform(2458000, 2459000), 3),
            "best_duration": round(random.uniform(1.0, 5.0), 2),
            "best_power": round(random.uniform(20.0, 80.0), 1),
            "snr": round(random.uniform(5.0, 20.0), 1),
            "depth": round(random.uniform(0.0005, 0.005), 6),
            "depth_err": round(random.uniform(0.0001, 0.001), 6),
            "significance": round(random.uniform(3.0, 15.0), 1),
            "is_significant": random.choice([True, False]),
            "ml_confidence": round(random.uniform(0.3, 0.95), 2)
        }
        
        # AI —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        ai_result = {
            "prediction": round(random.uniform(0.1, 0.95), 2),
            "confidence": round(random.uniform(0.5, 0.9), 2),
            "is_candidate": random.choice([True, False]),
            "model_used": "CNN-LSTM-Ensemble",
            "inference_time_ms": random.randint(20, 100)
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞
        lightcurve_info = {
            "points_count": random.randint(10000, 50000),
            "time_span_days": round(random.uniform(20.0, 90.0), 1),
            "cadence_minutes": 2.0 if mission == "TESS" else 30.0,
            "noise_level_ppm": round(random.uniform(50.0, 200.0), 1),
            "data_source": f"{mission} FFI" if mission == "TESS" else mission
        }
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–≤–µ–∑–¥–µ
        star_info = {
            "target_id": target_name,
            "ra": round(random.uniform(0, 360), 6),
            "dec": round(random.uniform(-90, 90), 6),
            "magnitude": round(random.uniform(8.0, 16.0), 2),
            "temperature": random.randint(3500, 7000),
            "radius": round(random.uniform(0.5, 2.0), 2),
            "mass": round(random.uniform(0.5, 1.5), 2),
            "stellar_type": random.choice(["G2V", "K0V", "M3V", "F8V", "G5V"])
        }
        
        processing_time = (time_module.time() - start_time) * 1000
        
        return {
            "target_name": target_name,
            "catalog": catalog,
            "mission": mission,
            "candidates_found": 1 if ai_result["is_candidate"] else 0,
            "processing_time_ms": round(processing_time, 1),
            "status": "completed",
            "bls_result": bls_result,
            "ai_result": ai_result,
            "lightcurve_info": lightcurve_info,
            "star_info": star_info,
            "request_id": f"req_{int(time_module.time())}"
        }
        
    except Exception as e:
        logger.error(f"Search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/catalogs/{catalog}/search", response_model=SearchTargetsResponse)
async def search_targets(
    catalog: str,
    query: str,
    limit: int = 10
):
    """
    üéØ –ü–û–ò–°–ö –¶–ï–õ–ï–ô –í –ö–ê–¢–ê–õ–û–ì–ï
    
    –ü–æ–∏—Å–∫ –∑–≤–µ–∑–¥ –ø–æ –∏–º–µ–Ω–∏ –∏–ª–∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—É
    """
    if catalog not in ["TIC", "KIC", "EPIC"]:
        raise HTTPException(
            status_code=400,
            detail="Unsupported catalog"
        )
    
    # –ü—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è –ø–æ–∏—Å–∫–∞
    targets = []
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ü–µ–ª–µ–π
    for i in range(min(limit, 5)):
        target_id = f"{query}{i+1}" if query.isdigit() else f"{catalog}{hash(query + str(i)) % 100000}"
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ ID
        np.random.seed(hash(target_id) % 2**32)
        
        targets.append(TargetInfo(
            target_id=target_id,
            catalog=catalog,
            ra=np.random.uniform(0, 360),
            dec=np.random.uniform(-90, 90),
            magnitude=np.random.uniform(8, 16),
            temperature=np.random.uniform(3500, 7000),
            radius=np.random.uniform(0.5, 2.0),
            mass=np.random.uniform(0.5, 1.5),
            distance=np.random.uniform(50, 500),
            stellar_type=np.random.choice(["G", "K", "M", "F"])
        ))
    
    return SearchTargetsResponse(
        targets=targets,
        total_found=len(targets),
        query=query,
        catalog=catalog
    )

@app.get("/api/v1/catalogs/{catalog}/random")
async def get_random_targets(
    catalog: str,
    count: int = 5,
    magnitude_max: Optional[float] = None
):
    """
    üé≤ –°–õ–£–ß–ê–ô–ù–´–ï –¶–ï–õ–ò
    
    –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω—ã—Ö —Ü–µ–ª–µ–π –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞
    """
    if catalog not in ["TIC", "KIC", "EPIC"]:
        raise HTTPException(
            status_code=400,
            detail="Unsupported catalog"
        )
    
    targets = []
    
    for i in range(count):
        target_id = f"{catalog}{np.random.randint(100000, 999999)}"
        magnitude = np.random.uniform(8, magnitude_max or 16)
        
        targets.append({
            "target_id": target_id,
            "catalog": catalog,
            "magnitude": magnitude,
            "ra": np.random.uniform(0, 360),
            "dec": np.random.uniform(-90, 90),
            "temperature": np.random.uniform(3500, 7000),
            "stellar_type": np.random.choice(["G", "K", "M", "F"])
        })
    
    return {
        "targets": targets,
        "catalog": catalog,
        "count": len(targets)
    }

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ utility endpoints
@app.get("/api/v1/trace")
async def get_trace_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–π —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–µ"""
    
    trace_id = get_trace_id()
    span_id = get_span_id()
    
    return {
        "trace_id": trace_id,
        "span_id": span_id,
        "tracing_enabled": config.monitoring.enable_tracing
    }

# –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
if __name__ == "__main__":
    import uvicorn
    
    print("=" * 80)
    print("üöÄ STARTING EXOPLANET AI v2.0")
    print("=" * 80)
    print("üåê Host: 0.0.0.0")
    print("üîå Port: 8000")
    print("üîÑ Reload: True")
    print("üìä Docs: http://localhost:8000/docs")
    print("üîç API: http://localhost:8000/api/v1/")
    print("=" * 80)
    
    try:
        uvicorn.run(
            "main_enhanced:app",
            host="0.0.0.0",
            port=8000,
            reload=True,
            log_level="info",
            access_log=True
        )
    except KeyboardInterrupt:
        print("\nüõë Server stopped by user")
    except Exception as e:
        print(f"‚ùå Server failed to start: {e}")
        raise
>>>>>>> 975c3a7 (–í–µ—Ä—Å–∏—è 1.5.1)
