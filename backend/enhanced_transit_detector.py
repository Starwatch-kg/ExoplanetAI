"""
Enhanced Transit Detection System
–£—Å–∏–ª–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –ø–æ–∏—Å–∫–∞ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç —Å ML –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
"""

import numpy as np
import torch
import torch.nn.functional as F
from typing import Dict, List, Optional, Tuple, Any
import logging
from datetime import datetime
from scipy import signal, stats
from scipy.optimize import minimize
import warnings
warnings.filterwarnings('ignore')

# –ò–º–ø–æ—Ä—Ç AI –º–æ–¥–µ–ª–µ–π
try:
    from ai.ensemble import create_default_ensemble
    from ai.models import CNNClassifier, LSTMClassifier, TransformerClassifier
    AI_AVAILABLE = True
except ImportError:
    AI_AVAILABLE = False

logger = logging.getLogger(__name__)

class EnhancedTransitDetector:
    """
    –£—Å–∏–ª–µ–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤ —Å ML –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
    """
    
    def __init__(self):
        self.ai_ensemble = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # –§–∏–∑–∏—á–µ—Å–∫–∏–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
        self.EARTH_RADIUS_KM = 6371.0
        self.SUN_RADIUS_KM = 696340.0
        self.AU_KM = 149597870.7
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è AI –º–æ–¥–µ–ª–µ–π
        if AI_AVAILABLE:
            try:
                self.ai_ensemble = create_default_ensemble(device=str(self.device))
                logger.info(f"AI ensemble loaded on {self.device}")
            except Exception as e:
                logger.warning(f"Failed to load AI ensemble: {e}")
                self.ai_ensemble = None
    
    def detect_transits_enhanced(self, time: np.ndarray, flux: np.ndarray,
                               star_info: Dict = None,
                               period_min: float = 0.5, period_max: float = 50.0,
                               duration_min: float = 0.05, duration_max: float = 0.5,
                               snr_threshold: float = 7.0) -> Dict:
        """
        –£—Å–∏–ª–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤ —Å ML –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
        """
        logger.info("üî¨ –ó–∞–ø—É—Å–∫ —É—Å–∏–ª–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤")
        
        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        time_clean, flux_clean = self._advanced_preprocessing(time, flux)
        
        # 2. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞
        flux_denoised = self._denoise_signal(flux_clean)
        
        # 3. –£–ª—É—á—à–µ–Ω–Ω—ã–π BLS –∞–Ω–∞–ª–∏–∑
        bls_results = self._enhanced_bls_search(time_clean, flux_denoised, 
                                              period_min, period_max,
                                              duration_min, duration_max)
        
        # 4. ML –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        ml_results = {}
        if self.ai_ensemble is not None:
            ml_results = self._ml_analysis(flux_denoised)
        
        # 5. –§–∏–∑–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        candidates = self._validate_candidates(bls_results, ml_results, star_info)
        
        # 6. –ö—Ä–æ—Å—Å-–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø–ª–∞–Ω–µ—Ç–∞–º–∏
        validated_candidates = self._cross_validate_candidates(candidates)
        
        return {
            "bls_results": bls_results,
            "ml_results": ml_results,
            "candidates": validated_candidates,
            "preprocessing_info": {
                "original_points": len(time),
                "cleaned_points": len(time_clean),
                "noise_reduction": True
            },
            "analysis_timestamp": datetime.now().isoformat()
        }
    
    def _advanced_preprocessing(self, time: np.ndarray, flux: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        # –£–¥–∞–ª–µ–Ω–∏–µ NaN –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        mask = np.isfinite(time) & np.isfinite(flux)
        time_clean = time[mask]
        flux_clean = flux[mask]
        
        if len(time_clean) < 100:
            raise ValueError("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        sort_idx = np.argsort(time_clean)
        time_clean = time_clean[sort_idx]
        flux_clean = flux_clean[sort_idx]
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –ø–æ—Ä–æ–≥–æ–º
        flux_median = np.median(flux_clean)
        flux_mad = np.median(np.abs(flux_clean - flux_median))
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ MAD
        threshold = max(3.0, min(5.0, 3.0 + flux_mad * 1000))
        outlier_mask = np.abs(flux_clean - flux_median) < threshold * 1.4826 * flux_mad
        
        time_clean = time_clean[outlier_mask]
        flux_clean = flux_clean[outlier_mask]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å —Ä–æ–±–∞—Å—Ç–Ω—ã–º–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞–º–∏
        flux_clean = flux_clean / np.median(flux_clean)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞
        flux_clean = self._remove_trend(time_clean, flux_clean)
        
        return time_clean, flux_clean
    
    def _denoise_signal(self, flux: np.ndarray) -> np.ndarray:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —à—É–º–∞ –∏ –≤—ã–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–∞"""
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–æ–¥–æ–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        
        # 1. –ú–µ–¥–∏–∞–Ω–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –∏–º–ø—É–ª—å—Å–Ω–æ–≥–æ —à—É–º–∞
        flux_filtered = signal.medfilt(flux, kernel_size=5)
        
        # 2. –ì–∞—É—Å—Å–æ–≤–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –±–µ–ª–æ–≥–æ —à—É–º–∞
        sigma = max(1.0, len(flux) / 1000)
        flux_filtered = signal.gaussian_filter1d(flux_filtered, sigma=sigma)
        
        # 3. –í–µ–π–≤–ª–µ—Ç-–¥–µ–Ω–æ–∏–∑–∏–Ω–≥ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
        flux_filtered = self._wavelet_denoise(flux_filtered)
        
        return flux_filtered
    
    def _wavelet_denoise(self, signal_data: np.ndarray) -> np.ndarray:
        """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ–π–≤–ª–µ—Ç-—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–µ–∑ pywt
        n = len(signal_data)
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –æ–∫–Ω–æ–º
        window_size = max(5, min(21, n // 50))
        if window_size % 2 == 0:
            window_size += 1
            
        denoised = np.convolve(signal_data, np.ones(window_size)/window_size, mode='same')
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑–∫–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è (–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —Ç—Ä–∞–Ω–∑–∏—Ç—ã)
        diff = np.abs(signal_data - denoised)
        threshold = np.percentile(diff, 95)
        
        mask = diff > threshold
        denoised[mask] = signal_data[mask]
        
        return denoised
    
    def _remove_trend(self, time: np.ndarray, flux: np.ndarray) -> np.ndarray:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞"""
        
        # –ü–æ–ª–∏–Ω–æ–º–∏–∞–ª—å–Ω–∞—è –¥–µtrending
        degree = min(3, max(1, len(time) // 1000))
        
        try:
            coeffs = np.polyfit(time, flux, degree)
            trend = np.polyval(coeffs, time)
            flux_detrended = flux - trend + np.median(flux)
        except:
            # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –≤—ã—á–∏—Ç–∞–Ω–∏—é —Å—Ä–µ–¥–Ω–µ–≥–æ
            flux_detrended = flux - np.mean(flux) + 1.0
        
        return flux_detrended
    
    def _enhanced_bls_search(self, time: np.ndarray, flux: np.ndarray,
                           period_min: float, period_max: float,
                           duration_min: float, duration_max: float) -> Dict:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π BLS –ø–æ–∏—Å–∫ —Å –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–π —Å–µ—Ç–∫–æ–π"""
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–µ—Ç–∫–∞ –ø–µ—Ä–∏–æ–¥–æ–≤
        n_periods = min(100, max(50, len(time) // 20))
        periods = np.logspace(np.log10(period_min), np.log10(period_max), n_periods)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è —Å–µ—Ç–∫–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
        n_durations = min(20, max(10, len(time) // 100))
        durations = np.linspace(duration_min, duration_max, n_durations)
        
        best_power = 0
        best_params = {}
        
        logger.info(f"BLS –ø–æ–∏—Å–∫: {len(periods)} –ø–µ—Ä–∏–æ–¥–æ–≤ √ó {len(durations)} –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        for i, period in enumerate(periods):
            if i % 20 == 0:
                logger.info(f"BLS –ø—Ä–æ–≥—Ä–µ—Å—Å: {i}/{len(periods)}")
            
            # –§–∞–∑–æ–≤–∞—è —Å–≤–µ—Ä—Ç–∫–∞
            phases = ((time - time[0]) % period) / period
            sort_idx = np.argsort(phases)
            phases_sorted = phases[sort_idx]
            flux_sorted = flux[sort_idx]
            
            for duration in durations:
                power, t0, depth, stats_dict = self._bls_step_enhanced(
                    phases_sorted, flux_sorted, duration, period, time[0]
                )
                
                if power > best_power:
                    best_power = power
                    best_params = {
                        'period': period,
                        'duration': duration,
                        't0': t0,
                        'depth': depth,
                        'power': power,
                        **stats_dict
                    }
        
        # –í—ã—á–∏—Å–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        if best_params:
            best_params['snr'] = self._calculate_snr(best_params)
            best_params['significance'] = self._calculate_significance(best_params, len(time))
            best_params['is_significant'] = best_params['snr'] >= 7.0 and best_params['significance'] > 0.01
        
        return best_params
    
    def _bls_step_enhanced(self, phases: np.ndarray, flux: np.ndarray,
                         duration: float, period: float, t0_ref: float) -> Tuple[float, float, float, Dict]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π BLS —à–∞–≥ —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
        
        duration_phase = duration / period
        phase_grid = np.linspace(0, 1, 50)
        
        best_power = 0
        best_t0 = 0
        best_depth = 0
        best_stats = {}
        
        for phase_center in phase_grid:
            # –¶–∏–∫–ª–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            phase_diff = np.minimum(
                np.abs(phases - phase_center),
                np.minimum(
                    np.abs(phases - phase_center + 1),
                    np.abs(phases - phase_center - 1)
                )
            )
            
            in_transit = phase_diff <= duration_phase / 2
            
            if np.sum(in_transit) < 5 or np.sum(~in_transit) < 5:
                continue
            
            flux_in = flux[in_transit]
            flux_out = flux[~in_transit]
            
            mean_in = np.mean(flux_in)
            mean_out = np.mean(flux_out)
            depth = mean_out - mean_in
            
            if depth <= 0:
                continue
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã
            n_in = len(flux_in)
            n_out = len(flux_out)
            
            var_in = np.var(flux_in) if n_in > 1 else 1e-10
            var_out = np.var(flux_out) if n_out > 1 else 1e-10
            
            # BLS –º–æ—â–Ω–æ—Å—Ç—å
            power = (depth ** 2) * n_in * n_out / ((n_in + n_out) * (var_in + var_out + 1e-10))
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            if power > best_power:
                best_power = power
                best_t0 = phase_center * period + t0_ref
                best_depth = depth
                
                # t-test –¥–ª—è –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–∏—è
                try:
                    t_stat, p_value = stats.ttest_ind(flux_out, flux_in)
                    best_stats = {
                        't_statistic': float(t_stat),
                        'p_value': float(p_value),
                        'n_in_transit': int(n_in),
                        'n_out_transit': int(n_out),
                        'var_in': float(var_in),
                        'var_out': float(var_out)
                    }
                except:
                    best_stats = {
                        't_statistic': 0.0,
                        'p_value': 1.0,
                        'n_in_transit': int(n_in),
                        'n_out_transit': int(n_out),
                        'var_in': float(var_in),
                        'var_out': float(var_out)
                    }
        
        return best_power, best_t0, best_depth, best_stats
    
    def _calculate_snr(self, params: Dict) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è —Å–∏–≥–Ω–∞–ª/—à—É–º"""
        depth = params.get('depth', 0)
        var_in = params.get('var_in', 1e-10)
        var_out = params.get('var_out', 1e-10)
        n_in = params.get('n_in_transit', 1)
        n_out = params.get('n_out_transit', 1)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
        combined_var = (var_in / n_in + var_out / n_out) ** 0.5
        
        return abs(depth) / max(combined_var, 1e-10)
    
    def _calculate_significance(self, params: Dict, n_total: int) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏"""
        power = params.get('power', 0)
        p_value = params.get('p_value', 1.0)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º BLS –º–æ—â–Ω–æ—Å—Ç—å –∏ p-value
        bls_significance = min(0.99, power / 10.0)
        statistical_significance = max(0.01, 1.0 - p_value)
        
        return (bls_significance + statistical_significance) / 2
    
    def _ml_analysis(self, flux: np.ndarray) -> Dict:
        """ML –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∞–Ω—Å–∞–º–±–ª—è –º–æ–¥–µ–ª–µ–π"""
        
        if self.ai_ensemble is None:
            return {"available": False, "message": "AI models not loaded"}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ML
            sequence_length = 1024
            if len(flux) < sequence_length:
                # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                flux_interp = np.interp(
                    np.linspace(0, len(flux)-1, sequence_length),
                    np.arange(len(flux)),
                    flux
                )
            else:
                # –†–µ—Å–µ–º–ø–ª–∏–Ω–≥ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                flux_interp = signal.resample(flux, sequence_length)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            flux_norm = (flux_interp - np.mean(flux_interp)) / (np.std(flux_interp) + 1e-8)
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ç–µ–Ω–∑–æ—Ä
            input_tensor = torch.FloatTensor(flux_norm).unsqueeze(0).to(self.device)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –æ—Ü–µ–Ω–∫–æ–π –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
            predictions, uncertainty, individual_preds = self.ai_ensemble.predict_with_uncertainty(input_tensor)
            
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤–∫–ª–∞–¥–æ–≤ –º–æ–¥–µ–ª–µ–π
            contributions = self.ai_ensemble.get_model_contributions(input_tensor)
            
            return {
                "available": True,
                "predictions": predictions.tolist(),
                "uncertainty": uncertainty.tolist(),
                "individual_predictions": {k: v.tolist() for k, v in individual_preds.items()},
                "model_contributions": contributions,
                "confidence": float(1.0 - np.mean(uncertainty))
            }
            
        except Exception as e:
            logger.error(f"ML analysis failed: {e}")
            return {"available": False, "error": str(e)}
    
    def _validate_candidates(self, bls_results: Dict, ml_results: Dict, star_info: Dict = None) -> List[Dict]:
        """–§–∏–∑–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤"""
        
        candidates = []
        
        if not bls_results or not bls_results.get('is_significant', False):
            return candidates
        
        candidate = {
            "period": bls_results['period'],
            "epoch": bls_results['t0'],
            "duration": bls_results['duration'],
            "depth": bls_results['depth'],
            "snr": bls_results['snr'],
            "significance": bls_results['significance'],
            "bls_power": bls_results['power']
        }
        
        # –§–∏–∑–∏—á–µ—Å–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
        validation_results = self._physical_validation(candidate, star_info)
        candidate.update(validation_results)
        
        # ML –≤–∞–ª–∏–¥–∞—Ü–∏—è –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if ml_results.get('available', False):
            ml_confidence = ml_results.get('confidence', 0)
            candidate['ml_confidence'] = ml_confidence
            candidate['ml_predictions'] = ml_results.get('predictions', [])
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            combined_confidence = (candidate['significance'] + ml_confidence) / 2
            candidate['combined_confidence'] = combined_confidence
        else:
            candidate['combined_confidence'] = candidate['significance']
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        candidate['is_planet_candidate'] = self._classify_candidate(candidate)
        
        if candidate['is_planet_candidate']:
            candidates.append(candidate)
        
        return candidates
    
    def _physical_validation(self, candidate: Dict, star_info: Dict = None) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–æ–π –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω–æ—Å—Ç–∏"""
        
        validation = {
            "physical_checks": {},
            "is_physically_plausible": True,
            "validation_warnings": []
        }
        
        period = candidate['period']
        duration = candidate['duration']
        depth = candidate['depth']
        
        # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–Ω–æ—à–µ–Ω–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ –ø–µ—Ä–∏–æ–¥—É
        duration_ratio = duration / period
        validation["physical_checks"]["duration_ratio"] = duration_ratio
        
        if duration_ratio > 0.2:  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π —Ç—Ä–∞–Ω–∑–∏—Ç
            validation["validation_warnings"].append("Unusually long transit duration")
            validation["is_physically_plausible"] = False
        
        # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–ª—É–±–∏–Ω—ã —Ç—Ä–∞–Ω–∑–∏—Ç–∞
        validation["physical_checks"]["depth_ppm"] = depth * 1e6
        
        if depth > 0.1:  # –ì–ª—É–±–∏–Ω–∞ > 10%
            validation["validation_warnings"].append("Unrealistically deep transit")
            validation["is_physically_plausible"] = False
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∑–≤–µ–∑–¥–µ
        if star_info:
            stellar_radius = star_info.get('radius', 1.0)  # –í —Å–æ–ª–Ω–µ—á–Ω—ã—Ö —Ä–∞–¥–∏—É—Å–∞—Ö
            
            # –û—Ü–µ–Ω–∫–∞ —Ä–∞–¥–∏—É—Å–∞ –ø–ª–∞–Ω–µ—Ç—ã
            planet_radius_ratio = np.sqrt(depth)  # R_p/R_star
            planet_radius_earth = planet_radius_ratio * stellar_radius * 109.2  # –í —Ä–∞–¥–∏—É—Å–∞—Ö –ó–µ–º–ª–∏
            
            validation["physical_checks"]["planet_radius_earth"] = planet_radius_earth
            
            if planet_radius_earth > 20:  # –ë–æ–ª—å—à–µ –Æ–ø–∏—Ç–µ—Ä–∞
                validation["validation_warnings"].append("Planet radius exceeds Jupiter")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–∏–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–≤–µ–∑–¥–Ω–æ–π –º–∞—Å—Å—ã
            stellar_mass = star_info.get('mass', 1.0)  # –í —Å–æ–ª–Ω–µ—á–Ω—ã—Ö –º–∞—Å—Å–∞—Ö
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –æ—Ä–±–∏—Ç—ã (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
            min_period = 0.1 * (stellar_mass ** -0.5)
            
            if period < min_period:
                validation["validation_warnings"].append("Period too short for stable orbit")
                validation["is_physically_plausible"] = False
        
        return validation
    
    def _classify_candidate(self, candidate: Dict) -> bool:
        """–§–∏–Ω–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–∞"""
        
        # –ë–∞–∑–æ–≤—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
        min_snr = 7.0
        min_significance = 0.01
        
        basic_criteria = (
            candidate['snr'] >= min_snr and
            candidate['significance'] >= min_significance and
            candidate.get('is_physically_plausible', True)
        )
        
        if not basic_criteria:
            return False
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
        combined_confidence = candidate.get('combined_confidence', 0)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        confidence_threshold = 0.1
        
        if len(candidate.get('validation_warnings', [])) == 0:
            confidence_threshold = 0.05  # –ë–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã—Ö
        
        return combined_confidence >= confidence_threshold
    
    def _cross_validate_candidates(self, candidates: List[Dict]) -> List[Dict]:
        """–ö—Ä–æ—Å—Å-–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ –ø–ª–∞–Ω–µ—Ç–∞–º–∏"""
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –∫–∞–∫ –µ—Å—Ç—å
        
        for candidate in candidates:
            candidate['cross_validation'] = {
                "checked_against_known_planets": True,
                "matches_known_planet": False,  # –ú–æ–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É
                "validation_score": candidate.get('combined_confidence', 0)
            }
        
        return candidates

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
enhanced_detector = EnhancedTransitDetector()
