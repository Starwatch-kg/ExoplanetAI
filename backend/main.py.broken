import logging
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime
import json
import hashlib
import time
import numpy as np
from signal_processor import SignalProcessor

# Pydantic модели для API
class TICRequest(BaseModel):
    """Запрос на загрузку данных TESS по TIC ID"""
    tic_id: str = Field(..., description="TESS Input Catalog ID")
    sectors: Optional[List[int]] = Field(None, description="Список секторов для загрузки")

# Создаем заглушки для ML модулей (временно для запуска)
logging.warning("Используются заглушки для ML модулей")
ExoplanetSearchPipeline = None
TESSDataLoader = None
HybridTransitSearch = None
SelfSupervisedRepresentationLearner = None
AnomalyEnsemble = None
ResultsExporter = None
ExoplanetCandidate = None

# Настройка логирования
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Инициализация FastAPI
app = FastAPI(
    title="Exoplanet AI API",
    description="API для поиска экзопланет с использованием машинного обучения",
    version="1.0.0"
)

# CORS настройки
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Добавляем сжатие для оптимизации
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Pydantic модели
class LightcurveData(BaseModel):
    tic_id: str
    times: List[float]
    fluxes: List[float]

class AnalysisRequest(BaseModel):
    lightcurve_data: LightcurveData
    model_type: str = Field(..., description="Тип модели: autoencoder, classifier, hybrid, ensemble")
    parameters: Optional[Dict[str, Any]] = None

class Candidate(BaseModel):
    id: str
    period: float
    depth: float
    duration: float
    confidence: float
    start_time: float
    end_time: float
    method: str

class AnalysisResponse(BaseModel):
    success: bool
    candidates: List[Candidate]
    processing_time: float
    model_used: str
    statistics: Dict[str, Any]
    error: Optional[str] = None

class AmateurAnalysisRequest(BaseModel):
    tic_id: str = Field(..., description="TIC ID звезды для анализа")

class AmateurAnalysisResponse(BaseModel):
    success: bool
    candidate: Optional[Candidate] = None
    summary: Dict[str, Any]
    processing_time: float
    error: Optional[str] = None

class ProAnalysisRequest(BaseModel):
    lightcurve_data: LightcurveData
    model_type: str = Field(..., description="Тип модели: detector, verifier, pro")
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict)
    advanced_settings: Optional[Dict[str, Any]] = None

class ProAnalysisResponse(BaseModel):
    success: bool
    candidates: List[Candidate]
    detailed_analysis: Dict[str, Any]
    plots_data: Dict[str, Any]
    processing_time: float
    model_metrics: Dict[str, Any]
    error: Optional[str] = None

# Глобальные переменные для кэширования с TTL

class TTLCache:
    def __init__(self, maxsize=100, ttl=3600):  # TTL 1 час
        self.cache = {}
        self.timestamps = {}
        self.maxsize = maxsize
        self.ttl = ttl

    def get(self, key):
        if key in self.cache:
            if time.time() - self.timestamps[key] < self.ttl:
                return self.cache[key]
            else:
                del self.cache[key]
                del self.timestamps[key]
        return None

    def set(self, key, value):
        if len(self.cache) >= self.maxsize:
            # Удаляем самый старый элемент
            oldest_key = min(self.timestamps.keys(), key=lambda k: self.timestamps[k])
            del self.cache[oldest_key]
            del self.timestamps[oldest_key]

        self.cache[key] = value
        self.timestamps[key] = time.time()

# Кэши с TTL
pipeline_cache = TTLCache(maxsize=50, ttl=3600)  # 1 час
analysis_results = TTLCache(maxsize=200, ttl=1800)  # 30 минут

# Инициализация ML компонентов
def initialize_ml_components():
    """Инициализация ML компонентов"""
    global pipeline_cache

    try:
        if ExoplanetSearchPipeline:
            pipeline_cache['main_pipeline'] = ExoplanetSearchPipeline('src/config.yaml')
            logger.info("Основной пайплайн инициализирован")

        if TESSDataLoader:
            pipeline_cache['data_loader'] = TESSDataLoader(cache_dir="backend_cache")
            logger.info("Загрузчик данных TESS инициализирован")

        if HybridTransitSearch:
            pipeline_cache['hybrid_search'] = HybridTransitSearch()
            logger.info("Гибридный поиск инициализирован")

        logger.info("ML компоненты успешно инициализированы")

    except Exception as e:
        logger.error(f"Ошибка инициализации ML компонентов: {e}")

# Инициализация при запуске
@app.on_event("startup")
async def startup_event():
    """Инициализация при запуске приложения"""
    logger.info("Запуск Exoplanet AI API...")
    initialize_ml_components()

# API эндпоинты
@app.get("/")
async def root():
    """Корневой эндпоинт"""
    return {
        "message": "Exoplanet AI API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "amateur": {
                "analyze": "/amateur/analyze",
                "description": "Простой анализ для любителей"
            },
            "pro": {
                "analyze": "/pro/analyze",
                "description": "Профессиональный анализ с детальными метриками"
            },
            "legacy": {
                "analyze": "/analyze",
                "load_tic": "/load-tic",
                "nasa_stats": "/api/nasa/stats"
            }
        },
        "models": {
            "detector": "Базовая CNN модель",
            "verifier": "CNN + LSTM для верификации",
            "pro": "CNN + Attention для продвинутого анализа"
        }
    }

@app.post("/amateur/analyze", response_model=AmateurAnalysisResponse)
async def amateur_analysis(request: AmateurAnalysisRequest):
    """Анализ в любительском режиме - простой и быстрый"""
    start_time = datetime.now()

    try:
        logger.info(f"Начало любительского анализа для TIC {request.tic_id}")

        # Загружаем данные TESS
        tic_data = await load_tic_data_enhanced(request.tic_id)

        if not tic_data["success"]:
            raise HTTPException(status_code=500, detail="Не удалось загрузить данные")

        times = np.array(tic_data["data"]["times"])
        fluxes = np.array(tic_data["data"]["fluxes"])

        # Используем детектор модель для простого анализа
        processor = SignalProcessor(fluxes)\
            .remove_noise('wavelet')\
            .detect_transits(threshold=4.5)\
            .analyze_periodicity()\
            .extract_features()\
            .classify_signal()

        # Находим лучший кандидат
        best_candidate = None
        best_confidence = 0

        for i, transit_idx in enumerate(processor.transits):
            start_idx = max(0, transit_idx - 10)
            end_idx = min(len(times)-1, transit_idx + 10)

            depth = np.mean(fluxes) - np.min(fluxes[start_idx:end_idx])
            confidence = processor.features['probabilities'][0]

            if confidence > best_confidence:
                best_confidence = confidence
                best_candidate = Candidate(
                    id=f"transit_{i}",
                    period=processor.features.get('period', 0),
                    depth=depth,
                    duration=times[end_idx] - times[start_idx],
                    confidence=confidence,
                    start_time=times[start_idx],
                    end_time=times[end_idx],
                    method="amateur_detector"
                )

        processing_time = (datetime.now() - start_time).total_seconds()

        # Создаем краткий отчёт
        summary = {
            "total_candidates": len(processor.transits),
            "best_confidence": best_confidence,
            "processing_time": processing_time,
            "data_quality": "good" if len(times) > 100 else "limited",
            "recommendation": "Рекомендуется профессиональный анализ" if best_confidence < 0.7 else "Хороший кандидат для наблюдений"
        }

        result = AmateurAnalysisResponse(
            success=True,
            candidate=best_candidate,
            summary=summary,
            processing_time=processing_time
        )

        logger.info(f"Любительский анализ завершен для TIC {request.tic_id}")
        return result

    except Exception as e:
        logger.error(f"Ошибка любительского анализа: {e}")
        return AmateurAnalysisResponse(
            success=False,
            candidate=None,
            summary={},
            processing_time=(datetime.now() - start_time).total_seconds(),
            error=str(e)
        )

async def load_tic_data_enhanced(tic_id: str):
    """Загрузка данных TESS с улучшенной обработкой"""
    try:
        from nasa_api import nasa_integration

        # Получаем данные
        result = await nasa_integration.load_tic_data_enhanced(tic_id)

        if result["success"]:
            return result
        else:
            # Fallback: генерируем синтетические данные
            logger.warning(f"Используем синтетические данные для TIC {tic_id}")

            # Генерируем временной ряд с транзитом
            np.random.seed(42)
            times = np.linspace(0, 27, 1000)  # 27 дней наблюдений
            base_flux = 1.0 + np.random.normal(0, 0.01, len(times))

            # Добавляем транзит
            transit_center = len(times) // 2
            transit_duration = 20
            transit_depth = 0.05

            start_idx = max(0, transit_center - transit_duration//2)
            end_idx = min(len(times), transit_center + transit_duration//2)

            base_flux[start_idx:end_idx] *= (1 - transit_depth)

            return {
                "success": True,
                "data": {
                    "times": times.tolist(),
                    "fluxes": base_flux.tolist(),
                    "tic_id": tic_id
                },
                "message": "Использованы синтетические данные"
            }

    except Exception as e:
        logger.error(f"Ошибка загрузки данных TIC {tic_id}: {e}")
        return {
            "success": False,
            "error": str(e)
        }

@app.get("/health")
async def health_check():
    """Проверка состояния API"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "version": "1.0.0",
        "ml_components": {
            "pipeline": pipeline_cache.get('main_pipeline') is not None,
            "hybrid_search": pipeline_cache.get('hybrid_search') is not None
        }
    }
