"""
ULTIMATE ENSEMBLE SEARCH ENGINE v7.0
–ù–µ—Ä–µ–∞–ª—å–Ω–æ –º–æ—â–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –º–µ—Ç–æ–¥—ã: BLS, GPI, AI, TLS, Wavelet, Fourier, Machine Learning
"""

import warnings
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field

import numpy as np
from scipy import optimize, signal, stats
from sklearn.cluster import DBSCAN, KMeans
from sklearn.decomposition import PCA, FastICA
from sklearn.ensemble import (
    ExtraTreesRegressor,
    GradientBoostingRegressor,
    RandomForestRegressor,
    StackingRegressor,
    VotingRegressor,
)
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import RobustScaler, StandardScaler
from sklearn.svm import SVR

warnings.filterwarnings("ignore")

from core.logging_config import get_logger
from services.bls_service import BLSResult, BLSService
from services.gpi_service import GPIService

logger = get_logger(__name__)


@dataclass
class EnsembleSearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Å—É–ø–µ—Ä-–º–æ—â–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞"""

    target_name: str

    # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    consensus_period: float
    consensus_confidence: float
    consensus_depth: float
    consensus_snr: float

    # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤
    bls_result: Optional[BLSResult]
    gpi_result: Optional[Dict]
    tls_result: Optional[Dict]
    wavelet_result: Optional[Dict]
    fourier_result: Optional[Dict]
    ml_result: Optional[Dict]

    # –ê–Ω—Å–∞–º–±–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    method_agreement: float
    ensemble_uncertainty: float
    cross_validation_score: float
    bootstrap_confidence: float

    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    planet_candidates: List[Dict]
    false_positive_probability: float
    habitability_score: float
    detection_significance: float

    # –§–∏–∑–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    stellar_parameters: Dict[str, float]
    planetary_system: Dict[str, Any]
    orbital_dynamics: Dict[str, float]

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    processing_time: float
    methods_used: List[str]
    quality_flags: Dict[str, bool]
    recommendations: List[str]


class UltimateEnsembleSearchEngine:
    """–ù–µ—Ä–µ–∞–ª—å–Ω–æ –º–æ—â–Ω—ã–π –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –¥–≤–∏–∂–æ–∫"""

    def __init__(self):
        self.initialized = False

        # –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.thread_executor = ThreadPoolExecutor(max_workers=8)
        self.process_executor = ProcessPoolExecutor(max_workers=4)

        # –ë–∞–∑–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã
        self.bls_service = BLSService()
        self.gpi_service = None  # –ë—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω

        # ML –º–æ–¥–µ–ª–∏ –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è
        self.ml_ensemble = None
        self.feature_extractors = {}
        self.scalers = {}

        # –ö—ç—à –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.cache = {}
        self.performance_stats = {
            "total_searches": 0,
            "avg_processing_time": 0.0,
            "method_success_rates": {},
            "ensemble_accuracy": 0.0,
        }

        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –º–µ—Ç–æ–¥–æ–≤
        self.method_weights = {
            "bls": 0.25,
            "gpi": 0.20,
            "tls": 0.15,
            "wavelet": 0.15,
            "fourier": 0.10,
            "ml_ensemble": 0.15,
        }

        logger.info("üöÄ Ultimate Ensemble Search Engine initialized")

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—É–ø–µ—Ä-–º–æ—â–Ω–æ–≥–æ –¥–≤–∏–∂–∫–∞"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
            await self.bls_service.initialize()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPI —Å–µ—Ä–≤–∏—Å–∞
            try:
                from services.gpi_service import GPIService

                self.gpi_service = GPIService()
                await self.gpi_service.initialize()
            except Exception as e:
                logger.warning(f"GPI service not available: {e}")

            # –°–æ–∑–¥–∞–Ω–∏–µ ML –∞–Ω—Å–∞–º–±–ª—è
            await self._initialize_ml_ensemble()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            await self._initialize_feature_extractors()

            self.initialized = True
            logger.info("‚úÖ Ultimate Ensemble Search Engine ready for MAXIMUM POWER!")

        except Exception as e:
            logger.error(f"‚ùå Failed to initialize ensemble engine: {e}")
            raise

    async def _initialize_ml_ensemble(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–æ—â–Ω–æ–≥–æ ML –∞–Ω—Å–∞–º–±–ª—è"""
        try:
            # –ë–∞–∑–æ–≤—ã–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä—ã
            rf_regressor = RandomForestRegressor(
                n_estimators=200,
                max_depth=15,
                min_samples_split=2,
                min_samples_leaf=1,
                random_state=42,
                n_jobs=-1,
            )

            gb_regressor = GradientBoostingRegressor(
                n_estimators=150,
                learning_rate=0.05,
                max_depth=8,
                subsample=0.8,
                random_state=42,
            )

            et_regressor = ExtraTreesRegressor(
                n_estimators=100,
                max_depth=12,
                min_samples_split=2,
                random_state=42,
                n_jobs=-1,
            )

            # –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
            mlp_regressor = MLPRegressor(
                hidden_layer_sizes=(256, 128, 64, 32),
                activation="relu",
                solver="adam",
                alpha=0.001,
                learning_rate="adaptive",
                max_iter=1000,
                random_state=42,
            )

            # SVM —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä
            svr_regressor = SVR(kernel="rbf", C=1.0, gamma="scale", epsilon=0.01)

            # –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç–µ–∫–∏–Ω–≥ –∞–Ω—Å–∞–º–±–ª—è
            base_models = [
                ("rf", rf_regressor),
                ("gb", gb_regressor),
                ("et", et_regressor),
                ("mlp", mlp_regressor),
                ("svr", svr_regressor),
            ]

            # –ú–µ—Ç–∞-–º–æ–¥–µ–ª—å –¥–ª—è —Å—Ç–µ–∫–∏–Ω–≥–∞
            meta_model = GradientBoostingRegressor(
                n_estimators=50, learning_rate=0.1, max_depth=5, random_state=42
            )

            # –§–∏–Ω–∞–ª—å–Ω—ã–π –∞–Ω—Å–∞–º–±–ª—å
            self.ml_ensemble = StackingRegressor(
                estimators=base_models, final_estimator=meta_model, cv=5, n_jobs=-1
            )

            logger.info("üß† ML Ensemble with 5 base models + stacking created")

        except Exception as e:
            logger.error(f"Failed to create ML ensemble: {e}")
            self.ml_ensemble = None

    async def _initialize_feature_extractors(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        self.feature_extractors = {
            "statistical": self._extract_statistical_features,
            "frequency": self._extract_frequency_features,
            "wavelet": self._extract_wavelet_features,
            "morphological": self._extract_morphological_features,
            "chaos": self._extract_chaos_features,
            "information": self._extract_information_features,
        }

        # –°–∫–µ–π–ª–µ—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        self.scalers = {
            "standard": StandardScaler(),
            "robust": RobustScaler(),
            "minmax": StandardScaler(),  # Placeholder
        }

        logger.info("üîß Advanced feature extractors initialized")

    async def ultimate_search(
        self,
        time: np.ndarray,
        flux: np.ndarray,
        flux_err: Optional[np.ndarray] = None,
        target_name: str = "unknown",
        stellar_params: Optional[Dict] = None,
        search_config: Optional[Dict] = None,
    ) -> EnsembleSearchResult:
        """
        –ù–ï–†–ï–ê–õ–¨–ù–û –ú–û–©–ù–´–ô –ê–ù–°–ê–ú–ë–õ–ï–í–´–ô –ü–û–ò–°–ö!
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Ç–æ–¥—ã –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        """
        start_time = time.time()

        logger.info(f"üöÄ LAUNCHING ULTIMATE SEARCH for {target_name}")
        logger.info(f"üî• MAXIMUM POWER MODE: {len(time)} data points")

        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.performance_stats["total_searches"] += 1

            # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞
            config = search_config or {}
            period_min = config.get("period_min", 0.5)
            period_max = config.get("period_max", 50.0)

            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            time_clean, flux_clean, flux_err_clean = (
                await self._ultimate_data_preprocessing(time, flux, flux_err)
            )

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            all_features = await self._extract_all_features(time_clean, flux_clean)

            # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–´–ô –ó–ê–ü–£–°–ö –í–°–ï–• –ú–ï–¢–û–î–û–í
            search_tasks = []

            # 1. BLS –ø–æ–∏—Å–∫
            search_tasks.append(
                asyncio.create_task(
                    self._run_bls_search(
                        time_clean,
                        flux_clean,
                        flux_err_clean,
                        period_min,
                        period_max,
                        target_name,
                    )
                )
            )

            # 2. GPI –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            if self.gpi_service:
                search_tasks.append(
                    asyncio.create_task(
                        self._run_gpi_search(time_clean, flux_clean, target_name)
                    )
                )

            # 3. TLS –ø–æ–∏—Å–∫
            search_tasks.append(
                asyncio.create_task(
                    self._run_tls_search(time_clean, flux_clean, period_min, period_max)
                )
            )

            # 4. Wavelet –∞–Ω–∞–ª–∏–∑
            search_tasks.append(
                asyncio.create_task(self._run_wavelet_analysis(time_clean, flux_clean))
            )

            # 5. Fourier –∞–Ω–∞–ª–∏–∑
            search_tasks.append(
                asyncio.create_task(
                    self._run_fourier_analysis(
                        time_clean, flux_clean, period_min, period_max
                    )
                )
            )

            # 6. ML –∞–Ω—Å–∞–º–±–ª—å
            if self.ml_ensemble:
                search_tasks.append(
                    asyncio.create_task(
                        self._run_ml_ensemble_search(
                            all_features, time_clean, flux_clean
                        )
                    )
                )

            # –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
            logger.info("‚ö° Running ALL methods in parallel...")
            search_results = await asyncio.gather(*search_tasks, return_exceptions=True)

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            method_results = {}
            methods_used = []

            for i, result in enumerate(search_results):
                method_names = [
                    "bls",
                    "gpi",
                    "tls",
                    "wavelet",
                    "fourier",
                    "ml_ensemble",
                ]
                method_name = (
                    method_names[i] if i < len(method_names) else f"method_{i}"
                )

                if not isinstance(result, Exception) and result is not None:
                    method_results[method_name] = result
                    methods_used.append(method_name)
                else:
                    logger.warning(f"Method {method_name} failed: {result}")

            # –°–£–ü–ï–†-–ú–û–©–ù–û–ï –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
            ensemble_result = await self._ultimate_ensemble_combination(
                method_results, all_features, time_clean, flux_clean, target_name
            )

            # –†–∞—Å—á–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç—Ä–∏–∫
            await self._calculate_advanced_ensemble_metrics(
                ensemble_result, method_results, all_features
            )

            # –§–∏–∑–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
            await self._perform_physical_modeling(ensemble_result, stellar_params or {})

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            recommendations = await self._generate_ultimate_recommendations(
                ensemble_result, method_results
            )

            processing_time = time.time() - start_time

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            await self._update_performance_stats(
                processing_time, methods_used, ensemble_result
            )

            # –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            final_result = EnsembleSearchResult(
                target_name=target_name,
                consensus_period=ensemble_result["consensus_period"],
                consensus_confidence=ensemble_result["consensus_confidence"],
                consensus_depth=ensemble_result["consensus_depth"],
                consensus_snr=ensemble_result["consensus_snr"],
                bls_result=method_results.get("bls"),
                gpi_result=method_results.get("gpi"),
                tls_result=method_results.get("tls"),
                wavelet_result=method_results.get("wavelet"),
                fourier_result=method_results.get("fourier"),
                ml_result=method_results.get("ml_ensemble"),
                method_agreement=ensemble_result["method_agreement"],
                ensemble_uncertainty=ensemble_result["ensemble_uncertainty"],
                cross_validation_score=ensemble_result["cross_validation_score"],
                bootstrap_confidence=ensemble_result["bootstrap_confidence"],
                planet_candidates=ensemble_result["planet_candidates"],
                false_positive_probability=ensemble_result[
                    "false_positive_probability"
                ],
                habitability_score=ensemble_result["habitability_score"],
                detection_significance=ensemble_result["detection_significance"],
                stellar_parameters=ensemble_result["stellar_parameters"],
                planetary_system=ensemble_result["planetary_system"],
                orbital_dynamics=ensemble_result["orbital_dynamics"],
                processing_time=processing_time,
                methods_used=methods_used,
                quality_flags=ensemble_result["quality_flags"],
                recommendations=recommendations,
            )

            logger.info(
                f"üéâ ULTIMATE SEARCH COMPLETED in {processing_time:.2f}s! "
                f"Consensus: P={final_result.consensus_period:.3f}d, "
                f"Confidence={final_result.consensus_confidence:.3f}, "
                f"Methods={len(methods_used)}"
            )

            return final_result

        except Exception as e:
            logger.error(f"üí• ULTIMATE SEARCH FAILED: {e}")
            raise

    async def _ultimate_data_preprocessing(
        self, time: np.ndarray, flux: np.ndarray, flux_err: Optional[np.ndarray]
    ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """–°—É–ø–µ—Ä-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""

        # –£–¥–∞–ª–µ–Ω–∏–µ NaN –∏ –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç–µ–π
        mask = np.isfinite(time) & np.isfinite(flux)
        if flux_err is not None:
            mask &= np.isfinite(flux_err) & (flux_err > 0)

        time_clean = time[mask]
        flux_clean = flux[mask]
        flux_err_clean = flux_err[mask] if flux_err is not None else None

        # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤

        # 1. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
        q1, q3 = np.percentile(flux_clean, [25, 75])
        iqr = q3 - q1
        lower_bound = q1 - 3 * iqr
        upper_bound = q3 + 3 * iqr

        stat_mask = (flux_clean >= lower_bound) & (flux_clean <= upper_bound)

        # 2. –ö–ª–∞—Å—Ç–µ—Ä–Ω–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤
        if len(flux_clean) > 100:
            try:
                from sklearn.preprocessing import StandardScaler

                features = np.column_stack(
                    [
                        StandardScaler()
                        .fit_transform(time_clean.reshape(-1, 1))
                        .flatten(),
                        StandardScaler()
                        .fit_transform(flux_clean.reshape(-1, 1))
                        .flatten(),
                    ]
                )

                clustering = DBSCAN(eps=0.3, min_samples=5).fit(features)
                cluster_mask = clustering.labels_ != -1

                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–∞—Å–æ–∫
                final_mask = stat_mask & cluster_mask
            except Exception:
                final_mask = stat_mask
        else:
            final_mask = stat_mask

        time_clean = time_clean[final_mask]
        flux_clean = flux_clean[final_mask]
        if flux_err_clean is not None:
            flux_err_clean = flux_err_clean[final_mask]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        flux_median = np.median(flux_clean)
        flux_clean = flux_clean / flux_median
        if flux_err_clean is not None:
            flux_err_clean = flux_err_clean / flux_median

        # –î–µ—Ç—Ä–µ–Ω–¥–∏–Ω–≥ —Å –ø–æ–º–æ—â—å—é –ø–æ–ª–∏–Ω–æ–º–∞
        if len(time_clean) > 50:
            try:
                poly_coeffs = np.polyfit(time_clean, flux_clean, deg=3)
                trend = np.polyval(poly_coeffs, time_clean)
                flux_clean = flux_clean / trend
            except (np.linalg.LinAlgError, np.RankWarning, ValueError) as e:
                logger.warning(f"Detrending failed for target: {e}. Continuing without detrending.")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –¥–µ—Ç—Ä–µ–Ω–¥–∏–Ω–≥–∞

        logger.info(f"üßπ Data preprocessing: {len(time)} ‚Üí {len(time_clean)} points")

        return time_clean, flux_clean, flux_err_clean

    async def _extract_all_features(
        self, time: np.ndarray, flux: np.ndarray
    ) -> Dict[str, np.ndarray]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        all_features = {}

        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_tasks = []
        for feature_type, extractor in self.feature_extractors.items():
            task = asyncio.get_event_loop().run_in_executor(
                self.thread_executor, extractor, time, flux
            )
            feature_tasks.append((feature_type, task))

        # –°–±–æ—Ä –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        for feature_type, task in feature_tasks:
            try:
                features = await task
                all_features[feature_type] = features
            except Exception as e:
                logger.warning(f"Feature extraction {feature_type} failed: {e}")

        logger.info(f"üîß Extracted {len(all_features)} feature types")
        return all_features

    def _extract_statistical_features(
        self, time: np.ndarray, flux: np.ndarray
    ) -> np.ndarray:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        features = []

        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        features.extend(
            [
                np.mean(flux),
                np.std(flux),
                np.var(flux),
                np.median(flux),
                np.min(flux),
                np.max(flux),
                np.ptp(flux),
                stats.skew(flux),
                stats.kurtosis(flux),
            ]
        )

        # –ö–≤–∞–Ω—Ç–∏–ª–∏
        for q in [0.05, 0.1, 0.25, 0.75, 0.9, 0.95]:
            features.append(np.quantile(flux, q))

        # –ú–æ–º–µ–Ω—Ç—ã –≤—ã—Å—à–∏—Ö –ø–æ—Ä—è–¥–∫–æ–≤
        for moment in range(3, 7):
            features.append(stats.moment(flux, moment=moment))

        # –≠–Ω—Ç—Ä–æ–ø–∏—è
        hist, _ = np.histogram(flux, bins=50, density=True)
        hist = hist[hist > 0]
        entropy = -np.sum(hist * np.log2(hist))
        features.append(entropy)

        return np.array(features)

    def _extract_frequency_features(
        self, time: np.ndarray, flux: np.ndarray
    ) -> np.ndarray:
        """–ß–∞—Å—Ç–æ—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        features = []

        # FFT –∞–Ω–∞–ª–∏–∑
        dt = np.median(np.diff(time))
        fft_flux = np.fft.fft(flux - np.mean(flux))
        freqs = np.fft.fftfreq(len(flux), dt)
        power = np.abs(fft_flux) ** 2

        # –ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã
        pos_mask = freqs > 0
        freqs_pos = freqs[pos_mask]
        power_pos = power[pos_mask]

        if len(power_pos) > 0:
            # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —á–∞—Å—Ç–æ—Ç–∞
            max_idx = np.argmax(power_pos)
            features.extend(
                [
                    freqs_pos[max_idx],  # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∞—è —á–∞—Å—Ç–æ—Ç–∞
                    power_pos[max_idx],  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å
                    1.0 / freqs_pos[max_idx] if freqs_pos[max_idx] > 0 else 0,  # –ü–µ—Ä–∏–æ–¥
                ]
            )

            # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã
            total_power = np.sum(power_pos)
            if total_power > 0:
                # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥
                spectral_centroid = np.sum(freqs_pos * power_pos) / total_power
                features.append(spectral_centroid)

                # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞
                spectral_spread = np.sqrt(
                    np.sum(((freqs_pos - spectral_centroid) ** 2) * power_pos)
                    / total_power
                )
                features.append(spectral_spread)

                # –°–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–∞—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è
                spectral_skewness = np.sum(
                    ((freqs_pos - spectral_centroid) ** 3) * power_pos
                ) / (total_power * spectral_spread**3)
                features.append(spectral_skewness)
            else:
                features.extend([0, 0, 0])

            # –ú–æ—â–Ω–æ—Å—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö –¥–∏–∞–ø–∞–∑–æ–Ω–∞—Ö
            freq_bands = [(0, 0.1), (0.1, 0.5), (0.5, 2.0), (2.0, 10.0)]
            for f_min, f_max in freq_bands:
                band_mask = (freqs_pos >= f_min) & (freqs_pos < f_max)
                band_power = np.sum(power_pos[band_mask]) if np.any(band_mask) else 0
                features.append(band_power)
        else:
            features.extend([0] * 10)

        return np.array(features)

    def _extract_wavelet_features(
        self, time: np.ndarray, flux: np.ndarray
    ) -> np.ndarray:
        """–í–µ–π–≤–ª–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        features = []

        try:
            from scipy import signal as scipy_signal

            # Continuous Wavelet Transform
            widths = np.arange(1, 31)
            cwt_matrix = scipy_signal.cwt(flux, scipy_signal.ricker, widths)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–∞—Å—à—Ç–∞–±–∞–º
            for i in range(min(10, cwt_matrix.shape[0])):
                scale_coeffs = cwt_matrix[i, :]
                features.extend(
                    [
                        np.mean(np.abs(scale_coeffs)),
                        np.std(scale_coeffs),
                        np.max(np.abs(scale_coeffs)),
                    ]
                )

            # –û–±—â–∏–µ –≤–µ–π–≤–ª–µ—Ç —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            features.extend(
                [
                    np.mean(np.abs(cwt_matrix)),
                    np.std(cwt_matrix),
                    np.max(np.abs(cwt_matrix)),
                ]
            )

        except Exception as e:
            logger.warning(f"Wavelet analysis failed: {e}")
            features = [0] * 33  # –ó–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏ –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å

        return np.array(features)

    def _extract_morphological_features(
        self, time: np.ndarray, flux: np.ndarray
    ) -> np.ndarray:
        """–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        features = []

        # –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ
        first_diff = np.diff(flux)
        second_diff = np.diff(first_diff)

        features.extend(
            [
                np.mean(np.abs(first_diff)),
                np.std(first_diff),
                np.mean(np.abs(second_diff)),
                np.std(second_diff),
            ]
        )

        # –õ–æ–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å—Ç—Ä–µ–º—É–º—ã
        from scipy.signal import find_peaks

        peaks, _ = find_peaks(flux)
        valleys, _ = find_peaks(-flux)

        features.extend(
            [
                len(peaks) / len(flux),  # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –ø–∏–∫–æ–≤
                len(valleys) / len(flux),  # –ü–ª–æ—Ç–Ω–æ—Å—Ç—å –≤–ø–∞–¥–∏–Ω
                np.mean(flux[peaks]) if len(peaks) > 0 else np.mean(flux),
                np.mean(flux[valleys]) if len(valleys) > 0 else np.mean(flux),
            ]
        )

        # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
        autocorr = np.correlate(flux - np.mean(flux), flux - np.mean(flux), mode="full")
        autocorr = autocorr[autocorr.size // 2 :]
        autocorr = autocorr / autocorr[0] if autocorr[0] != 0 else autocorr

        # –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        if len(autocorr) > 10:
            features.extend(
                [
                    autocorr[1] if len(autocorr) > 1 else 0,
                    autocorr[5] if len(autocorr) > 5 else 0,
                    autocorr[10] if len(autocorr) > 10 else 0,
                    np.argmax(autocorr[1:]) + 1 if len(autocorr) > 1 else 0,
                ]
            )
        else:
            features.extend([0, 0, 0, 0])

        return np.array(features)

    def _extract_chaos_features(self, time: np.ndarray, flux: np.ndarray) -> np.ndarray:
        """–ü—Ä–∏–∑–Ω–∞–∫–∏ —Ö–∞–æ—Å–∞ –∏ –Ω–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç–∏"""
        features = []

        # –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å –õ—è–ø—É–Ω–æ–≤–∞ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω—ã–π)
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —á–µ—Ä–µ–∑ —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–π
            diffs = np.diff(flux)
            lyapunov_approx = np.mean(np.log(np.abs(diffs) + 1e-10))
            features.append(lyapunov_approx)
        except Exception:
            features.append(0)

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        try:
            # Embedding dimension
            m = 3
            tau = 1

            if len(flux) > m * tau:
                # –°–æ–∑–¥–∞–Ω–∏–µ –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
                embedded = np.array(
                    [
                        flux[i : i + m * tau : tau]
                        for i in range(len(flux) - m * tau + 1)
                    ]
                )

                # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è —Å—É–º–º–∞
                distances = []
                for i in range(min(100, len(embedded))):
                    for j in range(i + 1, min(100, len(embedded))):
                        dist = np.linalg.norm(embedded[i] - embedded[j])
                        distances.append(dist)

                if distances:
                    correlation_dim = np.mean(np.log(distances + 1e-10))
                    features.append(correlation_dim)
                else:
                    features.append(0)
            else:
                features.append(0)
        except Exception:
            features.append(0)

        # –≠–Ω—Ç—Ä–æ–ø–∏—è –®–µ–Ω–Ω–æ–Ω–∞
        try:
            hist, _ = np.histogram(flux, bins=20, density=True)
            hist = hist[hist > 0]
            shannon_entropy = -np.sum(hist * np.log2(hist))
            features.append(shannon_entropy)
        except Exception:
            features.append(0)

        # –ü—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è
        try:

            def _maxdist(xi, xj, N):
                return max([abs(ua - va) for ua, va in zip(xi, xj)])

            def _phi(m):
                patterns = np.array([flux[i : i + m] for i in range(len(flux) - m + 1)])
                C = np.zeros(len(patterns))
                for i in range(len(patterns)):
                    template_i = patterns[i]
                    for j in range(len(patterns)):
                        if _maxdist(template_i, patterns[j], m) <= 0.1 * np.std(flux):
                            C[i] += 1.0
                phi = np.mean(np.log(C / len(patterns)))
                return phi

            ApEn = _phi(2) - _phi(3)
            features.append(ApEn)
        except Exception:
            features.append(0)

        return np.array(features)

    def _extract_information_features(
        self, time: np.ndarray, flux: np.ndarray
    ) -> np.ndarray:
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"""
        features = []

        # –í–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
        try:

            def mutual_info(x, y, bins=20):
                hist_xy, _, _ = np.histogram2d(x, y, bins=bins)
                hist_x, _ = np.histogram(x, bins=bins)
                hist_y, _ = np.histogram(y, bins=bins)

                hist_xy = hist_xy / np.sum(hist_xy)
                hist_x = hist_x / np.sum(hist_x)
                hist_y = hist_y / np.sum(hist_y)

                mi = 0
                for i in range(bins):
                    for j in range(bins):
                        if hist_xy[i, j] > 0:
                            mi += hist_xy[i, j] * np.log2(
                                hist_xy[i, j] / (hist_x[i] * hist_y[j])
                            )

                return mi

            # –í–∑–∞–∏–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ –∑–∞–¥–µ—Ä–∂–∫–∞–º–∏
            for lag in [1, 2, 5, 10]:
                if len(flux) > lag:
                    mi = mutual_info(flux[:-lag], flux[lag:])
                    features.append(mi)
                else:
                    features.append(0)
        except Exception:
            features.extend([0, 0, 0, 0])

        # –°–ª–æ–∂–Ω–æ—Å—Ç—å –õ–µ–º–ø–µ–ª—è-–ó–∏–≤–∞
        try:
            # –ë–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–∞
            binary_flux = (flux > np.median(flux)).astype(int)
            binary_string = "".join(map(str, binary_flux))

            # –ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–¥—Å—Ç—Ä–æ–∫
            substrings = set()
            for i in range(len(binary_string)):
                for j in range(i + 1, min(i + 10, len(binary_string) + 1)):
                    substrings.add(binary_string[i:j])

            lz_complexity = (
                len(substrings) / len(binary_string) if len(binary_string) > 0 else 0
            )
            features.append(lz_complexity)
        except Exception:
            features.append(0)

        return np.array(features)

    async def _run_bls_search(
        self,
        time: np.ndarray,
        flux: np.ndarray,
        flux_err: Optional[np.ndarray],
        period_min: float,
        period_max: float,
        target_name: str,
    ) -> BLSResult:
        """–ó–∞–ø—É—Å–∫ BLS –ø–æ–∏—Å–∫–∞"""
        try:
            result = await self.bls_service.analyze(
                time, flux, flux_err, period_min, period_max, 7.0, target_name
            )
            logger.info(
                f"‚úÖ BLS search completed: P={result.best_period:.3f}d, SNR={result.snr:.1f}"
            )
            return result
        except Exception as e:
            logger.error(f"‚ùå BLS search failed: {e}")
            return None

    async def _run_gpi_search(
        self, time: np.ndarray, flux: np.ndarray, target_name: str
    ) -> Dict:
        """–ó–∞–ø—É—Å–∫ GPI –ø–æ–∏—Å–∫–∞"""
        try:
            if self.gpi_service:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è GPI
                lightcurve_data = {
                    "time": time.tolist(),
                    "flux": flux.tolist(),
                    "target_name": target_name,
                }

                result = await self.gpi_service.search_exoplanets(lightcurve_data)
                logger.info(f"‚úÖ GPI search completed")
                return result
            else:
                return None
        except Exception as e:
            logger.error(f"‚ùå GPI search failed: {e}")
            return None

    async def _run_tls_search(
        self, time: np.ndarray, flux: np.ndarray, period_min: float, period_max: float
    ) -> Dict:
        """–ó–∞–ø—É—Å–∫ Transit Least Squares –ø–æ–∏—Å–∫–∞"""
        try:
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è TLS
            periods = np.logspace(np.log10(period_min), np.log10(period_max), 500)
            powers = np.zeros_like(periods)

            for i, period in enumerate(periods):
                # –§–∞–∑–æ–≤–∞—è —Å–≤–µ—Ä—Ç–∫–∞
                phase = (time % period) / period

                # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç—Ä–∞–Ω–∑–∏—Ç–∞
                best_power = 0
                for duration_frac in [0.01, 0.02, 0.05, 0.1]:
                    # –¢—Ä–∞–Ω–∑–∏—Ç–Ω–∞—è –º–∞—Å–∫–∞
                    transit_mask = phase < duration_frac

                    if np.sum(transit_mask) > 5 and np.sum(~transit_mask) > 20:
                        in_transit = np.mean(flux[transit_mask])
                        out_transit = np.mean(flux[~transit_mask])

                        if out_transit > 0:
                            depth = (out_transit - in_transit) / out_transit
                            if depth > 0:
                                power = depth * np.sqrt(np.sum(transit_mask))
                                best_power = max(best_power, power)

                powers[i] = best_power

            # –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            best_idx = np.argmax(powers)
            best_period = periods[best_idx]
            best_power = powers[best_idx]

            result = {
                "method": "TLS",
                "best_period": float(best_period),
                "best_power": float(best_power),
                "periods": periods.tolist(),
                "powers": powers.tolist(),
                "snr": float(best_power / np.std(powers)) if np.std(powers) > 0 else 0,
            }

            logger.info(f"‚úÖ TLS search completed: P={best_period:.3f}d")
            return result

        except Exception as e:
            logger.error(f"‚ùå TLS search failed: {e}")
            return None

    async def _run_wavelet_analysis(self, time: np.ndarray, flux: np.ndarray) -> Dict:
        """–ó–∞–ø—É—Å–∫ –≤–µ–π–≤–ª–µ—Ç –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            from scipy import signal as scipy_signal

            # Continuous Wavelet Transform
            widths = np.logspace(0, 2, 100)  # –û—Ç 1 –¥–æ 100
            cwt_matrix = scipy_signal.cwt(
                flux - np.mean(flux), scipy_signal.ricker, widths
            )

            # –ü–æ–∏—Å–∫ –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
            max_coeffs = np.max(np.abs(cwt_matrix), axis=1)
            best_scale_idx = np.argmax(max_coeffs)
            best_scale = widths[best_scale_idx]

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∞ –≤ –ø–µ—Ä–∏–æ–¥ (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
            dt = np.median(np.diff(time))
            estimated_period = best_scale * dt * 2  # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞

            # –ú–æ—â–Ω–æ—Å—Ç—å —Å–∏–≥–Ω–∞–ª–∞
            power = np.max(max_coeffs)
            snr = power / np.std(max_coeffs) if np.std(max_coeffs) > 0 else 0

            result = {
                "method": "Wavelet",
                "best_period": float(estimated_period),
                "best_scale": float(best_scale),
                "power": float(power),
                "snr": float(snr),
                "scales": widths.tolist(),
                "coefficients": max_coeffs.tolist(),
            }

            logger.info(f"‚úÖ Wavelet analysis completed: P={estimated_period:.3f}d")
            return result

        except Exception as e:
            logger.error(f"‚ùå Wavelet analysis failed: {e}")
            return None

    async def _run_fourier_analysis(
        self, time: np.ndarray, flux: np.ndarray, period_min: float, period_max: float
    ) -> Dict:
        """–ó–∞–ø—É—Å–∫ –§—É—Ä—å–µ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            dt = np.median(np.diff(time))
            flux_detrended = flux - np.mean(flux)

            # FFT
            fft_flux = np.fft.fft(flux_detrended)
            freqs = np.fft.fftfreq(len(flux_detrended), dt)
            power = np.abs(fft_flux) ** 2

            # –¢–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã
            pos_mask = freqs > 0
            freqs_pos = freqs[pos_mask]
            power_pos = power[pos_mask]

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω—É –ø–µ—Ä–∏–æ–¥–æ–≤
            periods = 1.0 / freqs_pos
            period_mask = (periods >= period_min) & (periods <= period_max)

            filtered_periods = periods[period_mask]
            filtered_power = power_pos[period_mask]

            if len(filtered_power) > 0:
                # –õ—É—á—à–∏–π –ø–µ—Ä–∏–æ–¥
                best_idx = np.argmax(filtered_power)
                best_period = filtered_periods[best_idx]
                best_power = filtered_power[best_idx]

                # SNR
                snr = (
                    best_power / np.std(filtered_power)
                    if np.std(filtered_power) > 0
                    else 0
                )

                # –ì–∞—Ä–º–æ–Ω–∏–∫–∏
                harmonics = []
                for n in [2, 3, 4, 5]:
                    harmonic_period = best_period / n
                    if period_min <= harmonic_period <= period_max:
                        # –ù–∞–π—Ç–∏ –±–ª–∏–∂–∞–π—à—É—é —á–∞—Å—Ç–æ—Ç—É
                        harmonic_freq = 1.0 / harmonic_period
                        freq_diff = np.abs(freqs_pos - harmonic_freq)
                        harmonic_idx = np.argmin(freq_diff)
                        if freq_diff[harmonic_idx] < 0.1 * harmonic_freq:
                            harmonics.append(
                                {
                                    "period": float(harmonic_period),
                                    "power": float(power_pos[harmonic_idx]),
                                    "harmonic": n,
                                }
                            )

                result = {
                    "method": "Fourier",
                    "best_period": float(best_period),
                    "best_power": float(best_power),
                    "snr": float(snr),
                    "harmonics": harmonics,
                    "periods": filtered_periods.tolist(),
                    "powers": filtered_power.tolist(),
                }

                logger.info(f"‚úÖ Fourier analysis completed: P={best_period:.3f}d")
                return result
            else:
                return None

        except Exception as e:
            logger.error(f"‚ùå Fourier analysis failed: {e}")
            return None

    async def _run_ml_ensemble_search(
        self, all_features: Dict[str, np.ndarray], time: np.ndarray, flux: np.ndarray
    ) -> Dict:
        """–ó–∞–ø—É—Å–∫ ML –∞–Ω—Å–∞–º–±–ª—è"""
        try:
            if not self.ml_ensemble:
                return None

            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_vector = []
            feature_names = []

            for feature_type, features in all_features.items():
                feature_vector.extend(features.tolist())
                feature_names.extend(
                    [f"{feature_type}_{i}" for i in range(len(features))]
                )

            if len(feature_vector) == 0:
                return None

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features_array = np.array(feature_vector).reshape(1, -1)

            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–µ—Ä–∏–æ–¥–∞ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞)
            # –ó–¥–µ—Å—å –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é —ç–≤—Ä–∏—Å—Ç–∏–∫—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–∏–æ–¥ –∏–∑ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            freq_features = all_features.get("frequency", np.array([]))
            if len(freq_features) > 2:
                estimated_period = freq_features[2]  # –¢—Ä–µ—Ç–∏–π —ç–ª–µ–º–µ–Ω—Ç - –ø–µ—Ä–∏–æ–¥
            else:
                estimated_period = 5.0  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

            # –û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            stat_features = all_features.get("statistical", np.array([]))
            if len(stat_features) > 1:
                confidence = (
                    min(1.0, stat_features[1] / stat_features[0])
                    if stat_features[0] > 0
                    else 0.5
                )
            else:
                confidence = 0.5

            # –û—Ü–µ–Ω–∫–∞ SNR
            if len(stat_features) > 8:
                snr = abs(stat_features[8])  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—É—Ä—Ç–æ–∑–∏—Å –∫–∞–∫ –ø—Ä–æ–∫—Å–∏ –¥–ª—è SNR
            else:
                snr = 5.0

            result = {
                "method": "ML_Ensemble",
                "best_period": float(estimated_period),
                "confidence": float(confidence),
                "snr": float(snr),
                "feature_count": len(feature_vector),
                "feature_types": list(all_features.keys()),
            }

            logger.info(f"‚úÖ ML ensemble completed: P={estimated_period:.3f}d")
            return result

        except Exception as e:
            logger.error(f"‚ùå ML ensemble failed: {e}")
            return None

    async def _ultimate_ensemble_combination(
        self,
        method_results: Dict,
        all_features: Dict,
        time: np.ndarray,
        flux: np.ndarray,
        target_name: str,
    ) -> Dict:
        """–°–£–ü–ï–†-–ú–û–©–ù–û–ï –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤"""

        logger.info("üî• ULTIMATE ENSEMBLE COMBINATION STARTING...")

        # –°–±–æ—Ä –ø–µ—Ä–∏–æ–¥–æ–≤ –∏ –≤–µ—Å–æ–≤ –æ—Ç –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
        periods = []
        weights = []
        confidences = []
        snrs = []

        for method_name, result in method_results.items():
            if result is None:
                continue

            method_weight = self.method_weights.get(method_name, 0.1)

            if method_name == "bls" and hasattr(result, "best_period"):
                periods.append(result.best_period)
                weights.append(
                    method_weight * (1 + result.snr / 20)
                )  # –ë–æ–Ω—É—Å –∑–∞ –≤—ã—Å–æ–∫–∏–π SNR
                confidences.append(result.significance)
                snrs.append(result.snr)

            elif isinstance(result, dict) and "best_period" in result:
                periods.append(result["best_period"])
                method_snr = result.get("snr", 5.0)
                weights.append(method_weight * (1 + method_snr / 20))
                confidences.append(result.get("confidence", 0.5))
                snrs.append(method_snr)

        if not periods:
            # –ï—Å–ª–∏ –Ω–∏–∫—Ç–æ –Ω–µ –Ω–∞—à–µ–ª –ø–µ—Ä–∏–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return {
                "consensus_period": 5.0,
                "consensus_confidence": 0.1,
                "consensus_depth": 0.001,
                "consensus_snr": 1.0,
                "method_agreement": 0.0,
                "ensemble_uncertainty": 1.0,
                "cross_validation_score": 0.0,
                "bootstrap_confidence": 0.0,
                "planet_candidates": [],
                "false_positive_probability": 0.9,
                "habitability_score": 0.0,
                "detection_significance": 0.0,
                "stellar_parameters": {},
                "planetary_system": {},
                "orbital_dynamics": {},
                "quality_flags": {"low_data_quality": True},
            }

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        weights = np.array(weights)
        weights = weights / np.sum(weights)

        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø–µ—Ä–∏–æ–¥–æ–≤
        consensus_period = np.average(periods, weights=weights)

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        period_agreement = await self._calculate_period_agreement(periods, weights)

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        consensus_confidence = np.average(confidences, weights=weights)

        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π SNR
        consensus_snr = np.average(snrs, weights=weights)

        # –û—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã —Ç—Ä–∞–Ω–∑–∏—Ç–∞
        consensus_depth = await self._estimate_transit_depth(
            time, flux, consensus_period
        )

        # Bootstrap –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏
        bootstrap_results = await self._bootstrap_analysis(
            method_results, time, flux, n_bootstrap=100
        )

        # –ü–æ–∏—Å–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–ª–∞–Ω–µ—Ç
        planet_candidates = await self._identify_planet_candidates(
            method_results, consensus_period, consensus_confidence
        )

        # –û—Ü–µ–Ω–∫–∞ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π
        false_positive_prob = await self._estimate_false_positive_probability(
            method_results, all_features, consensus_period, consensus_snr
        )

        # –û—Ü–µ–Ω–∫–∞ –æ–±–∏—Ç–∞–µ–º–æ—Å—Ç–∏
        habitability_score = await self._calculate_habitability_score(
            consensus_period, consensus_depth
        )

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
        detection_significance = await self._calculate_detection_significance(
            consensus_snr, period_agreement, len(method_results)
        )

        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv_score = await self._cross_validate_results(method_results, all_features)

        logger.info(
            f"üéØ CONSENSUS: P={consensus_period:.3f}d, Conf={consensus_confidence:.3f}"
        )

        return {
            "consensus_period": float(consensus_period),
            "consensus_confidence": float(consensus_confidence),
            "consensus_depth": float(consensus_depth),
            "consensus_snr": float(consensus_snr),
            "method_agreement": float(period_agreement),
            "ensemble_uncertainty": float(bootstrap_results["uncertainty"]),
            "cross_validation_score": float(cv_score),
            "bootstrap_confidence": float(bootstrap_results["confidence"]),
            "planet_candidates": planet_candidates,
            "false_positive_probability": float(false_positive_prob),
            "habitability_score": float(habitability_score),
            "detection_significance": float(detection_significance),
            "stellar_parameters": await self._estimate_stellar_parameters(time, flux),
            "planetary_system": await self._model_planetary_system(
                consensus_period, consensus_depth
            ),
            "orbital_dynamics": await self._calculate_orbital_dynamics(
                consensus_period
            ),
            "quality_flags": await self._assess_data_quality(time, flux, all_features),
        }

    async def _calculate_period_agreement(
        self, periods: List[float], weights: np.ndarray
    ) -> float:
        """–†–∞—Å—á–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –ø–µ—Ä–∏–æ–¥–æ–≤ –º–µ–∂–¥—É –º–µ—Ç–æ–¥–∞–º–∏"""
        if len(periods) < 2:
            return 1.0

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–µ—Ä–∏–æ–¥–æ–≤
        periods_array = np.array(periods).reshape(-1, 1)

        try:
            from sklearn.cluster import KMeans

            kmeans = KMeans(n_clusters=min(3, len(periods)), random_state=42)
            clusters = kmeans.fit_predict(periods_array)

            # –ù–∞–π—Ç–∏ —Å–∞–º—ã–π –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä
            unique_clusters, counts = np.unique(clusters, return_counts=True)
            main_cluster = unique_clusters[np.argmax(counts)]

            # –î–æ–ª—è –º–µ—Ç–æ–¥–æ–≤ –≤ –≥–ª–∞–≤–Ω–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
            main_cluster_mask = clusters == main_cluster
            agreement = np.sum(weights[main_cluster_mask])

            return min(1.0, agreement)

        except Exception:
            # Fallback: –ø—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–∞–∑–±—Ä–æ—Å–∞
            period_std = np.std(periods)
            period_mean = np.mean(periods)
            relative_std = period_std / period_mean if period_mean > 0 else 1.0

            return max(0.0, 1.0 - relative_std)

    async def _estimate_transit_depth(
        self, time: np.ndarray, flux: np.ndarray, period: float
    ) -> float:
        """–û—Ü–µ–Ω–∫–∞ –≥–ª—É–±–∏–Ω—ã —Ç—Ä–∞–Ω–∑–∏—Ç–∞"""
        try:
            # –§–∞–∑–æ–≤–∞—è —Å–≤–µ—Ä—Ç–∫–∞
            phase = (time % period) / period

            # –ü–æ–∏—Å–∫ –º–∏–Ω–∏–º—É–º–∞ (—Ç—Ä–∞–Ω–∑–∏—Ç–∞)
            phase_bins = np.linspace(0, 1, 100)
            binned_flux = []

            for i in range(len(phase_bins) - 1):
                mask = (phase >= phase_bins[i]) & (phase < phase_bins[i + 1])
                if np.sum(mask) > 0:
                    binned_flux.append(np.mean(flux[mask]))
                else:
                    binned_flux.append(np.mean(flux))

            binned_flux = np.array(binned_flux)

            # –ì–ª—É–±–∏–Ω–∞ –∫–∞–∫ —Ä–∞–∑–Ω–æ—Å—Ç—å –º–µ–∂–¥—É –º–∞–∫—Å–∏–º—É–º–æ–º –∏ –º–∏–Ω–∏–º—É–º–æ–º
            depth = (np.max(binned_flux) - np.min(binned_flux)) / np.max(binned_flux)

            return max(0.0, depth)

        except Exception:
            return 0.001  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

    async def _bootstrap_analysis(
        self,
        method_results: Dict,
        time: np.ndarray,
        flux: np.ndarray,
        n_bootstrap: int = 100,
    ) -> Dict:
        """Bootstrap –∞–Ω–∞–ª–∏–∑ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏"""
        try:
            bootstrap_periods = []

            # –ü—Ä–æ—Å—Ç–æ–π bootstrap –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ—é—â–∏—Ö—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            periods = []
            for result in method_results.values():
                if result is None:
                    continue

                if hasattr(result, "best_period"):
                    periods.append(result.best_period)
                elif isinstance(result, dict) and "best_period" in result:
                    periods.append(result["best_period"])

            if not periods:
                return {"uncertainty": 1.0, "confidence": 0.0}

            # Bootstrap –≤—ã–±–æ—Ä–∫–∞
            periods = np.array(periods)
            for _ in range(n_bootstrap):
                bootstrap_sample = np.random.choice(
                    periods, size=len(periods), replace=True
                )
                bootstrap_periods.append(np.mean(bootstrap_sample))

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ bootstrap
            bootstrap_periods = np.array(bootstrap_periods)
            uncertainty = (
                np.std(bootstrap_periods) / np.mean(bootstrap_periods)
                if np.mean(bootstrap_periods) > 0
                else 1.0
            )

            # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
            confidence_interval = np.percentile(bootstrap_periods, [2.5, 97.5])
            confidence = 1.0 - (
                confidence_interval[1] - confidence_interval[0]
            ) / np.mean(bootstrap_periods)

            return {
                "uncertainty": float(min(1.0, uncertainty)),
                "confidence": float(max(0.0, confidence)),
            }

        except Exception:
            return {"uncertainty": 0.5, "confidence": 0.5}

    async def _identify_planet_candidates(
        self, method_results: Dict, consensus_period: float, consensus_confidence: float
    ) -> List[Dict]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –ø–ª–∞–Ω–µ—Ç"""
        candidates = []

        try:
            # –û—Å–Ω–æ–≤–Ω–æ–π –∫–∞–Ω–¥–∏–¥–∞—Ç
            if consensus_confidence > 0.7:
                candidates.append(
                    {
                        "period": float(consensus_period),
                        "confidence": float(consensus_confidence),
                        "type": "primary",
                        "methods_supporting": len(
                            [r for r in method_results.values() if r is not None]
                        ),
                    }
                )

            # –ü–æ–∏—Å–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –º–µ—Ç–æ–¥–æ–≤
            for method_name, result in method_results.items():
                if result is None:
                    continue

                if hasattr(result, "secondary_periods"):
                    for sec_period in result.secondary_periods[
                        :2
                    ]:  # –ú–∞–∫—Å–∏–º—É–º 2 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö
                        if abs(sec_period - consensus_period) > 0.1:  # –ù–µ –¥—É–±–ª–∏–∫–∞—Ç
                            candidates.append(
                                {
                                    "period": float(sec_period),
                                    "confidence": 0.5,
                                    "type": "secondary",
                                    "source_method": method_name,
                                }
                            )

            return candidates[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤

        except Exception as e:
            logger.error(f"Error identifying candidates: {e}")
            return []

    async def _estimate_false_positive_probability(
        self,
        method_results: Dict,
        all_features: Dict,
        consensus_period: float,
        consensus_snr: float,
    ) -> float:
        """–û—Ü–µ–Ω–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –ª–æ–∂–Ω–æ–≥–æ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è"""
        try:
            # –ë–∞–∑–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ SNR
            base_fap = max(0.01, 1.0 / (consensus_snr**2))

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ –º–µ—Ç–æ–¥–æ–≤
            method_agreement = len(
                [r for r in method_results.values() if r is not None]
            ) / len(method_results)
            agreement_factor = 1.0 - method_agreement * 0.5

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            stat_features = all_features.get("statistical", np.array([]))
            if len(stat_features) > 5:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∏—Å–ø–µ—Ä—Å–∏—é –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
                variance_factor = min(
                    2.0, stat_features[1] * 10
                )  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
                base_fap *= variance_factor

            return min(0.99, base_fap * agreement_factor)

        except Exception as e:
            logger.error(f"Error estimating false positive probability: {e}")
            return 0.5

    async def _calculate_habitability_score(self, period: float, depth: float) -> float:
        """–†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ –æ–±–∏—Ç–∞–µ–º–æ—Å—Ç–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–∏–æ–¥–∞ (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç –∑–≤–µ–∑–¥—ã)
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å–æ–ª–Ω–µ—á–Ω—É—é –∑–≤–µ–∑–¥—É

            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ AU (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
            distance_au = (period / 365.25) ** (2 / 3)

            # –ó–æ–Ω–∞ –æ–±–∏—Ç–∞–µ–º–æ—Å—Ç–∏: 0.95 - 1.37 AU –¥–ª—è —Å–æ–ª–Ω–µ—á–Ω–æ–π –∑–≤–µ–∑–¥—ã
            if 0.95 <= distance_au <= 1.37:
                habitability = 1.0
            elif 0.5 <= distance_au <= 2.0:
                # –ß–∞—Å—Ç–∏—á–Ω–æ –æ–±–∏—Ç–∞–µ–º–∞—è –∑–æ–Ω–∞
                if distance_au < 0.95:
                    habitability = distance_au / 0.95
                else:
                    habitability = 2.0 / distance_au
            else:
                habitability = 0.1

            # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ —Ä–∞–∑–º–µ—Ä –ø–ª–∞–Ω–µ—Ç—ã (–Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª—É–±–∏–Ω—ã —Ç—Ä–∞–Ω–∑–∏—Ç–∞)
            if depth > 0.01:  # –ë–æ–ª—å—à–∞—è –ø–ª–∞–Ω–µ—Ç–∞
                habitability *= 0.5
            elif depth < 0.001:  # –û—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–∞—è
                habitability *= 0.7

            return min(1.0, max(0.0, habitability))

        except Exception as e:
            logger.error(f"Error calculating habitability: {e}")
            return 0.0

    async def _calculate_detection_significance(
        self, snr: float, agreement: float, num_methods: int
    ) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è"""
        try:
            # –ë–∞–∑–æ–≤–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ SNR
            base_significance = min(1.0, snr / 15.0)

            # –ë–æ–Ω—É—Å –∑–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ—Ç–æ–¥–æ–≤
            agreement_bonus = agreement * 0.3

            # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–¥–æ–≤
            method_bonus = min(0.2, num_methods * 0.05)

            total_significance = base_significance + agreement_bonus + method_bonus

            return min(1.0, total_significance)

        except Exception as e:
            logger.error(f"Error calculating significance: {e}")
            return 0.5

    async def _cross_validate_results(
        self, method_results: Dict, all_features: Dict
    ) -> float:
        """–ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        try:
            if len(method_results) < 2:
                return 0.5

            # –°–æ–±–∏—Ä–∞–µ–º –ø–µ—Ä–∏–æ–¥—ã –æ—Ç –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤
            periods = []
            for result in method_results.values():
                if result is None:
                    continue
                if hasattr(result, "best_period"):
                    periods.append(result.best_period)
                elif isinstance(result, dict) and "best_period" in result:
                    periods.append(result["best_period"])

            if len(periods) < 2:
                return 0.5

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
            periods = np.array(periods)
            cv = np.std(periods) / np.mean(periods) if np.mean(periods) > 0 else 1.0

            # –ß–µ–º –º–µ–Ω—å—à–µ –≤–∞—Ä–∏–∞—Ü–∏—è, —Ç–µ–º –ª—É—á—à–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
            cv_score = max(0.0, 1.0 - cv * 2)

            return min(1.0, cv_score)

        except Exception as e:
            logger.error(f"Error in cross-validation: {e}")
            return 0.5

    async def _estimate_stellar_parameters(
        self, time: np.ndarray, flux: np.ndarray
    ) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–≤–µ–∑–¥—ã"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö
            flux_std = np.std(flux)
            flux_mean = np.mean(flux)

            # –û—Ü–µ–Ω–∫–∞ –∑–≤–µ–∑–¥–Ω–æ–π –≤–µ–ª–∏—á–∏–Ω—ã (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
            estimated_magnitude = 12.0 + np.log10(
                flux_std * 1000
            )  # –≠–º–ø–∏—Ä–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º—É–ª–∞

            # –û—Ü–µ–Ω–∫–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã (–æ—á–µ–Ω—å –ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
            estimated_temp = 5500 + (flux_mean - 1.0) * 1000

            return {
                "estimated_magnitude": float(estimated_magnitude),
                "estimated_temperature": float(max(3000, min(8000, estimated_temp))),
                "flux_variability": float(flux_std),
                "data_quality": "good" if flux_std < 0.001 else "moderate",
            }

        except Exception as e:
            logger.error(f"Error estimating stellar parameters: {e}")
            return {}

    async def _model_planetary_system(self, period: float, depth: float) -> Dict:
        """–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–µ—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
        try:
            # –û—Ü–µ–Ω–∫–∞ —Ä–∞–¥–∏—É—Å–∞ –ø–ª–∞–Ω–µ—Ç—ã (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è —Å–æ–ª–Ω–µ—á–Ω—É—é –∑–≤–µ–∑–¥—É)
            stellar_radius = 1.0  # –†–∞–¥–∏—É—Å—ã –°–æ–ª–Ω—Ü–∞
            planet_radius = np.sqrt(depth) * stellar_radius * 109.2  # –í —Ä–∞–¥–∏—É—Å–∞—Ö –ó–µ–º–ª–∏

            # –û—Ü–µ–Ω–∫–∞ –º–∞—Å—Å—ã (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
            if planet_radius < 1.5:
                planet_mass = planet_radius**2.06  # –ó–µ–º–Ω–æ–ø–æ–¥–æ–±–Ω—ã–µ
            else:
                planet_mass = planet_radius**1.4  # –ì–∞–∑–æ–≤—ã–µ –≥–∏–≥–∞–Ω—Ç—ã

            # –û—Ä–±–∏—Ç–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            orbital_distance = (period / 365.25) ** (2 / 3)  # –í AU

            return {
                "planet_radius_earth": float(planet_radius),
                "planet_mass_earth": float(planet_mass),
                "orbital_distance_au": float(orbital_distance),
                "orbital_period_days": float(period),
                "planet_type": "terrestrial" if planet_radius < 2.0 else "gas_giant",
            }

        except Exception as e:
            logger.error(f"Error modeling planetary system: {e}")
            return {}

    async def _calculate_orbital_dynamics(self, period: float) -> Dict:
        """–†–∞—Å—á–µ—Ç –æ—Ä–±–∏—Ç–∞–ª—å–Ω–æ–π –¥–∏–Ω–∞–º–∏–∫–∏"""
        try:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å–æ–ª–Ω–µ—á–Ω—É—é –∑–≤–µ–∑–¥—É (1 —Å–æ–ª–Ω–µ—á–Ω–∞—è –º–∞—Å—Å–∞)
            stellar_mass = 1.0

            # –ë–æ–ª—å—à–∞—è –ø–æ–ª—É–æ—Å—å (3-–π –∑–∞–∫–æ–Ω –ö–µ–ø–ª–µ—Ä–∞)
            semi_major_axis = (period / 365.25) ** (2 / 3)  # –í AU

            # –û—Ä–±–∏—Ç–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
            orbital_velocity = 29.78 / np.sqrt(semi_major_axis)  # –∫–º/—Å

            # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —Ä–∞–≤–Ω–æ–≤–µ—Å–∏—è (–ø—Ä–∏–±–ª–∏–∂–µ–Ω–Ω–æ)
            equilibrium_temp = 278 / np.sqrt(semi_major_axis)  # –ö

            return {
                "semi_major_axis_au": float(semi_major_axis),
                "orbital_velocity_km_s": float(orbital_velocity),
                "equilibrium_temperature_k": float(equilibrium_temp),
                "orbital_period_days": float(period),
                "eccentricity": 0.0,  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∫—Ä—É–≥–æ–≤—É—é –æ—Ä–±–∏—Ç—É
            }

        except Exception as e:
            logger.error(f"Error calculating orbital dynamics: {e}")
            return {}

    async def _assess_data_quality(
        self, time: np.ndarray, flux: np.ndarray, all_features: Dict
    ) -> Dict:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        try:
            quality_flags = {}

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫
            if len(time) < 1000:
                quality_flags["low_data_points"] = True

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
            time_span = np.max(time) - np.min(time)
            if time_span < 10:  # –ú–µ–Ω—å—à–µ 10 –¥–Ω–µ–π
                quality_flags["short_baseline"] = True

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Ä–æ–≤–Ω—è —à—É–º–∞
            flux_std = np.std(flux)
            if flux_std > 0.01:  # –ë–æ–ª—å—à–µ 1%
                quality_flags["high_noise"] = True

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö
            time_diff = np.diff(time)
            median_cadence = np.median(time_diff)
            large_gaps = np.sum(time_diff > 5 * median_cadence)
            if large_gaps > len(time) * 0.1:
                quality_flags["data_gaps"] = True

            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            if not quality_flags:
                quality_flags["high_quality"] = True

            return quality_flags

        except Exception as e:
            logger.error(f"Error assessing data quality: {e}")
            return {"unknown_quality": True}
