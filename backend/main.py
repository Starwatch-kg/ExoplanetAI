#!/usr/bin/env python3
"""
FastAPI Backend –¥–ª—è –≤–µ–±-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –ø–æ–∏—Å–∫–∞ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º ML –ø–∞–π–ø–ª–∞–π–Ω–æ–º
"""

import os
import sys
import asyncio
import logging
import uvicorn
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ ML –º–æ–¥—É–ª–µ–π
src_path = str(Path(__file__).parent.parent / "src")
sys.path.insert(0, src_path)
sys.path.insert(0, str(Path(__file__).parent.parent))

from fastapi import FastAPI, HTTPException, BackgroundTasks, File, UploadFile, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from functools import lru_cache
from datetime import datetime
import json
import hashlib
import time
import numpy as np
from PIL import Image
import io
import psutil
import platform

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤ –Ω–∞—á–∞–ª–µ)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('exoplanet_api.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥—É–ª—å –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö TESS
try:
    from backend.real_tess_data import get_real_tess_data
    logger.info("–ú–æ–¥—É–ª—å —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö TESS –∑–∞–≥—Ä—É–∂–µ–Ω")
except ImportError as e:
    logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥—É–ª—å —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö TESS: {e}")
    get_real_tess_data = None

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ ML –º–æ–¥—É–ª–∏
ML_MODULES_AVAILABLE = False
try:
    from src.exoplanet_pipeline import ExoplanetSearchPipeline
    from src.tess_data_loader import TESSDataLoader
    from src.hybrid_transit_search import HybridTransitSearch
    from src.representation_learning import SelfSupervisedRepresentationLearner
    from src.anomaly_ensemble import AnomalyEnsemble
    from src.results_exporter import ResultsExporter, ExoplanetCandidate
    ML_MODULES_AVAILABLE = True
    logger.info("‚úÖ –í—Å–µ ML –º–æ–¥—É–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å ML –º–æ–¥—É–ª–∏: {e}")
    logger.info("üîÑ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∑–∞–≥–ª—É—à–∫–∏ –¥–ª—è ML –º–æ–¥—É–ª–µ–π")
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å
    ExoplanetSearchPipeline = None
    TESSDataLoader = None
    HybridTransitSearch = None
    SelfSupervisedRepresentationLearner = None
    AnomalyEnsemble = None
    ResultsExporter = None
    ExoplanetCandidate = None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FastAPI —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
app = FastAPI(
    title="Exoplanet AI API",
    description="üåå –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π API –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö NASA TESS",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json"
)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
app_start_time = datetime.now()
request_count = 0
active_analyses = 0

# CORS –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://localhost:3000"],  # Vite dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –î–æ–±–∞–≤–ª—è–µ–º —Å–∂–∞—Ç–∏–µ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Middleware –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
@app.middleware("http")
async def monitor_requests(request, call_next):
    global request_count
    request_count += 1
    
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    # –õ–æ–≥–∏—Ä—É–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    if process_time > 1.0:
        logger.warning(f"–ú–µ–¥–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {request.method} {request.url} - {process_time:.2f}s")
    
    response.headers["X-Process-Time"] = str(process_time)
    response.headers["X-Request-Count"] = str(request_count)
    
    return response

# Pydantic –º–æ–¥–µ–ª–∏
class LightcurveData(BaseModel):
    tic_id: str
    times: List[float]
    fluxes: List[float]

class AnalysisRequest(BaseModel):
    lightcurve_data: LightcurveData
    model_type: str = Field(..., description="–¢–∏–ø –º–æ–¥–µ–ª–∏: autoencoder, classifier, hybrid, ensemble")
    parameters: Optional[Dict[str, Any]] = None

class Candidate(BaseModel):
    id: str
    period: float
    depth: float
    duration: float
    confidence: float
    start_time: float
    end_time: float
    method: str

class AnalysisResponse(BaseModel):
    success: bool
    candidates: List[Candidate]
    processing_time: float
    model_used: str
    statistics: Dict[str, Any]
    error: Optional[str] = None

class TICRequest(BaseModel):
    tic_id: str
    sectors: Optional[List[int]] = None

# CNN –º–æ–¥–µ–ª–∏ –¥–ª—è API
class CNNTrainingRequest(BaseModel):
    model_type: str = Field(..., description="–¢–∏–ø CNN –º–æ–¥–µ–ª–∏: cnn, resnet, densenet, attention")
    model_params: Optional[Dict[str, Any]] = None
    training_params: Optional[Dict[str, Any]] = None
    data_params: Optional[Dict[str, Any]] = None

class CNNTrainingResponse(BaseModel):
    success: bool
    training_id: str
    message: str
    model_info: Optional[Dict[str, Any]] = None
    error: Optional[str] = None

class CNNTrainingStatus(BaseModel):
    training_id: str
    status: str  # 'running', 'completed', 'failed', 'not_found'
    current_epoch: int
    total_epochs: int
    current_metrics: Optional[Dict[str, float]] = None
    best_metrics: Optional[Dict[str, float]] = None
    progress_percentage: float
    estimated_time_remaining: Optional[float] = None

class CNNInferenceRequest(BaseModel):
    model_id: str
    lightcurve_data: LightcurveData
    preprocessing: Optional[Dict[str, Any]] = None

class CNNInferenceResponse(BaseModel):
    success: bool
    prediction: Dict[str, Any]
    confidence: float
    processing_time: float
    model_used: str
    error: Optional[str] = None

class CNNModelInfo(BaseModel):
    model_id: str
    model_type: str
    parameters: int
    accuracy: Optional[float] = None
    created_at: str
    status: str

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å TTL

class TTLCache:
    def __init__(self, maxsize=100, ttl=3600):  # TTL 1 —á–∞—Å
        self.cache = {}
        self.timestamps = {}
        self.maxsize = maxsize
        self.ttl = ttl
    
    def get(self, key):
        if key in self.cache:
            if time.time() - self.timestamps[key] < self.ttl:
                return self.cache[key]
            else:
                del self.cache[key]
                del self.timestamps[key]
        return None
    
    def set(self, key, value):
        if len(self.cache) >= self.maxsize:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–π —Å—Ç–∞—Ä—ã–π —ç–ª–µ–º–µ–Ω—Ç
            oldest_key = min(self.timestamps.keys(), key=lambda k: self.timestamps[k])
            del self.cache[oldest_key]
            del self.timestamps[oldest_key]
        
        self.cache[key] = value
        self.timestamps[key] = time.time()

# –ö—ç—à–∏ —Å TTL
pipeline_cache = TTLCache(maxsize=50, ttl=3600)  # 1 —á–∞—Å
analysis_results = TTLCache(maxsize=200, ttl=1800)  # 30 –º–∏–Ω—É—Ç

# CNN –∫—ç—à–∏ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
cnn_models_cache = TTLCache(maxsize=20, ttl=7200)  # 2 —á–∞—Å–∞
cnn_training_sessions = {}  # –ê–∫—Ç–∏–≤–Ω—ã–µ —Å–µ—Å—Å–∏–∏ –æ–±—É—á–µ–Ω–∏—è
cnn_inference_cache = TTLCache(maxsize=100, ttl=3600)  # 1 —á–∞—Å

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
def initialize_ml_components():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
    global pipeline_cache
    
    try:
        if ExoplanetSearchPipeline:
            pipeline_cache['main_pipeline'] = ExoplanetSearchPipeline('src/config.yaml')
            logger.info("–û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        if TESSDataLoader:
            pipeline_cache['data_loader'] = TESSDataLoader(cache_dir="backend_cache")
            logger.info("–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö TESS –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        if HybridTransitSearch:
            pipeline_cache['hybrid_search'] = HybridTransitSearch()
            logger.info("–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        logger.info("ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {e}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    logger.info("–ó–∞–ø—É—Å–∫ Exoplanet AI API...")
    initialize_ml_components()

# API —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    return {
        "message": "Exoplanet AI API",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "health": "/health",
            "analyze": "/analyze",
            "load_tic": "/load-tic",
            "models": "/models"
        }
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è API"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "ml_modules": "available" if ML_MODULES_AVAILABLE else "fallback",
        "cnn_available": CNN_AVAILABLE,
        "version": "2.0.0",
        "uptime": str(datetime.now() - app_start_time)
    }

@app.get("/api/system/metrics")
async def get_system_metrics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
    try:
        # CPU –º–µ—Ç—Ä–∏–∫–∏ (–±—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ –æ–∂–∏–¥–∞–Ω–∏—è)
        cpu_percent = psutil.cpu_percent(interval=None)  # –ú–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        cpu_count = psutil.cpu_count()
        
        # –ü–∞–º—è—Ç—å
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        memory_used = memory.used / (1024**3)  # GB
        memory_total = memory.total / (1024**3)  # GB
        
        # –î–∏—Å–∫
        disk = psutil.disk_usage('/')
        disk_percent = (disk.used / disk.total) * 100
        disk_free = disk.free / (1024**3)  # GB
        
        # –°–µ—Ç—å (—Å–∏–º—É–ª—è—Ü–∏—è –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
        network_latency = np.random.uniform(5, 25)  # ms
        
        # –ü—Ä–æ—Ü–µ—Å—Å—ã Python
        python_processes = len([p for p in psutil.process_iter(['pid', 'name']) 
                               if 'python' in p.info['name'].lower()])
        
        # –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã
        uptime = datetime.now() - app_start_time
        
        return {
            "timestamp": datetime.now().isoformat(),
            "system": {
                "platform": platform.system(),
                "architecture": platform.architecture()[0],
                "python_version": platform.python_version()
            },
            "cpu": {
                "usage_percent": round(cpu_percent, 1),
                "cores": cpu_count,
                "status": "normal" if cpu_percent < 80 else "high"
            },
            "memory": {
                "usage_percent": round(memory_percent, 1),
                "used_gb": round(memory_used, 2),
                "total_gb": round(memory_total, 2),
                "available_gb": round((memory_total - memory_used), 2),
                "status": "normal" if memory_percent < 80 else "high"
            },
            "disk": {
                "usage_percent": round(disk_percent, 1),
                "free_gb": round(disk_free, 2),
                "status": "normal" if disk_percent < 90 else "high"
            },
            "network": {
                "latency_ms": round(network_latency, 1),
                "status": "good" if network_latency < 50 else "slow"
            },
            "application": {
                "uptime": str(uptime).split('.')[0],  # –ë–µ–∑ –º–∏–∫—Ä–æ—Å–µ–∫—É–Ω–¥
                "requests_total": request_count,
                "active_analyses": active_analyses,
                "ml_modules_loaded": ML_MODULES_AVAILABLE,
                "cnn_available": CNN_AVAILABLE,
                "python_processes": python_processes
            }
        }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫: {e}")
        # Fallback –º–µ—Ç—Ä–∏–∫–∏
        return {
            "timestamp": datetime.now().isoformat(),
            "cpu": {"usage_percent": np.random.uniform(15, 35)},
            "memory": {"usage_percent": np.random.uniform(35, 55)},
            "network": {"latency_ms": np.random.uniform(10, 30)},
            "application": {
                "uptime": str(datetime.now() - app_start_time).split('.')[0],
                "requests_total": request_count,
                "active_analyses": active_analyses,
                "status": "running"
            }
        }

@app.get("/api/nasa/stats")
async def get_nasa_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –†–ï–ê–õ–¨–ù–û–ô —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ NASA –¥–ª—è –ª–µ–Ω–¥–∏–Ω–≥–∞"""
    try:
        # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—à NASA API –º–æ–¥—É–ª—å
        from nasa_api import nasa_integration
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ NASA
        real_stats = await nasa_integration.get_nasa_statistics()
        
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–∞ —Ä–µ–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ NASA: {real_stats}")
        return real_stats
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ NASA: {e}")
        
        # Fallback –¥–∞–Ω–Ω—ã–µ
        return {
            "totalPlanets": 5635,
            "totalHosts": 4143,
            "lastUpdated": datetime.now().isoformat(),
            "source": "Fallback data",
            "error": str(e)
        }


@app.post("/load-tic")
async def load_tic_data(request: TICRequest):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö TESS –ø–æ TIC ID"""
    try:
        tic_id = request.tic_id
        sector = request.sectors[0] if request.sectors else None
        
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö TESS –¥–ª—è TIC {tic_id}")
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ lightkurve
        real_data = await get_real_tess_data(tic_id, sector)
        
        if real_data:
            logger.info(f"–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è TIC {tic_id} –∏–∑ {real_data['data_source']}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            response = {
                "success": True,
                "data": {
                    "tic_id": real_data["tic_id"],
                    "times": real_data["times"],
                    "fluxes": real_data["fluxes"],
                    "flux_errors": real_data.get("flux_errors", []),
                    "star_parameters": real_data.get("stellar_params", {}),
                    "transit_parameters": real_data.get("transit_info", {})
                },
                "metadata": real_data.get("metadata", {}),
                "data_source": real_data["data_source"],
                "transit_detected": real_data.get("transit_info", {}).get("detected", False),
                "message": f"–î–∞–Ω–Ω—ã–µ TIC {tic_id} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"
            }
            
            return response
        else:
            # Fallback –∫ NASA API
            from nasa_api import nasa_integration
            result = await nasa_integration.load_tic_data_enhanced(tic_id)
            
            if result["success"]:
                return {
                    "success": True,
                    "data": result["data"],
                    "data_source": "NASA MAST API",
                    "message": result["message"]
                }
            else:
                raise HTTPException(status_code=500, detail="–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö TIC {request.tic_id}: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")

def create_cache_key(data: dict) -> str:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
    data_str = json.dumps(data, sort_keys=True)
    return hashlib.md5(data_str.encode()).hexdigest()

from signal_processor import SignalProcessor

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è CNN
try:
    from cnn_models import CNNModelFactory
    from cnn_trainer import CNNTrainer, create_synthetic_data
    from image_classifier import get_classifier
    CNN_AVAILABLE = True
except ImportError as e:
    logger.warning(f"CNN –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
    CNN_AVAILABLE = False

@app.post("/analyze", response_model=AnalysisResponse)
async def analyze_lightcurve(request: AnalysisRequest):
    """–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    start_time = datetime.now()
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫—ç—à–∞
        cache_data = {
            "times": request.lightcurve_data.times[:100],  # –ü–µ—Ä–≤—ã–µ 100 —Ç–æ—á–µ–∫ –¥–ª—è –∫–ª—é—á–∞
            "fluxes": request.lightcurve_data.fluxes[:100],
            "model_type": request.model_type,
            "tic_id": request.lightcurve_data.tic_id
        }
        cache_key = create_cache_key(cache_data)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_result = analysis_results.get(cache_key)
        if cached_result:
            logger.info(f"–í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è TIC {request.lightcurve_data.tic_id}")
            return cached_result
        
        logger.info(f"–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –º–æ–¥–µ–ª—å—é {request.model_type}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        times = np.array(request.lightcurve_data.times)
        fluxes = np.array(request.lightcurve_data.fluxes)
        tic_id = request.lightcurve_data.tic_id
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ —Å–∏–≥–Ω–∞–ª–æ–≤
        processor = SignalProcessor(fluxes)\
            .remove_noise('wavelet')\
            .detect_transits(threshold=4.5)\
            .analyze_periodicity()\
            .extract_features()\
            .classify_signal()  # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ç—Ä–∞–Ω–∑–∏—Ç–æ–≤
        candidates = []
        for i, transit_idx in enumerate(processor.transits):
            # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã - –∫–∞–∂–¥—ã–π –∏–Ω–¥–µ–∫—Å —Ç—Ä–∞–Ω–∑–∏—Ç–∞ —ç—Ç–æ —Ü–µ–Ω—Ç—Ä —Ç—Ä–∞–Ω–∑–∏—Ç–∞
            start_idx = max(0, transit_idx - 10)
            end_idx = min(len(times)-1, transit_idx + 10)
            
            depth = np.mean(fluxes) - np.min(fluxes[start_idx:end_idx])
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = processor.features['probabilities'][0]  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ '–ø–ª–∞–Ω–µ—Ç–∞'
            
            candidates.append(Candidate(
                id=f"transit_{i}",
                period=processor.features.get('period', 0),
                depth=depth,
                duration=times[end_idx] - times[start_idx],
                confidence=confidence,
                start_time=times[start_idx],
                end_time=times[end_idx],
                method="wavelet+matched_filter+cnn"
            ))
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        processing_time = (datetime.now() - start_time).total_seconds()
        
        statistics = {
            "total_candidates": len(candidates),
            "average_confidence": np.mean([c.confidence for c in candidates]) if candidates else 0,
            "processing_time": processing_time,
            "data_points": len(times),
            "time_span": float(times[-1] - times[0]) if len(times) > 1 else 0,
            "mean": processor.features['mean'],
            "std": processor.features['std'],
            "skew": processor.features['skew'],
            "kurtosis": processor.features['kurtosis'],
            "detected_period": processor.features.get('period', None)
        }
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = AnalysisResponse(
            success=True,
            candidates=candidates,
            processing_time=processing_time,
            model_used=request.model_type,
            statistics=statistics
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à
        analysis_results.set(cache_key, result)
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ –∫—ç—à –¥–ª—è TIC {tic_id}")
        
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return AnalysisResponse(
            success=False,
            candidates=[],
            processing_time=(datetime.now() - start_time).total_seconds(),
            model_used=request.model_type,
            statistics={},
            error=str(e)
        )

# ===== CNN API ENDPOINTS =====

@app.get("/api/cnn/models")
async def get_cnn_models():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö CNN –º–æ–¥–µ–ª–µ–π"""
    if not CNN_AVAILABLE:
        raise HTTPException(status_code=503, detail="CNN –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    try:
        models_info = []
        model_types = ['cnn', 'resnet', 'densenet', 'attention']
        
        for model_type in model_types:
            info = CNNModelFactory.get_model_info(model_type)
            models_info.append({
                'model_type': model_type,
                'name': info.get('name', model_type.upper()),
                'description': info.get('description', ''),
                'complexity': info.get('complexity', 'Unknown'),
                'parameters': info.get('parameters', [])
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –∫—ç—à–∞
        saved_models = []
        for key in cnn_models_cache.cache.keys():
            if key.startswith('model_'):
                model_data = cnn_models_cache.get(key)
                if model_data:
                    saved_models.append({
                        'model_id': key,
                        'model_type': model_data.get('model_type', 'unknown'),
                        'accuracy': model_data.get('accuracy'),
                        'created_at': model_data.get('created_at'),
                        'status': 'ready'
                    })
        
        return {
            'available_architectures': models_info,
            'saved_models': saved_models,
            'cnn_available': CNN_AVAILABLE
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è CNN –º–æ–¥–µ–ª–µ–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/cnn/train", response_model=CNNTrainingResponse)
async def start_cnn_training(request: CNNTrainingRequest, background_tasks: BackgroundTasks):
    """–ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è CNN –º–æ–¥–µ–ª–∏"""
    if not CNN_AVAILABLE:
        raise HTTPException(status_code=503, detail="CNN –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —Å–µ—Å—Å–∏–∏ –æ–±—É—á–µ–Ω–∏—è
        training_id = f"training_{int(time.time())}_{hash(request.model_type) % 10000}"
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        model_params = request.model_params or {}
        training_params = request.training_params or {
            'epochs': 50,
            'batch_size': 32,
            'learning_rate': 1e-3,
            'early_stopping_patience': 10
        }
        data_params = request.data_params or {
            'n_train_samples': 800,
            'n_val_samples': 200,
            'input_length': 2000,
            'noise_level': 0.1
        }
        
        # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
        trainer = CNNTrainer(
            model_type=request.model_type,
            model_params=model_params,
            save_dir=f'cnn_experiments/{training_id}'
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–µ—Å—Å–∏–∏
        cnn_training_sessions[training_id] = {
            'trainer': trainer,
            'status': 'initializing',
            'start_time': time.time(),
            'model_type': request.model_type,
            'training_params': training_params,
            'data_params': data_params,
            'current_epoch': 0,
            'total_epochs': training_params['epochs']
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –≤ —Ñ–æ–Ω–µ
        background_tasks.add_task(run_cnn_training, training_id, trainer, training_params, data_params)
        
        logger.info(f"–ó–∞–ø—É—â–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ CNN {request.model_type} —Å ID: {training_id}")
        
        return CNNTrainingResponse(
            success=True,
            training_id=training_id,
            message=f"–û–±—É—á–µ–Ω–∏–µ {request.model_type} –º–æ–¥–µ–ª–∏ –∑–∞–ø—É—â–µ–Ω–æ",
            model_info={
                'model_type': request.model_type,
                'parameters': model_params,
                'training_parameters': training_params
            }
        )
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è CNN: {e}")
        return CNNTrainingResponse(
            success=False,
            training_id="",
            message="–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è",
            error=str(e)
        )


async def run_cnn_training(training_id: str, trainer: CNNTrainer, 
                          training_params: Dict[str, Any], data_params: Dict[str, Any]):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –æ–±—É—á–µ–Ω–∏—è CNN"""
    try:
        session = cnn_training_sessions[training_id]
        session['status'] = 'preparing_data'
        
        # –°–æ–∑–¥–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        logger.info(f"–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è {training_id}")
        X_train, y_train = create_synthetic_data(
            data_params['n_train_samples'], 
            data_params['input_length'],
            data_params['noise_level']
        )
        X_val, y_val = create_synthetic_data(
            data_params['n_val_samples'], 
            data_params['input_length'],
            data_params['noise_level']
        )
        
        # –°–æ–∑–¥–∞–µ–º DataLoader'—ã
        from torch.utils.data import DataLoader, TensorDataset
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        
        train_loader = DataLoader(train_dataset, batch_size=training_params['batch_size'], shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=training_params['batch_size'], shuffle=False)
        
        session['status'] = 'building_model'
        
        # –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å
        trainer.build_model(
            input_length=data_params['input_length'],
            num_classes=2
        )
        
        # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        trainer.setup_optimizer('adamw', learning_rate=training_params['learning_rate'])
        trainer.setup_scheduler('cosine', T_max=training_params['epochs'])
        
        session['status'] = 'training'
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        logger.info(f"–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è {training_id}")
        results = trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            epochs=training_params['epochs'],
            early_stopping_patience=training_params['early_stopping_patience'],
            verbose=False
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        model_id = f"model_{training_id}"
        cnn_models_cache.set(model_id, {
            'trainer': trainer,
            'model_type': session['model_type'],
            'accuracy': results['best_metrics']['best_val_accuracy'],
            'created_at': datetime.now().isoformat(),
            'training_results': results,
            'data_params': data_params
        })
        
        session['status'] = 'completed'
        session['results'] = results
        session['model_id'] = model_id
        
        logger.info(f"–û–±—É—á–µ–Ω–∏–µ {training_id} –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è {training_id}: {e}")
        session['status'] = 'failed'
        session['error'] = str(e)


@app.get("/api/cnn/training/{training_id}/status", response_model=CNNTrainingStatus)
async def get_training_status(training_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –æ–±—É—á–µ–Ω–∏—è CNN"""
    if training_id not in cnn_training_sessions:
        raise HTTPException(status_code=404, detail="–°–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    session = cnn_training_sessions[training_id]
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    current_metrics = None
    best_metrics = None
    progress_percentage = 0.0
    estimated_time_remaining = None
    
    if 'trainer' in session and session['trainer'].metrics_tracker.train_losses:
        metrics = session['trainer'].metrics_tracker
        current_epoch = len(metrics.train_losses)
        
        if current_epoch > 0:
            current_metrics = {
                'train_loss': metrics.train_losses[-1],
                'val_loss': metrics.val_losses[-1] if metrics.val_losses else 0,
                'train_accuracy': metrics.train_accuracies[-1],
                'val_accuracy': metrics.val_accuracies[-1] if metrics.val_accuracies else 0
            }
            
            best_metrics = metrics.get_best_metrics()
            progress_percentage = (current_epoch / session['total_epochs']) * 100
            
            # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏
            if len(metrics.epochs_times) > 0:
                avg_epoch_time = np.mean(metrics.epochs_times)
                remaining_epochs = session['total_epochs'] - current_epoch
                estimated_time_remaining = avg_epoch_time * remaining_epochs
        
        session['current_epoch'] = current_epoch
    
    return CNNTrainingStatus(
        training_id=training_id,
        status=session['status'],
        current_epoch=session.get('current_epoch', 0),
        total_epochs=session['total_epochs'],
        current_metrics=current_metrics,
        best_metrics=best_metrics,
        progress_percentage=progress_percentage,
        estimated_time_remaining=estimated_time_remaining
    )


@app.post("/api/cnn/inference", response_model=CNNInferenceResponse)
async def cnn_inference(request: CNNInferenceRequest):
    """–ò–Ω—Ñ–µ—Ä–µ–Ω—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω–æ–π CNN –º–æ–¥–µ–ª–∏"""
    if not CNN_AVAILABLE:
        raise HTTPException(status_code=503, detail="CNN –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    start_time = time.time()
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cache_key = f"inference_{request.model_id}_{hash(str(request.lightcurve_data.times[:100]))}"
        cached_result = cnn_inference_cache.get(cache_key)
        if cached_result:
            return cached_result
        
        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model_data = cnn_models_cache.get(request.model_id)
        if not model_data:
            raise HTTPException(status_code=404, detail="–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        
        trainer = model_data['trainer']
        model = trainer.model
        device = trainer.device
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        times = np.array(request.lightcurve_data.times)
        fluxes = np.array(request.lightcurve_data.fluxes)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        input_length = model_data['data_params']['input_length']
        if len(fluxes) != input_length:
            # –ò–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –∏–ª–∏ –æ–±—Ä–µ–∑–∫–∞ –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
            from scipy import interpolate
            f = interpolate.interp1d(np.linspace(0, 1, len(fluxes)), fluxes, kind='linear')
            fluxes = f(np.linspace(0, 1, input_length))
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        fluxes = (fluxes - np.mean(fluxes)) / np.std(fluxes)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä
        import torch
        input_tensor = torch.FloatTensor(fluxes).unsqueeze(0).unsqueeze(0).to(device)
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        model.eval()
        with torch.no_grad():
            output = model(input_tensor)
            probabilities = torch.softmax(output, dim=1)
            prediction = torch.argmax(output, dim=1)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
        pred_class = int(prediction.cpu().numpy()[0])
        confidence = float(probabilities.cpu().numpy()[0].max())
        class_probabilities = probabilities.cpu().numpy()[0].tolist()
        
        processing_time = time.time() - start_time
        
        result = CNNInferenceResponse(
            success=True,
            prediction={
                'class': pred_class,
                'class_name': 'Transit' if pred_class == 1 else 'No Transit',
                'probabilities': {
                    'no_transit': class_probabilities[0],
                    'transit': class_probabilities[1]
                }
            },
            confidence=confidence,
            processing_time=processing_time,
            model_used=f"{model_data['model_type']} (ID: {request.model_id})"
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        cnn_inference_cache.set(cache_key, result)
        
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ CNN –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {e}")
        return CNNInferenceResponse(
            success=False,
            prediction={},
            confidence=0.0,
            processing_time=time.time() - start_time,
            model_used=request.model_id,
            error=str(e)
        )


@app.delete("/api/cnn/models/{model_id}")
async def delete_cnn_model(model_id: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ CNN –º–æ–¥–µ–ª–∏"""
    if model_id in cnn_models_cache.cache:
        del cnn_models_cache.cache[model_id]
        if model_id in cnn_models_cache.timestamps:
            del cnn_models_cache.timestamps[model_id]
        
        logger.info(f"–ú–æ–¥–µ–ª—å {model_id} —É–¥–∞–ª–µ–Ω–∞")
        return {"success": True, "message": f"–ú–æ–¥–µ–ª—å {model_id} —É–¥–∞–ª–µ–Ω–∞"}
    else:
        raise HTTPException(status_code=404, detail="–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")


@app.get("/api/cnn/training/{training_id}/metrics")
async def get_training_metrics(training_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –æ–±—É—á–µ–Ω–∏—è"""
    if training_id not in cnn_training_sessions:
        raise HTTPException(status_code=404, detail="–°–µ—Å—Å–∏—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    session = cnn_training_sessions[training_id]
    
    if 'trainer' not in session:
        return {"message": "–ú–µ—Ç—Ä–∏–∫–∏ –µ—â–µ –Ω–µ –¥–æ—Å—Ç—É–ø–Ω—ã"}
    
    metrics = session['trainer'].metrics_tracker
    
    return {
        'training_id': training_id,
        'status': session['status'],
        'metrics': {
            'train_losses': metrics.train_losses,
            'val_losses': metrics.val_losses,
            'train_accuracies': metrics.train_accuracies,
            'val_accuracies': metrics.val_accuracies,
            'learning_rates': metrics.learning_rates,
            'epochs_times': metrics.epochs_times,
            'train_f1_scores': metrics.train_f1_scores,
            'val_f1_scores': metrics.val_f1_scores,
            'val_aucs': metrics.val_aucs
        },
        'best_metrics': metrics.get_best_metrics() if metrics.train_losses else {}
    }


@app.post("/api/cnn/classify-image")
async def classify_image(
    image: UploadFile = File(...),
    model_type: str = Form(default="resnet")
):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é CNN"""
    if not CNN_AVAILABLE:
        raise HTTPException(status_code=503, detail="CNN –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
    if not image.content_type or not image.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="–§–∞–π–ª –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º")
    
    try:
        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_data = await image.read()
        pil_image = Image.open(io.BytesIO(image_data))
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        classifier = get_classifier(model_type)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º
        result = classifier.classify_image(pil_image)
        
        logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image.filename}: {result['class_name']} ({result['confidence']:.3f})")
        
        return result
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")


@app.get("/api/cnn/image-models")
async def get_image_models():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    if not CNN_AVAILABLE:
        raise HTTPException(status_code=503, detail="CNN –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    models = [
        {
            'id': 'cnn',
            'name': 'Basic CNN',
            'description': '–ë–∞–∑–æ–≤–∞—è —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π',
            'complexity': 'Low',
            'speed': 'Fast'
        },
        {
            'id': 'resnet',
            'name': 'ResNet CNN',
            'description': 'CNN —Å –æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏',
            'complexity': 'Medium',
            'speed': 'Medium'
        },
        {
            'id': 'densenet',
            'name': 'DenseNet CNN',
            'description': 'CNN —Å –ø–ª–æ—Ç–Ω—ã–º–∏ —Å–≤—è–∑—è–º–∏ –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤',
            'complexity': 'High',
            'speed': 'Slow'
        },
        {
            'id': 'attention',
            'name': 'Attention CNN',
            'description': 'CNN —Å –º–µ—Ö–∞–Ω–∏–∑–º–æ–º –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –≤–∞–∂–Ω—ã—Ö –æ–±–ª–∞—Å—Ç—è—Ö',
            'complexity': 'High',
            'speed': 'Slow'
        }
    ]
    
    return {'models': models}


@app.post("/api/cnn/classify-batch")
async def classify_image_batch(
    images: List[UploadFile] = File(...),
    model_type: str = Form(default="resnet")
):
    """–ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    if not CNN_AVAILABLE:
        raise HTTPException(status_code=503, detail="CNN –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    if len(images) > 10:
        raise HTTPException(status_code=400, detail="–ú–∞–∫—Å–∏–º—É–º 10 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞ —Ä–∞–∑")
    
    try:
        # –ß–∏—Ç–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        pil_images = []
        filenames = []
        
        for img in images:
            if not img.content_type or not img.content_type.startswith('image/'):
                raise HTTPException(status_code=400, detail=f"–§–∞–π–ª {img.filename} –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º")
            
            image_data = await img.read()
            pil_image = Image.open(io.BytesIO(image_data))
            pil_images.append(pil_image)
            filenames.append(img.filename)
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
        classifier = get_classifier(model_type)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–∞–∫–µ—Ç
        results = classifier.classify_batch(pil_images)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–º–µ–Ω–∞ —Ñ–∞–π–ª–æ–≤ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        for i, result in enumerate(results):
            result['filename'] = filenames[i]
        
        logger.info(f"–ü–∞–∫–µ—Ç–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
        return {'results': results}
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞–∫–µ—Ç–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {str(e)}")


# ===== LIGHTCURVE ANALYSIS API ENDPOINTS =====

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–æ–≤
lightcurve_analysis_sessions = {}
image_classification_sessions = {}

@app.get("/api/lightcurve/analysis/{tic_id}/status")
async def get_lightcurve_analysis_status(tic_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ TIC ID
        if tic_id in lightcurve_analysis_sessions:
            session = lightcurve_analysis_sessions[tic_id]
            return {
                "status": session.get("status", "unknown"),
                "progress": session.get("progress", 0),
                "message": session.get("message", ""),
                "results": session.get("results", None),
                "error": session.get("error", None),
                "started_at": session.get("started_at", None),
                "completed_at": session.get("completed_at", None)
            }
        else:
            # –ï—Å–ª–∏ –∞–Ω–∞–ª–∏–∑ –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ç—É—Å "not_started"
            return {
                "status": "not_started",
                "progress": 0,
                "message": "–ê–Ω–∞–ª–∏–∑ –Ω–µ –∑–∞–ø—É—â–µ–Ω –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ TIC ID",
                "results": None,
                "error": None,
                "started_at": None,
                "completed_at": None
            }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è TIC {tic_id}: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {str(e)}")


@app.post("/api/lightcurve/analysis/{tic_id}/start")
async def start_lightcurve_analysis(tic_id: str, background_tasks: BackgroundTasks):
    """–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞"""
    try:
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∞–Ω–∞–ª–∏–∑–∞
        session_id = f"analysis_{tic_id}_{int(time.time())}"
        lightcurve_analysis_sessions[tic_id] = {
            "session_id": session_id,
            "status": "running",
            "progress": 0,
            "message": "–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞...",
            "results": None,
            "error": None,
            "started_at": datetime.now().isoformat(),
            "completed_at": None
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑ –≤ —Ñ–æ–Ω–µ
        background_tasks.add_task(run_lightcurve_analysis, tic_id)
        
        return {
            "session_id": session_id,
            "status": "started",
            "message": f"–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ –¥–ª—è TIC {tic_id} –∑–∞–ø—É—â–µ–Ω"
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è TIC {tic_id}: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}")


async def run_lightcurve_analysis(tic_id: str):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞"""
    try:
        session = lightcurve_analysis_sessions[tic_id]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        session["progress"] = 25
        session["message"] = "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö TESS..."
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞
        # –ü–æ–∫–∞ —á—Ç–æ –∏–º–∏—Ç–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É
        await asyncio.sleep(2)
        
        session["progress"] = 50
        session["message"] = "–ê–Ω–∞–ª–∏–∑ —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤..."
        await asyncio.sleep(2)
        
        session["progress"] = 75
        session["message"] = "–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤..."
        await asyncio.sleep(1)
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –∞–Ω–∞–ª–∏–∑
        session["status"] = "completed"
        session["progress"] = 100
        session["message"] = "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω"
        session["completed_at"] = datetime.now().isoformat()
        session["results"] = {
            "tic_id": tic_id,
            "transit_detected": False,
            "confidence": 0.0,
            "analysis_summary": "–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω. –¢—Ä–∞–Ω–∑–∏—Ç–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ."
        }
        
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ –¥–ª—è TIC {tic_id} –∑–∞–≤–µ—Ä—à–µ–Ω")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∞–Ω–∞–ª–∏–∑–µ –∫—Ä–∏–≤–æ–π –±–ª–µ—Å–∫–∞ –¥–ª—è TIC {tic_id}: {e}")
        session = lightcurve_analysis_sessions.get(tic_id, {})
        session["status"] = "error"
        session["error"] = str(e)
        session["completed_at"] = datetime.now().isoformat()


# ===== IMAGE CLASSIFICATION API ENDPOINTS =====

@app.post("/api/cnn/classify")
async def classify_image_simple(
    image: UploadFile = File(...),
    model: str = Form(default="exoplanet_cnn"),
    background_tasks: BackgroundTasks = None
):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å frontend)"""
    try:
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ID –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_id = hashlib.md5(f"{image.filename}_{time.time()}".encode()).hexdigest()[:12]
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        image_classification_sessions[image_id] = {
            "image_id": image_id,
            "status": "processing",
            "progress": 0,
            "message": "–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...",
            "results": None,
            "error": None,
            "started_at": datetime.now().isoformat(),
            "completed_at": None
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –≤ —Ñ–æ–Ω–µ –µ—Å–ª–∏ –µ—Å—Ç—å background_tasks
        if background_tasks:
            background_tasks.add_task(run_image_classification, image_id, image, model)
        else:
            # –°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
            await run_image_classification_sync(image_id, image, model)
        
        return {
            "image_id": image_id,
            "status": "started" if background_tasks else "completed",
            "message": f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∑–∞–ø—É—â–µ–Ω–∞" if background_tasks else "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {str(e)}")


@app.get("/api/cnn/classify/{image_id}/status")
async def get_image_classification_status(image_id: str):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        if image_id in image_classification_sessions:
            session = image_classification_sessions[image_id]
            return {
                "status": session.get("status", "unknown"),
                "progress": session.get("progress", 0),
                "message": session.get("message", ""),
                "results": session.get("results", None),
                "error": session.get("error", None),
                "started_at": session.get("started_at", None),
                "completed_at": session.get("completed_at", None)
            }
        else:
            return {
                "status": "not_found",
                "progress": 0,
                "message": "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞",
                "results": None,
                "error": None,
                "started_at": None,
                "completed_at": None
            }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è {image_id}: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞: {str(e)}")


async def run_image_classification(image_id: str, image: UploadFile, model: str):
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        session = image_classification_sessions[image_id]
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
        session["progress"] = 25
        session["message"] = "–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è..."
        
        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_data = await image.read()
        await asyncio.sleep(0.5)  # –ò–º–∏—Ç–∞—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        
        session["progress"] = 50
        session["message"] = "–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è..."
        await asyncio.sleep(1)
        
        session["progress"] = 75
        session["message"] = "–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."
        await asyncio.sleep(0.5)
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        session["status"] = "completed"
        session["progress"] = 100
        session["message"] = "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
        session["completed_at"] = datetime.now().isoformat()
        session["results"] = {
            "class": "–¢—Ä–∞–Ω–∑–∏—Ç —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç—ã",
            "confidence": 0.85,
            "description": "–û–±–Ω–∞—Ä—É–∂–µ–Ω –≤–æ–∑–º–æ–∂–Ω—ã–π —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã–π —Å–∏–≥–Ω–∞–ª —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç—ã"
        }
        
        logger.info(f"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_id}: {e}")
        session = image_classification_sessions.get(image_id, {})
        session["status"] = "error"
        session["error"] = str(e)
        session["completed_at"] = datetime.now().isoformat()


async def run_image_classification_sync(image_id: str, image: UploadFile, model: str):
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        session = image_classification_sessions[image_id]
        
        # –ß–∏—Ç–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_data = await image.read()
        
        # –ò–º–∏—Ç–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
        session["status"] = "completed"
        session["progress"] = 100
        session["message"] = "–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞"
        session["completed_at"] = datetime.now().isoformat()
        session["results"] = {
            "class": "–¢—Ä–∞–Ω–∑–∏—Ç —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç—ã",
            "confidence": 0.85,
            "description": "–û–±–Ω–∞—Ä—É–∂–µ–Ω –≤–æ–∑–º–æ–∂–Ω—ã–π —Ç—Ä–∞–Ω–∑–∏—Ç–Ω—ã–π —Å–∏–≥–Ω–∞–ª —ç–∫–∑–æ–ø–ª–∞–Ω–µ—Ç—ã"
        }
        
        logger.info(f"–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_id}: {e}")
        session = image_classification_sessions.get(image_id, {})
        session["status"] = "error"
        session["error"] = str(e)
        session["completed_at"] = datetime.now().isoformat()


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
